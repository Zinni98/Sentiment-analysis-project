{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"movie_reviews\")\n",
    "nltk.download(\"subjectivity\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"sentiwordnet\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import subjectivity\n",
    "\n",
    "\n",
    "subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
    "obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
    "corpus = subj + obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart',\n",
       " 'and',\n",
       " 'alert',\n",
       " ',',\n",
       " 'thirteen',\n",
       " 'conversations',\n",
       " 'about',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'small',\n",
       " 'gem',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(subj[0])\n",
    "print(obj[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(obj))\n",
    "print(len(subj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0576"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_average_sentence_length(corpus):\n",
    "    lengths = []\n",
    "    for sent in corpus:\n",
    "        lengths.append(len(sent))\n",
    "    return sum(lengths)/len(lengths)\n",
    "\n",
    "compute_average_sentence_length(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(corpus):\n",
    "  corpus_words = get_corpus_words(corpus)\n",
    "  vocab = dict()\n",
    "  for word in corpus_words:\n",
    "    try:\n",
    "      vocab[word] += 1\n",
    "    except:\n",
    "      vocab[word] = 1\n",
    "  return vocab\n",
    "\n",
    "def get_corpus_words(corpus):\n",
    "    return [w for sent in corpus for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from tqdm import tqdm\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.vocab import FastText\n",
    "import torch\n",
    "\n",
    "# function inspired by https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings/notebook\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    null_embedding = torch.tensor([0.0]*300)\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "          if torch.equal(embeddings_index.get_vecs_by_tokens(word), null_embedding):\n",
    "            raise KeyError\n",
    "          a[word] = embeddings_index.get_vecs_by_tokens(word)\n",
    "          k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print()\n",
    "    print(f'Found embeddings for {len(a) / len(vocab):.2%} of vocab')\n",
    "    print(f'Found embeddings for  {k / (k + i):.2%} of all text')\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vectors = GloVe(name='840B', dim=300, cache = \"/content/gdrive/My Drive/nlu-project/Embeddings/.vector_cache\")\n",
    "vocab = create_vocab(corpus)\n",
    "oov = check_coverage(vocab, global_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text = FastText('en', cache = \"/content/gdrive/My Drive/nlu-project/Embeddings/.vector_cache\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = movie_reviews\n",
    "neg = mr.paras(categories = \"neg\")\n",
    "pos = mr.paras(categories = \"pos\")\n",
    "print(f\"length of each part of the dataset:\\n - pos: {len(pos)} \\n - neg: {len(neg)}\\n\")\n",
    "print(pos[0])\n",
    "corpus = pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.629"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_average_document_length(corpus):\n",
    "    lengths = []\n",
    "    for doc in corpus:\n",
    "        lengths.append(len(doc))\n",
    "    return sum(lengths)/len(lengths)\n",
    "\n",
    "compute_average_document_length(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.270127800422937"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_average_sentence_length(corpus):\n",
    "    lengths = []\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            lengths.append(len(sent))\n",
    "    return sum(lengths)/len(lengths)\n",
    "\n",
    "compute_average_sentence_length(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefinition to account for documents\n",
    "def get_corpus_words(corpus):\n",
    "    return [w for doc in corpus for sent in doc for w in sent]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking coverage of the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(corpus)\n",
    "oov = check_coverage(vocab, global_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_underscores(corpus):\n",
    "  for doc in corpus:\n",
    "    for sent in doc:\n",
    "      for idx, word in enumerate(sent):\n",
    "        if \"_\" in word:\n",
    "          cleaned_word = _clean_word(word)\n",
    "          sent[idx] = cleaned_word\n",
    "  return corpus\n",
    "\n",
    "\n",
    "def _clean_word(word: str):\n",
    "  word = word.replace(\"_\", \" \")\n",
    "  word = word.split()\n",
    "  return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = remove_underscores(corpus)\n",
    "vocab = create_vocab(get_corpus_words(clean_corpus))\n",
    "oov = check_coverage(vocab, global_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0262c2e7a08424d65c968f8ecfc5afb6b5a99089f86fd0fa27478ea619b0ef2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
