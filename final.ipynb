{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hxs1itjcVCNG"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")"
      ],
      "metadata": {
        "id": "AgeDhuIOVHTI",
        "outputId": "3378eca4-ff8b-4534-c74d-a8ae8581f58d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gdrive/My Drive/nlu-project\")"
      ],
      "metadata": {
        "id": "RPVMeJhiVbRz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cGZ6gd-IVCNK"
      },
      "outputs": [],
      "source": [
        "from corpus import MovieReviewsCorpus, MovieReviewsCorpusPhrases\n",
        "from models import BiLSTM, BiLSTMAttention\n",
        "from preprocess import MRPipelineTokens, MRPipelinePhrases\n",
        "from utils import *\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.vocab import FastText\n",
        "from data import MovieReviewsDataset, get_data\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "om_U1yTNVCNL",
        "outputId": "6bcec7d1-84f0-4c02-d7d1-d53919ff1e93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"movie_reviews\")\n",
        "nltk.download(\"subjectivity\")\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mC9UJsJlVCNM"
      },
      "outputs": [],
      "source": [
        "global_vectors = GloVe(name='840B', dim=300, cache = \"/content/gdrive/My Drive/nlu-project/.vector_cache\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mr_pipeline = MRPipelineTokens()\n",
        "corpus = MovieReviewsCorpus(mr_pipeline)"
      ],
      "metadata": {
        "id": "_8Ibvcw3gAlA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov = check_coverage(corpus.vocab, global_vectors)"
      ],
      "metadata": {
        "id": "NjOQ48t5kGPS",
        "outputId": "98ecb844-6e35-468f-b400-f3b4a11a871d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 39275/39275 [00:01<00:00, 22466.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found embeddings for 92.58% of vocab\n",
            "Found embeddings for  95.28% of all text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3UCffSKYVCNM"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = corpus.get_embedding_matrix(global_vectors, 300)\n",
        "ds = corpus.get_indexed_corpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a-fDJy7jVCNN",
        "outputId": "0e7c81d2-ecb8-49f2-b68d-a000eeeeef72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/gdrive/My Drive/nlu-project/data.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  self.corpus = np.array(raw_dataset[0], dtype = object)\n"
          ]
        }
      ],
      "source": [
        "dataset = MovieReviewsDataset(ds)\n",
        "train_loader, test_loader = get_data(128, dataset, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CHAp4Kg1VCNO"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    in_size = targets.size(dim=0)\n",
        "\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dHprTN-TVCNP"
      },
      "outputs": [],
      "source": [
        "def test_step(net, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      in_size = targets.size(dim=0)\n",
        "\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = outputs.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GsGOsFFfVCNQ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "\n",
        "def main(train_loader, test_loader, embedding_matrix, device = \"cuda\", epochs = 10):\n",
        "\n",
        "  net = BiLSTMAttention(embedding_matrix, device = device, input_size=300).to(device)\n",
        "\n",
        "  optimizer = Adam(net.parameters(), 0.001, betas = (0.9, 0.999), amsgrad=True)\n",
        "\n",
        "  cost_function = nn.CrossEntropyLoss()\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    test_loss, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "    print(f\"Test loss: {test_loss} \\n Test accuracy: {test_accuracy}\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "  \n",
        "  _, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "\n",
        "\n",
        "  return test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchtext torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
      ],
      "metadata": {
        "id": "4zbpq0xByiYi",
        "outputId": "d615857d-0271-4e0b-86f4-fed14b2cd691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-G1JNa_8VCNQ",
        "outputId": "75db78b8-75e8-45b4-b299-2fbff307966b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/gdrive/My Drive/nlu-project/utils.py:40: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  X = np.array(list(X))\n",
            "/content/gdrive/My Drive/nlu-project/utils.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(list(X))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.004780624248087406 \n",
            " Training accuracy: 67.625\n",
            "Test loss: 0.006847285926342011 \n",
            " Test accuracy: 64.25\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.003231080211699009 \n",
            " Training accuracy: 82.375\n",
            "Test loss: 0.006645809859037399 \n",
            " Test accuracy: 82.25\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.0018964504916220903 \n",
            " Training accuracy: 91.1875\n",
            "Test loss: 0.005972939282655716 \n",
            " Test accuracy: 85.75\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.0009313559159636498 \n",
            " Training accuracy: 95.6875\n",
            "Test loss: 0.004490049406886101 \n",
            " Test accuracy: 86.25\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0003248953731963411 \n",
            " Training accuracy: 99.3125\n",
            "Test loss: 0.005463392287492752 \n",
            " Test accuracy: 78.25\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.00010079089843202382 \n",
            " Training accuracy: 99.875\n",
            "Test loss: 0.005245868116617203 \n",
            " Test accuracy: 83.25\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 7.388301761238835e-05 \n",
            " Training accuracy: 99.9375\n",
            "Test loss: 0.004944950565695763 \n",
            " Test accuracy: 83.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 6.944489185116254e-05 \n",
            " Training accuracy: 99.9375\n",
            "Test loss: 0.021811606884002684 \n",
            " Test accuracy: 60.75000000000001\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.00011345983264618553 \n",
            " Training accuracy: 99.75\n",
            "Test loss: 0.026333068013191224 \n",
            " Test accuracy: 60.75000000000001\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.00017153924287413247 \n",
            " Training accuracy: 99.3125\n",
            "Test loss: 0.006349506378173828 \n",
            " Test accuracy: 81.5\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.00010809586936375126 \n",
            " Training accuracy: 99.6875\n",
            "Test loss: 0.011185465604066849 \n",
            " Test accuracy: 74.0\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 3.40837865951471e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.006088074073195457 \n",
            " Test accuracy: 81.75\n",
            "------------------------------------------------------------------\n",
            "epoch 12:\n",
            "Training loss: 1.410130749718519e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.009130968600511552 \n",
            " Test accuracy: 78.75\n",
            "------------------------------------------------------------------\n",
            "epoch 13:\n",
            "Training loss: 7.657815222046339e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.005878193378448486 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 14:\n",
            "Training loss: 5.380352085921913e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.004830688238143921 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 15:\n",
            "Training loss: 3.5610434588306816e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.004874595850706101 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 16:\n",
            "Training loss: 2.6447908203408586e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.004888199791312217 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 17:\n",
            "Training loss: 2.637260113260709e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.004957984760403633 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 18:\n",
            "Training loss: 2.203367657784838e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.004998884275555611 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 19:\n",
            "Training loss: 1.7720155710776454e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.0050279185175895694 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 20:\n",
            "Training loss: 1.648013981139229e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.005063729584217072 \n",
            " Test accuracy: 87.25\n",
            "------------------------------------------------------------------\n",
            "epoch 21:\n",
            "Training loss: 1.5487318933082862e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.00513325035572052 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 22:\n",
            "Training loss: 1.3135196104485658e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.005148380696773529 \n",
            " Test accuracy: 87.25\n",
            "------------------------------------------------------------------\n",
            "epoch 23:\n",
            "Training loss: 1.2133618292864412e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.005159156620502472 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 24:\n",
            "Training loss: 1.1759179596992908e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.005196283310651779 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 25:\n",
            "Training loss: 1.0404886575088312e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.005234152004122734 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 26:\n",
            "Training loss: 1.0849389309441903e-06 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.0052960353344678876 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 27:\n",
            "Training loss: 8.569920419176924e-07 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.005288185700774193 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 28:\n",
            "Training loss: 8.557666296837851e-07 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.005291293859481811 \n",
            " Test accuracy: 86.75\n",
            "------------------------------------------------------------------\n",
            "epoch 29:\n",
            "Training loss: 9.60666059199866e-07 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.005404924228787422 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86.5"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "main(train_loader, test_loader, embedding_matrix, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYPh87T2nGv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('nlu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e0262c2e7a08424d65c968f8ecfc5afb6b5a99089f86fd0fa27478ea619b0ef2"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}