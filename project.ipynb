{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinni98/Sentiment-analysis-project/blob/refactoring/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NBg5KtW5AMy"
      },
      "source": [
        "## Polarity Classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8msH_nw3rf39",
        "outputId": "780e2e84-dc47-43c2-df03-bfb35390ed6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gdrive/My Drive/nlu-project\")"
      ],
      "metadata": {
        "id": "vdSs1gS1rgh_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvHgFZJe4_Xq",
        "outputId": "e0238755-a5b9-4358-a048-0b9b2bcc17f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "import torch\n",
        "from nltk.corpus import movie_reviews\n",
        "import numpy as np\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"movie_reviews\")\n",
        "nltk.download(\"subjectivity\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"sentiwordnet\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fglEQLVLtc9C"
      },
      "source": [
        "## Exploratory analysis\n",
        "\n",
        "Firstly let's explore the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dKetqTKrZFE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "2d7a7c37-ca09-400f-eb85-ae19768fc243"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b7195547899d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"neg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"length of each part of the dataset:\\n - pos: {len(pos)} \\n - neg: {len(neg)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'movie_reviews' is not defined"
          ]
        }
      ],
      "source": [
        "mr = movie_reviews\n",
        "neg = mr.paras(categories = \"neg\")\n",
        "pos = mr.paras(categories = \"pos\")\n",
        "print(f\"length of each part of the dataset:\\n - pos: {len(pos)} \\n - neg: {len(neg)}\")\n",
        "print(pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Y4wuDNt7XW"
      },
      "source": [
        "It's easy to see that data comes in the following format:\n",
        "\n",
        "- pos = [doc1, doc2, ..., doc1000] (the same applies for negative sentiment examples)\n",
        "\n",
        "Where each doc has the following structure:\n",
        "\n",
        "- doc1 = [sentence_1, sentence_2, ..., sentence_k]\n",
        "\n",
        "Each sentence is a list of tokens, so the dataset is already tokenized.\n",
        "\n",
        "### Word embedding\n",
        "Since I'm going to use deep learning models, I'm going to choose a word embedding to transform the text into vectors.\n",
        "I'm going to start with a pretrained version of GloVe word embedding.\n",
        "Since is a pre-trained word embedding (hence basically a lookup table), I'm going to check how many words of the vocabulary are covered by the pretrained word embedding model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_SEn4EFxKTd"
      },
      "outputs": [],
      "source": [
        "def create_vocab(corpus_words):\n",
        "    vocab = dict()\n",
        "    for word in corpus_words:\n",
        "      try:\n",
        "        vocab[word] += 1\n",
        "      except:\n",
        "        vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "def get_corpus_words(corpus) -> list:\n",
        "    return [w for doc in corpus for sent in doc for w in sent]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdofBhKWxQHx"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from tqdm import tqdm\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "\n",
        "global_vectors = GloVe(name='840B', dim=300)\n",
        "\n",
        "# function inspired by https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings/notebook\n",
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    null_embedding = torch.tensor([0.0]*300)\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "          if torch.equal(embeddings_index.get_vecs_by_tokens(word), null_embedding):\n",
        "            raise KeyError\n",
        "          a[word] = embeddings_index.get_vecs_by_tokens(word)\n",
        "          k += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print()\n",
        "    print(f'Found embeddings for {len(a) / len(vocab):.2%} of vocab')\n",
        "    print(f'Found embeddings for  {k / (k + i):.2%} of all text')\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x\n",
        "\n",
        "vocab = create_vocab(get_corpus_words(pos + neg))\n",
        "oov = check_coverage(vocab, global_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K0HUOB2-E1V"
      },
      "outputs": [],
      "source": [
        "oov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEjIRmuG-Quz"
      },
      "source": [
        "I'm going to see which are the words that are not covered by the embedding (Out Of Vocabulary words), so I can try to see if there are some tenchniques that can be applied in order to improve coverage.\n",
        "The majority of OOV words aren't related with a praticular sentiment (they are basically nouns or some type punctuation), so they can be safely removed. That happens because unknown words are encoded as $[0] * embedding.length$, so no useful information is added.\n",
        "Others OOV words are regular words surrounded by underscores, so they are not recognized by the fixed word embedding. To avoid this problem I implemented a procedure in order to clean these words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgvlBpnMB8EO"
      },
      "outputs": [],
      "source": [
        "def remove_underscores(corpus):\n",
        "  for doc in corpus:\n",
        "    for sent in doc:\n",
        "      for idx, word in enumerate(sent):\n",
        "        if \"_\" in word:\n",
        "          cleaned_word = _clean_word(word)\n",
        "          sent[idx] = cleaned_word\n",
        "  return corpus\n",
        "\n",
        "\n",
        "def _clean_word(word: str):\n",
        "  word = word.replace(\"_\", \" \")\n",
        "  word = word.split()\n",
        "  word = \" \".join(word)\n",
        "  return word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUphYnDOa785",
        "outputId": "e0b314d1-9afc-4056-bcd1-d731c2fb907a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 39519/39519 [00:01<00:00, 28083.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Found embeddings for 92.48% of vocab\n",
            "Found embeddings for  99.61% of all text\n"
          ]
        }
      ],
      "source": [
        "corpus = pos + neg\n",
        "clean_corpus = remove_underscores(corpus), oov\n",
        "vocab = create_vocab(get_corpus_words(clean_corpus))\n",
        "oov = check_coverage(vocab, global_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import spacy\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.sentiment.util import mark_negation\n",
        "\n",
        "from nltk.corpus import subjectivity\n",
        "\n",
        "\n",
        "CONTRACTION_MAP =  {\"ain't\": \"is not\",\n",
        "                        \"aren't\": \"are not\",\n",
        "                        \"can't\": \"cannot\",\n",
        "                        \"can't've\": \"cannot have\",\n",
        "                        \"'cause\": \"because\",\n",
        "                        \"could've\": \"could have\",\n",
        "                        \"couldn't\": \"could not\",\n",
        "                        \"couldn't've\": \"could not have\",\n",
        "                        \"didn't\": \"did not\",\n",
        "                        \"doesn't\": \"does not\",\n",
        "                        \"don't\": \"do not\",\n",
        "                        \"hadn't\": \"had not\",\n",
        "                        \"hadn't've\": \"had not have\",\n",
        "                        \"hasn't\": \"has not\",\n",
        "                        \"haven't\": \"have not\",\n",
        "                        \"he'd\": \"he would\",\n",
        "                        \"he'd've\": \"he would have\",\n",
        "                        \"he'll\": \"he will\",\n",
        "                        \"he'll've\": \"he he will have\",\n",
        "                        \"he's\": \"he is\",\n",
        "                        \"how'd\": \"how did\",\n",
        "                        \"how'd'y\": \"how do you\",\n",
        "                        \"how'll\": \"how will\",\n",
        "                        \"how's\": \"how is\",\n",
        "                        \"i'd\": \"i would\",\n",
        "                        \"i'd've\": \"i would have\",\n",
        "                        \"i'll\": \"i will\",\n",
        "                        \"i'll've\": \"i will have\",\n",
        "                        \"i'm\": \"i am\",\n",
        "                        \"i've\": \"i have\",\n",
        "                        \"isn't\": \"is not\",\n",
        "                        \"it'd\": \"it would\",\n",
        "                        \"it'd've\": \"it would have\",\n",
        "                        \"it'll\": \"it will\",\n",
        "                        \"it'll've\": \"it will have\",\n",
        "                        \"it's\": \"it is\",\n",
        "                        \"let's\": \"let us\",\n",
        "                        \"ma'am\": \"madam\",\n",
        "                        \"mayn't\": \"may not\",\n",
        "                        \"might've\": \"might have\",\n",
        "                        \"mightn't\": \"might not\",\n",
        "                        \"mightn't've\": \"might not have\",\n",
        "                        \"must've\": \"must have\",\n",
        "                        \"mustn't\": \"must not\",\n",
        "                        \"mustn't've\": \"must not have\",\n",
        "                        \"needn't\": \"need not\",\n",
        "                        \"needn't've\": \"need not have\",\n",
        "                        \"o'clock\": \"of the clock\",\n",
        "                        \"oughtn't\": \"ought not\",\n",
        "                        \"oughtn't've\": \"ought not have\",\n",
        "                        \"shan't\": \"shall not\",\n",
        "                        \"sha'n't\": \"shall not\",\n",
        "                        \"shan't've\": \"shall not have\",\n",
        "                        \"she'd\": \"she would\",\n",
        "                        \"she'd've\": \"she would have\",\n",
        "                        \"she'll\": \"she will\",\n",
        "                        \"she'll've\": \"she will have\",\n",
        "                        \"she's\": \"she is\",\n",
        "                        \"should've\": \"should have\",\n",
        "                        \"shouldn't\": \"should not\",\n",
        "                        \"shouldn't've\": \"should not have\",\n",
        "                        \"so've\": \"so have\",\n",
        "                        \"so's\": \"so as\",\n",
        "                        \"that'd\": \"that would\",\n",
        "                        \"that'd've\": \"that would have\",\n",
        "                        \"that's\": \"that is\",\n",
        "                        \"there'd\": \"there would\",\n",
        "                        \"there'd've\": \"there would have\",\n",
        "                        \"there's\": \"there is\",\n",
        "                        \"they'd\": \"they would\",\n",
        "                        \"they'd've\": \"they would have\",\n",
        "                        \"they'll\": \"they will\",\n",
        "                        \"they'll've\": \"they will have\",\n",
        "                        \"they're\": \"they are\",\n",
        "                        \"they've\": \"they have\",\n",
        "                        \"to've\": \"to have\",\n",
        "                        \"wasn't\": \"was not\",\n",
        "                        \"we'd\": \"we would\",\n",
        "                        \"we'd've\": \"we would have\",\n",
        "                        \"we'll\": \"we will\",\n",
        "                        \"we'll've\": \"we will have\",\n",
        "                        \"we're\": \"we are\",\n",
        "                        \"we've\": \"we have\",\n",
        "                        \"weren't\": \"were not\",\n",
        "                        \"what'll\": \"what will\",\n",
        "                        \"what'll've\": \"what will have\",\n",
        "                        \"what're\": \"what are\",\n",
        "                        \"what's\": \"what is\",\n",
        "                        \"what've\": \"what have\",\n",
        "                        \"when's\": \"when is\",\n",
        "                        \"when've\": \"when have\",\n",
        "                        \"where'd\": \"where did\",\n",
        "                        \"where's\": \"where is\",\n",
        "                        \"where've\": \"where have\",\n",
        "                        \"who'll\": \"who will\",\n",
        "                        \"who'll've\": \"who will have\",\n",
        "                        \"who's\": \"who is\",\n",
        "                        \"who've\": \"who have\",\n",
        "                        \"why's\": \"why is\",\n",
        "                        \"why've\": \"why have\",\n",
        "                        \"will've\": \"will have\",\n",
        "                        \"won't\": \"will not\",\n",
        "                        \"won't've\": \"will not have\",\n",
        "                        \"would've\": \"would have\",\n",
        "                        \"wouldn't\": \"would not\",\n",
        "                        \"wouldn't've\": \"would not have\",\n",
        "                        \"y'all\": \"you all\",\n",
        "                        \"y'all'd\": \"you all would\",\n",
        "                        \"y'all'd've\": \"you all would have\",\n",
        "                        \"y'all're\": \"you all are\",\n",
        "                        \"y'all've\": \"you all have\",\n",
        "                        \"you'd\": \"you would\",\n",
        "                        \"you'd've\": \"you would have\",\n",
        "                        \"you'll\": \"you will\",\n",
        "                        \"you'll've\": \"you will have\",\n",
        "                        \"you're\": \"you are\",\n",
        "                        \"you've\": \"you have\",\n",
        "                    }\n",
        "\n",
        "class PipelineElement(ABC):\n",
        "  \"\"\"\n",
        "  Abstract class for the definition of each element\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def __call__(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "class Pipeline():\n",
        "  \"\"\"\n",
        "  Pipeline class which collects pipeline elements (in the order given).\n",
        "  This class implements __call__ method so it is a callable.\n",
        "  When called it applies all the PipelineElements in the order given.\n",
        "  \"\"\"\n",
        "  def __init__(self, *args):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    *args\n",
        "      PipelineElements\n",
        "    \"\"\"\n",
        "    self.pipeline = []\n",
        "    for arg in args:\n",
        "      self.add_pipeline_element(arg)\n",
        "\n",
        "  def add_pipeline_element(self, element: PipelineElement, position: int = None):\n",
        "    \"\"\"\n",
        "    Adds a new pipeline element to the pipeline\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    element : PipelineElement\n",
        "      the element to be added to the pipeline\n",
        "    \n",
        "    position : int\n",
        "      position in the pipeline where the element should be added\n",
        "      position ranges from 0 to (n_elements - 1) where n_elements\n",
        "      is the number of elements in the pipeline.\n",
        "    Raises\n",
        "    ------\n",
        "    TypeError\n",
        "      If the type of element is not PipelineElement\n",
        "    \"\"\"\n",
        "    if not issubclass(type(element), PipelineElement):\n",
        "      raise TypeError(\"Wrong element type, only Pipeline elements subclasses can be added\")\n",
        "    if position:\n",
        "      if position >= len(self.pipeline):\n",
        "        raise ValueError(\"position index exceeds the lenght of the pipeline\")\n",
        "      self.pipeline.insert(position, element)\n",
        "    else:\n",
        "      self.pipeline.append(element)\n",
        "  \n",
        "  def pipe(self, corpus):\n",
        "    \"\"\"\n",
        "    Applies each element in the pipeline\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : list\n",
        "      list containing each document in the corpus\n",
        "    \"\"\"\n",
        "    for el in self.pipeline:\n",
        "        corpus = el(corpus)\n",
        "    return corpus\n",
        "  \n",
        "  def get_elements(self):\n",
        "    \"\"\"\n",
        "    Gives elements of the pipeline with respective index indicateing the order\n",
        "    in which elements are called\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      Where the key indicates the position of each element in the pipeline\n",
        "      (i.e. execution order, where 0 is the first element of the pipeline\n",
        "      being called) and the value indicates the actual element.\n",
        "    \"\"\"\n",
        "    res = {}\n",
        "    for idx, el in pipeline:\n",
        "      res[idx] = el\n",
        "    return res\n",
        " \n",
        "  def __call__(self, *args):\n",
        "      if args[0] == None:\n",
        "          raise ValueError(\"Need a corpus as argument\")\n",
        "      corpus = args[0]\n",
        "      return self.pipe(corpus)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(pipeline)\n",
        "        \n",
        "# Flattened Elements\n",
        "\n",
        "class UnderscoreRemoverFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Assumes the corpus is flat (i.e. the corpus is a list of documents,\n",
        "  each document is a list of words, therefore the document is not\n",
        "  divided in sentences)\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(UnderscoreRemoverFlat, self).__init__()\n",
        "\n",
        "  def remove_underscores(self, corpus):\n",
        "    \"\"\"\n",
        "    Solves the problem where some of the words are surrounded by underscores\n",
        "    (e.g. \"_hello_\")\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : list of list of list\n",
        "      corpus to be processed\n",
        "    \"\"\"\n",
        "    for doc in corpus:\n",
        "        for idx, word in enumerate(doc):\n",
        "            if \"_\" in word:\n",
        "                cleaned_word = self._clean_word(word)\n",
        "                doc[idx] = cleaned_word\n",
        "    return corpus\n",
        "\n",
        "\n",
        "  def _clean_word(self, word: str):\n",
        "    word = word.replace(\"_\", \" \")\n",
        "    # in order to remove spaces before and after the word\n",
        "    word = word.split()\n",
        "    word = \" \".join(word)\n",
        "    return word\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_underscores(corpus)\n",
        "\n",
        "class CharacterRepetitionRemoverFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Reduces repetition to two characters \n",
        "  for alphabets and to one character for punctuations.\n",
        "\n",
        "  Examples\n",
        "  --------\n",
        "  >>> reducing_character_repetitions([[\"Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\"]])\n",
        "  Really, Great !?.;:)\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(CharacterRepetitionRemoverFlat, self).__init__()\n",
        "\n",
        "  def reducing_character_repetitions(self, corpus):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "      corpus : list of list of list\n",
        "    Returns\n",
        "    -------\n",
        "    list of list\n",
        "      Formatted text with alphabets repeating to \n",
        "      two characters & punctuations limited to one repetition \n",
        "\n",
        "    \"\"\"\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "        new_doc = [self._clean_repetitions(w) for w in doc]\n",
        "        new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  # inspired by https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a\n",
        "  def _clean_repetitions(self, word):\n",
        "    # Pattern matching for all case alphabets\n",
        "    pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "\n",
        "    # Limiting all the repetitions to two characters.\n",
        "    formatted_text = pattern_alpha.sub(r\"\\1\\1\", word) \n",
        "\n",
        "    # Pattern matching for all the punctuations that can occur\n",
        "    pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "\n",
        "    # Limiting punctuations in previously formatted string to only one.\n",
        "    combined_formatted = pattern_punct.sub(r'\\1', formatted_text)\n",
        "\n",
        "    # The below statement is replacing repetitions of spaces that occur more than two times with that of one occurrence.\n",
        "    final_formatted = re.sub(' {2,}',' ', combined_formatted)\n",
        "    return final_formatted\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.reducing_character_repetitions(corpus)\n",
        "\n",
        "class ApostrophesMergerFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Merges words like \"don't\" which in the original corpus are\n",
        "  separated like: [\"don\", \"'\", \"t\"]\n",
        "\n",
        "  Examples\n",
        "  --------\n",
        "  >>> am = ApostrophesMergerFlat()\n",
        "  >>> am([[\"I\", \"'\", \"ve\", \"a\", \"pair\", \"of\", \"shoes\"]])\n",
        "  [[\"I've\", \"a\", \"pair\", \"of\", \"shoes\"]]\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ApostrophesMergerFlat, self).__init__()\n",
        "\n",
        "  def merge_apostrophes(self, corpus):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : list of list of list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of list\n",
        "      Formatted text where contractions are merged into one single word\n",
        "    \n",
        "    \"\"\"\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      indexes = self._get_neg_indexes(doc)\n",
        "      for el in indexes:\n",
        "        doc[el[0]:el[1]] = [\"\".join(doc[el[0]:el[1]])]\n",
        "      new_corpus.append(doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def _get_neg_indexes(self, sent):\n",
        "\n",
        "    # s not considered because contraction can be either \"is\", genitive or \"has\"\n",
        "    contr = [\"t\", \"ve\", \"re\", \"ll\", \"d\", \"all\", \"y\", \"cause\", \"m\", \"clock\", \"am\"] #, \"s\"]\n",
        "    indexes = []\n",
        "    for idx, word in enumerate(sent):\n",
        "      # Try-except to avoid out of range indexes (there can be some \"'\" a the beginning or end of the phrase)\n",
        "      try:\n",
        "        if word==\"'\" and sent[idx+1] in contr:\n",
        "          indexes.append((idx-1,idx+2))\n",
        "      except:\n",
        "        pass\n",
        "    return indexes\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.merge_apostrophes(corpus)\n",
        "\n",
        "\n",
        "class ContractionCleanerFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Clean all contractions by using a predifined contraction map\n",
        "\n",
        "  Example\n",
        "  -------\n",
        "  >>> cc = ContractionCleanerFlat\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(ContractionCleanerFlat, self).__init__()\n",
        "\n",
        "  def clean_contractions(self, corpus):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : list of list of list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of list\n",
        "      Formatted text where contractions are merged into one single word\n",
        "\n",
        "    \"\"\"\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for word in doc:\n",
        "        try:\n",
        "            correct = CONTRACTION_MAP[word]\n",
        "            correct = correct.split()\n",
        "            new_doc += correct\n",
        "        except:\n",
        "            new_doc.append(word)\n",
        "      new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_contractions(corpus)\n",
        "\n",
        "class SpecialCharsCleanerFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Removes all special characters which are not part of\n",
        "  the folllowing regex pattern: \"[^a-zA-Z0-9:€$-,%?!]+\"\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(SpecialCharsCleanerFlat, self).__init__()\n",
        "\n",
        "  def clean_special_chars(self, corpus):\n",
        "    new_corpus = [[self._clean_special_word(w) for w in doc] for doc in corpus]\n",
        "    new_corpus = [[w for w in doc] for doc in corpus]\n",
        "    return new_corpus\n",
        "    \n",
        "  def _clean_special_word(self, word):\n",
        "    # The formatted text after removing not necessary punctuations.\n",
        "    formatted_text = re.sub(r\"[^a-zA-Z0-9:€$-,%?!]+\", '', word) \n",
        "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
        "    return formatted_text\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_special_chars(corpus)\n",
        "\n",
        "class StopWordsRemoverFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Removes stopwords from the document.\n",
        "  It doesn't remove stopwords that contain negations\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(StopWordsRemoverFlat, self).__init__()\n",
        "\n",
        "  def remove_stop_words(self, corpus):\n",
        "    stops = stopwords.words(\"english\")\n",
        "    stops = [word for word in stops if \"'t\" not in word or \"not\" not in word]\n",
        "    return [[word for word in doc if word not in stops] for doc in corpus]\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_stop_words(corpus)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# Non-flattened elements\n",
        "\"\"\"\n",
        "These items are the same as before with the only difference that now\n",
        "the assumption is that the corpus is not flattened, so each document\n",
        "is composed by serveral separated sentences:\n",
        "\n",
        "Example\n",
        "-------\n",
        "\n",
        "\"\"\"\n",
        "class UnderscoreRemover(PipelineElement):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(UnderscoreRemover, self).__init__()\n",
        "\n",
        "  def remove_underscores(self, corpus):\n",
        "    \"\"\"\n",
        "    Solves the problem where some of the words are surrounded by underscores\n",
        "    (e.g. \"_hello_\")\n",
        "    \"\"\"\n",
        "    for doc in corpus:\n",
        "      for sent_idx, sent in enumerate(doc):\n",
        "        new_sent = []\n",
        "        for idx, word in enumerate(sent):\n",
        "          if \"_\" in word:\n",
        "            cleaned_word = self._clean_word(word)\n",
        "            new_sent += cleaned_word\n",
        "          else:\n",
        "            new_sent.append(word)\n",
        "        if len(new_sent) > 0:\n",
        "          doc[sent_idx] = new_sent\n",
        "    return corpus\n",
        "\n",
        "  def _clean_word(self, word: str):\n",
        "    word = word.replace(\"_\", \" \")\n",
        "    # remove spaces before and after the word\n",
        "    word = word.split()\n",
        "    return word\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_underscores(corpus)\n",
        "\n",
        "\n",
        "\n",
        "class CharacterRepetitionRemover(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(CharacterRepetitionRemover, self).__init__()\n",
        "\n",
        "  def reducing_character_repetitions(self, corpus):\n",
        "      new_corpus = [[[self._clean_repetitions(w) for w in sent] for sent in doc] for doc in corpus]\n",
        "      return new_corpus\n",
        "      # inspired by https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a\n",
        "\n",
        "  def _clean_repetitions(self, word):\n",
        "    \"\"\"\n",
        "    This Function will reduce repetition to two characters \n",
        "    for alphabets and to one character for punctuations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        word: str                \n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Finally formatted text with alphabets repeating to \n",
        "        one characters & punctuations limited to one repetition \n",
        "        \n",
        "    Example:\n",
        "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
        "    Output : Really, Great !?.;:)\n",
        "\n",
        "    \"\"\"\n",
        "    # Pattern matching for all case alphabets\n",
        "    pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "\n",
        "    # Limiting all the repetitions to two characters.\n",
        "    # MODIFIED: keep only one repetition of the character\n",
        "    formatted_text = pattern_alpha.sub(r\"\\1\\1\", word) \n",
        "\n",
        "    # Pattern matching for all the punctuations that can occur\n",
        "    pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "\n",
        "    # Limiting punctuations in previously formatted string to only one.\n",
        "    combined_formatted = pattern_punct.sub(r'\\1', formatted_text)\n",
        "\n",
        "    # The below statement is replacing repetitions of spaces that occur more than two times with that of one occurrence.\n",
        "    final_formatted = re.sub(' {2,}',' ', combined_formatted)\n",
        "    return final_formatted\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.reducing_character_repetitions(corpus)\n",
        "\n",
        "\n",
        "class ApostrophesMerger(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(ApostrophesMerger, self).__init__()\n",
        "\n",
        "  def merge_apostrophes(self, corpus):\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for sent in doc:\n",
        "        indexes = self._get_neg_indexes(sent)\n",
        "        for el in indexes:\n",
        "          sent[el[0]:el[1]] = [\"\".join(sent[el[0]:el[1]])]\n",
        "        new_doc.append(sent)\n",
        "      new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def _get_neg_indexes(self, sent):\n",
        "    contr = [\"t\", \"ve\", \"re\", \"ll\", \"d\", \"all\", \"y\", \"cause\", \"m\", \"clock\", \"am\"]#, \"s\"]\n",
        "    indexes = []\n",
        "    for idx, word in enumerate(sent):\n",
        "      # Try-except to avoid out of range indexes (there can be some \"'\" a the beginning or end of the phrase)\n",
        "      try:\n",
        "        if word==\"'\" and sent[idx+1] in contr:\n",
        "          indexes.append((idx-1,idx+2))\n",
        "      except:\n",
        "        pass\n",
        "      return indexes\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.merge_apostrophes(corpus)\n",
        "\n",
        "\n",
        "class ContractionCleaner(PipelineElement):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ContractionCleaner, self).__init__()\n",
        "\n",
        "  def clean_contractions(self, corpus):\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for sent in doc:\n",
        "        new_sent = []\n",
        "        for word in sent:\n",
        "          try:\n",
        "            correct = CONTRACTION_MAP[word]\n",
        "            correct = correct.split()\n",
        "            new_sent += correct\n",
        "          except:\n",
        "            new_sent.append(word)\n",
        "        new_doc.append(new_sent)\n",
        "      new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_contractions(corpus)\n",
        "\n",
        "\n",
        "class SpecialCharsCleaner(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(SpecialCharsCleaner, self).__init__()\n",
        "  \n",
        "  def clean_special_chars(self, corpus):\n",
        "      for idx_doc, doc in enumerate(corpus):\n",
        "        for sent_idx, sent in enumerate(doc):\n",
        "          new_sent = []\n",
        "          for word_idx, word in enumerate(sent):\n",
        "            new_word = self._clean_special_word(word)\n",
        "            if new_word != \" \":\n",
        "              new_sent += new_word.split()\n",
        "          if len(new_sent) > 0:\n",
        "            doc[sent_idx] = new_sent\n",
        "      return corpus\n",
        "    \n",
        "  def _clean_special_word(self, word):\n",
        "    # The formatted text after removing not necessary punctuations.\n",
        "    formatted_text = re.sub(r\"[^a-zA-Z0-9:€$-,%?!]+\", ' ', word) \n",
        "    return formatted_text\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_special_chars(corpus)\n",
        "\n",
        "class StopWordsRemover(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(StopWordsRemover, self).__init__()\n",
        "\n",
        "  def remove_stop_words(self, corpus):\n",
        "    stops = stopwords.words(\"english\")\n",
        "    # Don't want to remove stop words associated with negations\n",
        "    stops = [word for word in stops if \"'t\" not in word or \"not\" not in word]\n",
        "    return [[[word for word in sent if word not in stops] for sent in doc] for doc in corpus]\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_stop_words(corpus)\n",
        "\n",
        "class ShallowObjectiveSentsRemover(PipelineElement):\n",
        "  def __init__(self, threshold = .5, clf = MultinomialNB, trained = False):\n",
        "    self.vectorizer = CountVectorizer()\n",
        "    self.classifier = clf()\n",
        "    if not trained:\n",
        "      self.best_estimator = self._train()\n",
        "    else:\n",
        "      self.best_estimator = self.classifier\n",
        "  \n",
        "  def _train(self):\n",
        "    subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "    obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
        "\n",
        "    corpus = [self.neg_marking_list2str(d) for d in subj] + [self.neg_marking_list2str(d) for d in obj]\n",
        "    vectors = self.vectorizer.fit_transform(corpus)\n",
        "    labels = np.array([1] * len(subj) + [0] * len(obj))\n",
        "    scores = cross_validate(self.classifier, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['accuracy'], return_estimator=True)\n",
        "    estimator = scores[\"estimator\"][scores[\"test_accuracy\"].argmax()]\n",
        "    return estimator\n",
        "\n",
        "  def neg_marking_list2str(self, sent):\n",
        "    # takes the doc and produces a single list\n",
        "    # negates the whole document\n",
        "    negated_doc = mark_negation(sent, double_neg_flip=True)\n",
        "    return \" \".join([w for w in negated_doc])\n",
        "    \n",
        "\n",
        "  def remove_objective_sents(self, corpus):\n",
        "    transformed_corpus = [[self.vectorizer.transform([self.neg_marking_list2str(sent)]) for sent in doc] for doc in corpus]\n",
        "    res = [[corpus[doc_idx][sent_idx] for sent_idx, sent in enumerate(doc) if self.best_estimator.predict(sent).item()]\n",
        "           for doc_idx, doc in enumerate(transformed_corpus)]\n",
        "    return res\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_objective_sents(corpus)\n",
        "\n",
        "class Flattener(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  \n",
        "  def flatten(self, corpus):\n",
        "    corpus = [[w for sent in doc for w in sent] for doc in corpus]\n",
        "    return corpus\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.flatten(corpus)\n"
      ],
      "metadata": {
        "id": "spYv4QqyqJ0H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skv2-rEQwMtR"
      },
      "source": [
        "### Corpus class\n",
        "I'm going to create a class for the representation of the corpus in order to have a self contained way to have all the information about corpus attributes (vocab, words ....)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C3PyvlOV60V7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "import numpy as np\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "\n",
        "class MovieReviewsCorpus():\n",
        "  def __init__(self, preprocess_pipeline = None):\n",
        "    # list of documents, each document is a list containing words of that document\n",
        "    self.mr = movie_reviews\n",
        "    self.pipeline = preprocess_pipeline\n",
        "    # Corpus as list of documents. Documents as list of sentences. Sentences as list of tokens\n",
        "    self.unprocessed_corpus, self.labels = self._get_corpus()\n",
        "    if preprocess_pipeline == None:\n",
        "      self.pipeline = Pipeline(Flattener())\n",
        "      self.processed_corpus = self._preprocess()\n",
        "    else:\n",
        "        # Flattened and preprocessed corpus\n",
        "      self.processed_corpus = self._preprocess()\n",
        "\n",
        "    self.corpus_words = self.get_corpus_words()\n",
        "    self.vocab = self._create_vocab()\n",
        "\n",
        "  def _preprocess(self):\n",
        "    return self.pipeline(self.unprocessed_corpus)\n",
        "\n",
        "  def _get_corpus(self):\n",
        "    neg = self.mr.paras(categories = \"neg\")\n",
        "    pos = self.mr.paras(categories = \"pos\")\n",
        "    labels = [0] * len(pos) + [1] * len(neg)\n",
        "    return neg + pos, labels\n",
        "\n",
        "  def movie_reviews_dataset_raw(self):\n",
        "    \"\"\"\n",
        "    Returns the dataset containing:\n",
        "\n",
        "    - A list of all the documents\n",
        "    - The corresponding label for each document\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple(list, list)\n",
        "        The dataset: first element is the list of the document, the second element of the tuple is the associated label (positive or negative) for each document\n",
        "    \"\"\"\n",
        "\n",
        "    return self.flattened_corpus, self.labels\n",
        "\n",
        "  def get_corpus_words(self) -> list:\n",
        "    \"\"\"\n",
        "    list of all the words in the corpus\n",
        "    \"\"\"\n",
        "    return [w for doc in self.processed_corpus for w in doc]\n",
        "  \n",
        "  def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        A 2D which each row has the corresponding embedding from the vocabulary\n",
        "    \"\"\"\n",
        "    matrix_length = len(self.vocab)\n",
        "    embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "    # If I use torch.zeros directly it crashes (don't know why)\n",
        "    embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "    null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "    for idx, key in enumerate(self.vocab.keys()):\n",
        "      if torch.equal(embedding[key], null_embedding):\n",
        "        embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "      else:\n",
        "        embedding_matrix[idx] = embedding[key]\n",
        "            \n",
        "    return embedding_matrix\n",
        "  \n",
        "  def get_fasttext_embedding_matrix(self, embedding, embedding_dim):\n",
        "      matrix_length = len(self.vocab)\n",
        "      embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "      # If I use torch.zeros directly it crashes (don't know why)\n",
        "      embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "      null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "        tensor_embedding = torch.from_numpy(embedding[key].copy())\n",
        "        if torch.equal(tensor_embedding, null_embedding):\n",
        "          embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "        else:\n",
        "          embedding_matrix[idx] = tensor_embedding\n",
        "            \n",
        "      return embedding_matrix\n",
        "  \n",
        "  def get_indexed_corpus(self):\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    Dictionary\n",
        "        Containing correspondences word -> index\n",
        "    \n",
        "    list(list(torch.tensor))\n",
        "        The corpus represented as indexes corresponding to each word\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    for idx, key in enumerate(self.vocab.keys()):\n",
        "      vocab[key] = idx\n",
        "    \n",
        "    indexed_corpus = [torch.tensor([torch.tensor(vocab[w], dtype=torch.int32) for w in doc]) for doc in self.processed_corpus]\n",
        "    return indexed_corpus, self.labels\n",
        "\n",
        "\n",
        "  def _create_vocab(self):\n",
        "    vocab = dict()\n",
        "    for word in self.corpus_words:\n",
        "      try:\n",
        "        vocab[word] += 1\n",
        "      except:\n",
        "        vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.processed_corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zZaF5yV9rlo-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class MovieReviewsDataset(Dataset):\n",
        "  def __init__(self, raw_dataset):\n",
        "    super(MovieReviewsDataset, self).__init__()\n",
        "    self.corpus = raw_dataset[0]\n",
        "    self.targets = raw_dataset[1]\n",
        "    self.max_element = len(max(self.corpus, key=lambda x: len(x)))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    item = self.corpus[index]\n",
        "    label = self.targets[index]\n",
        "    return (item, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr7PPG1k4gFq"
      },
      "source": [
        "### Create the model class\n",
        "Let's first try with a simple BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bCvqaJ8-hKmT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix = None, device = \"cuda\", input_size = 300, hidden_size = 128, output_size = 2):\n",
        "      super().__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.device = device\n",
        "      if embedding_matrix != None:\n",
        "        self.embedding = self.create_embedding_layer(embedding_matrix)\n",
        "      else:\n",
        "        self.embedding = None\n",
        "      self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True, bidirectional=True)\n",
        "      self.fc = nn.Sequential(nn.ReLU(),\n",
        "                              nn.BatchNorm1d(hidden_size*2, eps = 1e-08),\n",
        "                              nn.Dropout(0.2),\n",
        "                              nn.Linear(hidden_size*2, output_size)\n",
        "                              )\n",
        "\n",
        "    def create_embedding_layer(self, embedding_matrix):\n",
        "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "        emb_layer = nn.Embedding(num_embeddings, embedding_dim, -1)\n",
        "        emb_layer.load_state_dict({\"weight\": embedding_matrix})\n",
        "        return emb_layer\n",
        "\n",
        "    # function taken from https://discuss.pytorch.org/t/how-to-use-pack-sequence-if-we-are-going-to-use-word-embedding-and-bilstm/28184/4\n",
        "    def simple_elementwise_apply(self, fn, packed_sequence):\n",
        "        \"\"\"applies a pointwise function fn to each element in packed_sequence\"\"\"\n",
        "        return torch.nn.utils.rnn.PackedSequence(fn(packed_sequence.data), packed_sequence.batch_sizes)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        if self.cuda:\n",
        "            return (torch.zeros(2, batch_size, self.hidden_size).to(self.device),\n",
        "                    torch.zeros(2, batch_size, self.hidden_size).to(self.device),)\n",
        "\n",
        "    def common(self, x):\n",
        "      batch_size = x.batch_sizes[0].item()\n",
        "      hidden = self.init_hidden(batch_size)\n",
        "\n",
        "      if self.embedding != None:\n",
        "        x = self.simple_elementwise_apply(self.embedding, x)\n",
        "\n",
        "      # output: batch_size, sequence_length, hidden_size * 2 (since is bilstm)\n",
        "      out, _ = self.lstm(x, hidden)\n",
        "      out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "\n",
        "      return out, input_sizes\n",
        "\n",
        "    def forward(self, x):\n",
        "      batch_size = x.batch_sizes[0].item()\n",
        "      out, input_sizes = self.common(x)\n",
        "      # Interested only in the last layer\n",
        "      out = out[list(range(batch_size)), input_sizes - 1, :]\n",
        "      out = self.fc(out)\n",
        "      out = out.squeeze()\n",
        "      return out\n",
        "\n",
        "class Residual(nn.Module):\n",
        "  def __init__(self, in_size, out_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = nn.Sequential(nn.ReLU(),\n",
        "                              nn.Linear(in_size, in_size)\n",
        "                              )\n",
        "    self.fc2 = nn.Sequential(nn.ReLU(),\n",
        "                              nn.Linear(in_size, in_size),\n",
        "                              nn.ReLU()\n",
        "                              )\n",
        "\n",
        "    self.fc3 = nn.Linear(in_size, out_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    lay1 = self.fc1(x)\n",
        "    lay2 = self.fc2(lay1) + x\n",
        "    out = self.fc3(lay2)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class BiLSTMAttention(BiLSTM):\n",
        "  # BiLSTM with attention inspired by the following paper: https://aclanthology.org/S18-1040.pdf\n",
        "  def __init__(self, embedding_matrix = None, device=\"cuda\", input_size=300,\n",
        "                hidden_size=128, context_size = None, output_size=2):\n",
        "    super().__init__(embedding_matrix, device, input_size, hidden_size, output_size)\n",
        "    # Not self attention :)\n",
        "    if context_size:\n",
        "      self.attention = nn.Linear(self.hidden_size * 2, context_size)\n",
        "      self.history = nn.Parameter(torch.randn(context_size))\n",
        "    else:\n",
        "      self.attention = nn.Linear(self.hidden_size * 2, 1)\n",
        "      self.history = None\n",
        "    \n",
        "    self.fc = Residual(hidden_size*2, output_size)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out, input_sizes = super().common(x)\n",
        "\n",
        "    if self.history != None:\n",
        "      attention_values = torch.tanh(self.attention(out))\n",
        "      attention_weights = torch.softmax(attention_values.matmul(self.history), dim = 1).unsqueeze(1)\n",
        "      # n_docs, sequence_length\n",
        "    else:\n",
        "      attention_values = torch.tanh(self.attention(out)).squeeze(dim = 2)\n",
        "      attention_weights = torch.softmax(attention_values, dim = 1).unsqueeze(1)\n",
        "      # n_docs, sequence_length\n",
        "\n",
        "    out = torch.sum(attention_weights.matmul(out), dim = 1)\n",
        "\n",
        "    out = self.fc(out)\n",
        "\n",
        "    attention_weights = attention_weights.squeeze(dim = 1)\n",
        "    att = [doc[:input_sizes[idx]] for idx, doc in enumerate(attention_weights)]\n",
        "\n",
        "    return out, att\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yZFg9BvQdZHB"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for batch_idx, (inputs, targets, _) in enumerate(data_loader):\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    in_size = targets.size(dim=0)\n",
        "\n",
        "    outputs, _ = net(inputs)\n",
        "\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LJef4h1cfWym"
      },
      "outputs": [],
      "source": [
        "def test_step(net, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets, _) in enumerate(data_loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      in_size = targets.size(dim=0)\n",
        "\n",
        "      outputs, _ = net(inputs)\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = outputs.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OOeUmEHZNBvw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim import RAdam\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "def main(train_loader, test_loader, embedding_matrix, device = \"cuda\", epochs = 10):\n",
        "\n",
        "  net = BiLSTMAttention(embedding_matrix, device = device, input_size=300).to(device)\n",
        "\n",
        "  optimizer = Adam(net.parameters(), 0.001, betas = (0.9, 0.999), amsgrad=True)\n",
        "\n",
        "  cost_function = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Empirical result in this scenario:\n",
        "  # Even if I am using an adaptive lr, the schduler has been shown to guarantee\n",
        "  # a more stable convergence (more stable results across folds in k-fold)\n",
        "  scheduler = ExponentialLR(optimizer, 0.8)\n",
        "\n",
        "  flag = False\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    test_loss, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "    print(f\"Test loss: {test_loss} \\n Test accuracy: {test_accuracy}\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    if train_accuracy == 100:\n",
        "      if flag:\n",
        "        break\n",
        "      else:\n",
        "        flag = True\n",
        "    scheduler.step()\n",
        "  _, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "\n",
        "  return test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GfNtrS8jmATx"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def pad(batch: List[torch.tensor], max_size: int):\n",
        "  \"\"\"\n",
        "  Pads elements in the batch in order to have the same length,\n",
        "  that is the length of the longest element in the sequence\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  batch : list of nn.tensor\n",
        "    batch of elements. Each sequence of the batch can be either a tensor\n",
        "    containing indexes (i.e. [2 423 1 ... 123] where each number correspond to\n",
        "    one entry in a vocabulary)or can be a tensor containing directly the embeddings\n",
        "    (i.e. [[embedding_word1], [embedding_word2], ..., [embedding_wordn]])\n",
        "  \n",
        "  max_size : int\n",
        "    size of the longest sequence in the batch\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  list of torch.tensor\n",
        "    Batch where all elements are padded\n",
        "\n",
        "  \"\"\"\n",
        "  try:\n",
        "    pad = torch.tensor([-1]*batch[0].size(dim=1), dtype = torch.float).to(\"cuda\")\n",
        "    embedded = 1\n",
        "  except:\n",
        "    pad = torch.tensor([-1])\n",
        "    embedded = 0\n",
        "  for idx in range(len(batch)):\n",
        "      remaining = max_size - batch[idx].size(dim = 0)\n",
        "      abc = pad.repeat(remaining)\n",
        "      if embedded:\n",
        "        batch[idx] = torch.cat((batch[idx], pad.repeat(remaining, 1)), dim = 0)\n",
        "      else:\n",
        "        batch[idx] = torch.cat((batch[idx], pad.repeat(remaining)), dim = 0)\n",
        "  return batch\n",
        "\n",
        "def batch_to_tensor(X: List[torch.tensor], max_size):\n",
        "  \"\"\"\n",
        "  Transforms the entire batch into a tensor\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  X : list of torch.tensor\n",
        "    already padded batch\n",
        "\n",
        "  max_size : int\n",
        "    maximum size of the sequences\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  torch.tensor\n",
        "    Batch in tensor type\n",
        "  \"\"\"\n",
        "  try:\n",
        "    X_tensor = torch.zeros((len(X), max_size, X[0].size(dim=1)), dtype=torch.float).to(\"cuda\")\n",
        "  except:\n",
        "    X_tensor = torch.zeros((len(X), max_size), dtype=torch.int32)\n",
        "\n",
        "  for i, embed in enumerate(X):\n",
        "      X_tensor[i] = embed\n",
        "  return X_tensor\n",
        "\n",
        "def sort_ds(X, Y):\n",
        "  \"\"\"\n",
        "  Sort inputs by document lengths\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  X : list of torch.tensor\n",
        "    The batch\n",
        "  Y : list\n",
        "    Labels\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "    batch sorted, labels sorted (in order to keep correspondances),\n",
        "    document lengths sorted, indexes resulting from the argsort \n",
        "  \"\"\"\n",
        "  document_lengths = np.array([tens.size(dim = 0) for tens in X])\n",
        "  indexes = np.argsort(document_lengths)\n",
        "  document_lengths = document_lengths.tolist()\n",
        "\n",
        "  X_sorted = [X[idx] for idx in indexes][::-1]\n",
        "  Y_sorted = [Y[idx] for idx in indexes][::-1]\n",
        "  document_lengths = torch.tensor([document_lengths[idx] for idx in indexes][::-1])\n",
        "\n",
        "  return X_sorted, Y_sorted, document_lengths, indexes\n",
        "\n",
        "def collate(batch):\n",
        "  \"\"\"\n",
        "  collate function for batch of corpus\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "    packed sequence for the batch, tensor of labels, indexes for original\n",
        "    position of the elements (used for lbsa method)\n",
        "  \"\"\"\n",
        "  X, Y = list(zip(*batch))\n",
        "  # Sort dataset\n",
        "  X, Y, document_lengths, indexes = sort_ds(X, Y)\n",
        "\n",
        "  # Get tensor sizes\n",
        "  max_size = torch.max(document_lengths).item()\n",
        "\n",
        "  # Pad tensor each element\n",
        "  X = pad(X, max_size)\n",
        "\n",
        "  # Transform the batch to a tensor\n",
        "  X_tensor = batch_to_tensor(X, max_size)\n",
        "  Y_tensor = torch.tensor(Y)\n",
        "  # Return the padded sequence object\n",
        "  X_final = pack_padded_sequence(X_tensor, document_lengths, batch_first=True)\n",
        "  return X_final, Y_tensor, indexes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size: int, dataset, collate_fn, random_state = 42):\n",
        "  \"\"\"\n",
        "  Performs a stratified random split of the dataset using a 80/20 ratio.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "    training set data loader, test set data loader\n",
        "  \"\"\"\n",
        "  train_indexes, test_indexes = train_test_split(list(range(len(dataset.targets))), test_size = 0.2,\n",
        "                                                  stratify = dataset.targets, random_state = random_state)\n",
        "\n",
        "  train_ds = Subset(dataset, train_indexes)\n",
        "  test_ds = Subset(dataset, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "  test_loader = DataLoader(test_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "c-flsP1XPxxI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "def main_cross_validation(main_fn, dataset, embedding_matrix, collate_fn,\n",
        "                          device = \"cuda\", epochs = 20, random_state = 42, batch_size = 32):\n",
        "\n",
        "  targets = np.asarray(dataset.targets, dtype=np.int64)\n",
        "\n",
        "  skf = StratifiedKFold(10, shuffle = True, random_state=random_state)\n",
        "\n",
        "  fold_accuracies = []\n",
        "  \n",
        "  for fold, (train_indexes, val_indexes) in enumerate(skf.split(np.zeros(len(dataset)),\n",
        "                                                      targets)):\n",
        "    print(f\"\\n Fold: {fold}\")\n",
        "    train_sampler = SubsetRandomSampler(train_indexes)\n",
        "    val_sampler = SubsetRandomSampler(val_indexes)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size = batch_size, sampler = train_sampler,\n",
        "                              collate_fn = collate_fn, pin_memory=True)\n",
        "    val_loader = DataLoader(dataset, batch_size = batch_size, sampler = val_sampler,\n",
        "                            collate_fn = collate_fn, pin_memory = True)\n",
        "\n",
        "\n",
        "    val_accuracy = main_fn(train_loader, val_loader, embedding_matrix, device, epochs)\n",
        "    \n",
        "    fold_accuracies.append(val_accuracy)\n",
        "\n",
        "\n",
        "  fold_accuracies = np.array(fold_accuracies)\n",
        "\n",
        "  return fold_accuracies.mean(), fold_accuracies.std()\n",
        "\n"
      ],
      "metadata": {
        "id": "FkRBKES24_0v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lexicon Based Supervised Attention Model"
      ],
      "metadata": {
        "id": "MkeZyHH15r6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieReviewsCorpusLBSA():\n",
        "  def __init__(self, preprocess_pipeline = None):\n",
        "      \"\"\"\n",
        "      If non preprocess_pipeline is given, the text gets tokenized by default\n",
        "      using spacy tokenizer\n",
        "      \"\"\"\n",
        "      self.mr = movie_reviews\n",
        "      self.pipeline = preprocess_pipeline\n",
        "      self.raw_corpus, self.labels = self._get_raw_corpus()\n",
        "      if self.pipeline == None:\n",
        "          self.processed_corpus = self.raw_corpus\n",
        "      else:\n",
        "          self.processed_corpus = self._preprocess()\n",
        "      \n",
        "      self.vocab = self._create_vocab()\n",
        "      \n",
        "\n",
        "  def _get_raw_corpus(self):\n",
        "      neg = self.mr.paras(categories = \"neg\")\n",
        "      pos = self.mr.paras(categories = \"pos\")\n",
        "      labels = [0]*len(neg) + [1]*len(pos)\n",
        "      return neg + pos, labels\n",
        "  \n",
        "  def _preprocess(self):\n",
        "      if self.pipeline != None:\n",
        "          return self.pipeline(self.raw_corpus)\n",
        "      else:\n",
        "          return self.raw_corpus\n",
        "      \n",
        "  def _create_vocab(self):\n",
        "      vocab = dict()\n",
        "      corpus_words = [w for doc in self.processed_corpus for sent in doc for w in sent]\n",
        "      for word in corpus_words:\n",
        "          try:\n",
        "              vocab[word] += 1\n",
        "          except:\n",
        "              vocab[word] = 1\n",
        "      return vocab\n",
        "\n",
        "  def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      np.ndarray\n",
        "          A 2D which each row has the corresponding embedding from the vocabulary\n",
        "      \"\"\"\n",
        "      matrix_length = len(self.vocab)\n",
        "      embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "      # If I use torch.zeros directly it crashes (don't know why)\n",
        "      embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "      null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "          if torch.equal(embedding[key], null_embedding):\n",
        "              embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "          else:\n",
        "              embedding_matrix[idx] = embedding[key]\n",
        "              \n",
        "      return embedding_matrix\n",
        "  \n",
        "  def get_indexed_corpus(self):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      Dictionary\n",
        "          Containing correspondences word -> index\n",
        "      \n",
        "      list(int)\n",
        "          labels associated with each document\n",
        "      \"\"\"\n",
        "      vocab = {}\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "          vocab[key] = idx\n",
        "      \n",
        "      # each doc is a list of tensor which represent sentences, each sentence is a tensor of indexed words\n",
        "      indexed_corpus = [[torch.tensor([vocab[w] for w in sent], dtype=torch.int32) \n",
        "                        for sent in doc]\n",
        "                        for doc in self.processed_corpus]\n",
        "      return indexed_corpus, self.labels\n",
        "  \n",
        "  \n",
        "  \n",
        "  def __len__(self):\n",
        "      return len(self.processed_corpus)\n",
        "\n",
        "c = MovieReviewsCorpusLBSA()"
      ],
      "metadata": {
        "id": "n9gmVtdg_Gf1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "\n",
        "class MovieReviewsDatasetLBSA(Dataset):\n",
        "  def __init__(self, corpus):\n",
        "    super(MovieReviewsDatasetLBSA, self).__init__()\n",
        "    self.corpus = corpus\n",
        "    indexed_corpus = self.corpus.get_indexed_corpus()\n",
        "    # Word level gold attention vector\n",
        "    self.word_lambda = 3\n",
        "    self.sentence_lambda = 3\n",
        "    self.sentiment_degree = self._compute_sentiment_degree()\n",
        "    self.wl_gold_av = self._compute_gold_words()\n",
        "    self.sl_gold_av = self._compute_gold_sents()\n",
        "    self.data = indexed_corpus[0]\n",
        "    self.targets = indexed_corpus[1]\n",
        "  \n",
        "  def _compute_sentiment_degree(self):\n",
        "    senti_vocab = self._build_senti_vocab(self.corpus.vocab)\n",
        "    path = '/content/gdrive/My Drive/nlu-project/lexicons/'\n",
        "    mpqa_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'mpqa/mpqa.json')\n",
        "    bingliu_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'bingliu/bingliu.json')\n",
        "    inquirer_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'inquirer/inquirer.json')\n",
        "    concreteness_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'concreteness/concreteness.json')\n",
        "    twitter_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'twitter/twitter.json')\n",
        "    qwn_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'qwn/qwn.json')\n",
        "    social_sent = self._build_social_sent_vocab(self.corpus.vocab, path + 'social_sent')\n",
        "    senticnet_vocab = self.build_sentic_net_vocab(self.corpus.vocab, path + \"sentic_net/senticnet.txt\")\n",
        "    social_sent = self._build_social_sent_vocab(self.corpus.vocab, path + \"social_sent\")\n",
        "    res = self._compute_average_sentiment_degree(senti_vocab,\n",
        "                                                 mpqa_vocab,\n",
        "                                                 bingliu_vocab,\n",
        "                                                 inquirer_vocab,\n",
        "                                                 concreteness_vocab,\n",
        "                                                 twitter_vocab,\n",
        "                                                 qwn_vocab,\n",
        "                                                 senticnet_vocab,\n",
        "                                                 social_sent\n",
        "                                                 )\n",
        "    \n",
        "    corpus = self.corpus.processed_corpus\n",
        "    scores = [[[res[word] for word in sent] for sent in doc] for doc in corpus]\n",
        "    return scores\n",
        "\n",
        "  def _compute_gold_sents(self):\n",
        "    sentence_sentiment_degree  = [[sum(sent)/len(sent) for sent in doc] for doc in self.sentiment_degree]\n",
        "    gold = [self._normalized_softmax(doc, self.sentence_lambda) for doc in sentence_sentiment_degree]\n",
        "    return gold\n",
        "\n",
        "\n",
        "  def _compute_gold_words(self):\n",
        "    gold = [[self._normalized_softmax(sent_scores, self.word_lambda) for sent_scores in doc] for doc in self.sentiment_degree]\n",
        "    return gold\n",
        "\n",
        "  def _normalized_softmax(self, sequence, lam):\n",
        "    multiplied_sequence = [lam * el for el in sequence]\n",
        "    total = sum([math.exp(el) for el in sequence])\n",
        "    res = torch.tensor([math.exp(lam * el)/total for el in sequence])\n",
        "    return res\n",
        "\n",
        "  def _build_0_1_vocab(self, vocab, path):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/williamleif/socialsent/blob/master/socialsent/data/lexicons/mpqa.json\n",
        "\n",
        "    Values:\n",
        "    - 1 = positive\n",
        "    - 0 = neutral\n",
        "    - -1 = negative\n",
        "    \n",
        "    The absolute value will be taken\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "      lexicon = json.load(f)\n",
        "    \n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "    \n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = lexicon[key]\n",
        "        res_vocab[key] = abs(value)\n",
        "      except KeyError:\n",
        "        pass\n",
        "    \n",
        "    return res_vocab\n",
        "\n",
        "\n",
        "  def _build_senti_vocab(self, vocab):\n",
        "    \"\"\"\n",
        "    builds a vocab using senti-wordnet\n",
        "    \"\"\"\n",
        "    senti_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      senti_vocab[key] = 0\n",
        "\n",
        "    max_value = 0\n",
        "    for key in senti_vocab.keys():\n",
        "      senses = list(swn.senti_synsets(key))\n",
        "      pos = 0\n",
        "      neg = 0\n",
        "      for sense in senses:\n",
        "        if sense.synset.name().split(\".\")[0] == key:\n",
        "          pos += sense.pos_score()\n",
        "          neg += sense.neg_score()\n",
        "      if (pos != 0) or (neg != 0):\n",
        "        senti_vocab[key] = max(pos, neg)\n",
        "      if senti_vocab[key] > max_value:\n",
        "        max_value = senti_vocab[key]\n",
        "\n",
        "    # for key in senti_vocab.keys():\n",
        "      # senti_vocab[key] = self.maprange((0, max_value), (0, 1), senti_vocab[key])\n",
        "\n",
        "    return senti_vocab\n",
        "  \n",
        "  def build_sentic_net_vocab(self, vocab, path):\n",
        "\n",
        "    df = pd.read_csv(path, sep=\"\\t+\")\n",
        "\n",
        "    df.replace([\"negative\", \"positive\"], 1, inplace = True)\n",
        "    # df.set_index([\"CONCEPT\"], inplace = True)\n",
        "\n",
        "    df = dict(zip(df.CONCEPT, df.POLARITY))\n",
        "\n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "\n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = df[key]\n",
        "        res_vocab[key] = value\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    return res_vocab\n",
        "  \n",
        "  def _build_social_sent_vocab(self, vocab, path):\n",
        "    word_path = f\"{path}/frequent_words/\"\n",
        "    adj_path = f\"{path}/adjectives\"\n",
        "    word_files = [os.path.join(word_path, filename) for filename in os.listdir(word_path) if \".tsv\" in filename]\n",
        "    adj_files = [os.path.join(adj_path, filename) for filename in os.listdir(adj_path) if \".tsv\" in filename]\n",
        "\n",
        "    word_dfs = [pd.read_csv(f, sep = \"\\t\", names = [\"word\", \"mean\", \"std\"]) for f in word_files]\n",
        "    adj_dfs = [pd.read_csv(f, sep = \"\\t\", names = [\"word\", \"mean\", \"std\"]) for f in adj_files]\n",
        "\n",
        "    words = pd.read_csv(f\"{word_path}/2000.tsv\", sep = \"\\t\", names = [\"word\", \"mean\", \"std\"])\n",
        "    adjs = pd.read_csv(f\"{adj_path}/2000.tsv\", sep = \"\\t\", names = [\"word\", \"mean\", \"std\"])\n",
        "    tot = pd.concat([words, adjs])\n",
        "\n",
        "    tot = tot.drop(\"std\", axis = 1)\n",
        "    tot[\"mean\"] = tot[\"mean\"].abs()\n",
        "    tot.sort_values(by=[\"mean\"], inplace = True)\n",
        "    tot.drop_duplicates(subset = \"word\", keep=\"last\", inplace = True)\n",
        "\n",
        "    tot = dict(zip(tot[\"word\"], tot[\"mean\"]))\n",
        "\n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "\n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = tot[key]\n",
        "        res_vocab[key] = value\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    return res_vocab\n",
        "  \n",
        "  def maprange(self, a, b, s):\n",
        "    \"\"\"\n",
        "    Maps the number s from range a = [a1, a2] to range b = [b1, b2]\n",
        "    \"\"\"\n",
        "    # Source: https://rosettacode.org/wiki/Map_range#Python\n",
        "    (a1, a2), (b1, b2) = a, b\n",
        "    return  b1 + ((s - a1) * (b2 - b1) / (a2 - a1))\n",
        "  \n",
        "  def _compute_average_sentiment_degree(self, *args):\n",
        "    \"\"\"\n",
        "    Assumption: all arguments in args are dictionaries containing the same keys\n",
        "    and a numbers as value.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict\n",
        "      average of the sentiment degree across dictionaries for each word\n",
        "    \n",
        "    Example\n",
        "    -------\n",
        "    we have two dictionaries that give a sentiment degree to words:\n",
        "    a = {\"good\": 0.9, \"bad\": 0.7}\n",
        "    b = {\"good\": 0.5, \"bad\": 0.1}\n",
        "\n",
        "    result = {\"good\": 0.7, \"bad\": 0.4}\n",
        "    \"\"\"\n",
        "    n_args = len(args)\n",
        "    res = {}\n",
        "    for arg in args:\n",
        "      for key in arg.keys():\n",
        "        try:\n",
        "          res[key].append(arg[key])\n",
        "        except KeyError:\n",
        "          res[key] = []\n",
        "    for key in res.keys():\n",
        "      if len(res[key]) != 0:\n",
        "        res[key] = sum(res[key]) / len(res[key])\n",
        "      else:\n",
        "        res[key] = 0\n",
        "    \n",
        "    return res\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = self.data[index]\n",
        "    label = self.targets[index]\n",
        "    gold_word = self.wl_gold_av[index]\n",
        "    gold_sent = self.sl_gold_av[index]\n",
        "    return (item, label, gold_word, gold_sent)"
      ],
      "metadata": {
        "id": "8lGBLNBxo4Jv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand:\n",
        "- If it is better to introduce intermediate supervision\n",
        "\n",
        "- If it is better to use one hot encoding for the output\n",
        "\n",
        "- If I intepreted well the word-loss"
      ],
      "metadata": {
        "id": "IT_UVsevkmNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLBSA(BiLSTMAttention):\n",
        "    # Lexicon Based Supervised Attention model (LBSA) inspired by the following paper: https://aclanthology.org/C18-1074.pdf\n",
        "    def __init__(self, embedding_matrix, device=\"cuda\", input_size=300,\n",
        "                 hidden_size=128, context_size = 20, output_size=2):\n",
        "\n",
        "        super(EncoderLBSA, self).__init__(embedding_matrix, device, input_size, hidden_size, context_size, output_size)\n",
        "        self.fc = Residual(hidden_size*2, hidden_size)\n",
        "\n",
        "    # TODO: Pass the part inside for to super.forward()  \n",
        "    def forward(self, x):\n",
        "      att = []\n",
        "      res = []\n",
        "      for i, doc in enumerate(x):\n",
        "        doc = doc.to(self.device)\n",
        "        batch_size = doc.batch_sizes[0].item()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        doc = self.simple_elementwise_apply(self.embedding, doc)\n",
        "\n",
        "        out, _ = self.lstm(doc, hidden)\n",
        "        out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "        # n_sents, n_words_per_sent, hidden_size * 2 (since is bilstm)\n",
        "\n",
        "\n",
        "        attention_values = torch.tanh(self.attention(out))\n",
        "        # n_sents, n_words_per_sent, context_size\n",
        "\n",
        "        attention_weights = torch.softmax(attention_values.matmul(self.history), dim = 1).unsqueeze(1)\n",
        "        # n_sents, n_words_per_sent\n",
        "\n",
        "        out = torch.sum(attention_weights.matmul(out), dim = 1)\n",
        "        # n_sents, hidden*2\n",
        "\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        attention_weights = attention_weights.squeeze(dim=1)\n",
        "\n",
        "        att.append([sent[:input_sizes[idx]] for idx, sent in enumerate(attention_weights)])\n",
        "\n",
        "        res.append(out)\n",
        "      # n_doc, seq_lengths, hidden * 2\n",
        "      return res, att"
      ],
      "metadata": {
        "id": "k_dniRPY5q2A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sortLBSA(X, w_gold, s_gold):\n",
        "\n",
        "  sentence_lengths = [np.array([sent.size(dim=0) for sent in doc]) for doc in X]\n",
        "  indexes = [np.argsort(doc) for doc in sentence_lengths]\n",
        "  indexes = [el.tolist() for el in indexes]\n",
        "\n",
        "  X_sorted = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(X)]\n",
        "  # w_gold = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(w_gold)]#\n",
        "  # s_gold = [torch.tensor([doc[idx2] for idx2 in indexes[idx]][::-1]) for idx, doc in enumerate(s_gold)]#\n",
        "  sentence_lengths = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(sentence_lengths)]\n",
        "\n",
        "  return X_sorted, w_gold, s_gold, sentence_lengths, indexes\n",
        "\n",
        "def padLBSA(batch, max_sizes):\n",
        "    pad = torch.tensor([-1])\n",
        "    for idx1, doc in enumerate(batch):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        remaining = max_sizes[idx1] - sent.size(dim = 0)\n",
        "        batch[idx1][idx2] = torch.cat((sent, pad.repeat(remaining)), dim = 0)\n",
        "    return batch\n",
        "\n",
        "def to_tensorLBSA(batch, max_sizes):\n",
        "  res = []\n",
        "  for idx, doc in enumerate(batch):\n",
        "    buff = torch.zeros(len(doc), max_sizes[idx], dtype=torch.int32)\n",
        "    for idx2, sent in enumerate(doc):\n",
        "      buff[idx2] = sent\n",
        "\n",
        "    res.append(buff)\n",
        "  return res\n",
        "\n",
        "\n",
        "def collateLBSA(batch):\n",
        "  X, Y, w_gold, s_gold = list(zip(*batch))\n",
        "\n",
        "  X, w_gold, s_gold, sentence_lengths, indexes = sortLBSA(X, w_gold, s_gold)\n",
        "  # can take doc[0] since senetence_lengths is sorted\n",
        "  max_sizes = [doc[0] for doc in sentence_lengths]\n",
        "\n",
        "  # Pad tensor each element\n",
        "  X = padLBSA(X, max_sizes)\n",
        "  # Transform the batch to a tensor\n",
        "  X = to_tensorLBSA(X, max_sizes)\n",
        "\n",
        "  # Return the padded sequence object\n",
        "  X = [pack_padded_sequence(doc, sentence_lengths[idx], batch_first=True) for idx, doc in enumerate(X)]\n",
        "  return X, Y, w_gold, s_gold, indexes"
      ],
      "metadata": {
        "id": "JyjRmkGAIu94"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def element_wise_log_loss(out, labels):\n",
        "  res = - out.log().mul(labels).sum(dim=0)\n",
        "  return res\n",
        "\n",
        "def loss_LBSA(outputs, targets, mu_w = 0.0005, mu_s = 0.025):\n",
        "  dec_output, w_att, s_att = outputs\n",
        "  target, w_gold, s_gold = targets\n",
        "\n",
        "  total_loss = 0\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "\n",
        "  cross_loss = ce(dec_output, target)\n",
        "  total_loss += cross_loss\n",
        "\n",
        "  # pensare a fare uno scaling della loss se è molto diversa dalla cross entropy\n",
        "\n",
        "  w_loss = torch.mean(torch.tensor([\n",
        "    torch.sum(torch.tensor([\n",
        "        element_wise_log_loss(w_att[idx1][idx2], sent) for idx2, sent in enumerate(doc)\n",
        "    ])) * mu_w for idx1, doc in enumerate(w_gold)\n",
        "  ]))\n",
        "  total_loss += w_loss\n",
        "\n",
        "  s_loss = torch.mean(torch.tensor([\n",
        "      element_wise_log_loss(s_att[idx], doc) * mu_s for idx, doc in enumerate(s_gold)\n",
        "  ]))\n",
        "  total_loss += s_loss\n",
        "\n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "VXw9lvmaWM3B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_LBSA(encoder, decoder, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "\n",
        "  for batch_idx, (inputs, target, w_gold, s_gold, _) in enumerate(data_loader):\n",
        "    \n",
        "    in_size = len(target)\n",
        "\n",
        "    enc_output, w_att = encoder(inputs)\n",
        "    \n",
        "    batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "    dec_input, target, indexes = collate(batch)\n",
        "    \n",
        "    s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "\n",
        "    target = target.to(device)\n",
        "    for idx1, doc in enumerate(w_gold):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "    for idx, doc in enumerate(s_gold):\n",
        "       s_gold[idx] = doc.to(device)\n",
        "    \n",
        "\n",
        "    dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "    outputs = (dec_output, w_att, s_att)\n",
        "    targets = (target, w_gold, s_gold)\n",
        "\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "yZF4uo99VKsh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_LBSA(encoder, decoder, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, target, w_gold, s_gold, _) in enumerate(data_loader):\n",
        "      in_size = len(target)\n",
        "\n",
        "      enc_output, w_att = encoder(inputs)\n",
        "      \n",
        "      batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "      dec_input, target, indexes = collate(batch)\n",
        "\n",
        "      s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "      # Not sorting also w_gold becuse in the encoder, documents don't get shuffled\n",
        "\n",
        "      target = target.to(device)\n",
        "      for idx1, doc in enumerate(w_gold):\n",
        "        for idx2, sent in enumerate(doc):\n",
        "          w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "      for idx, doc in enumerate(s_gold):\n",
        "        s_gold[idx] = doc.to(device)\n",
        "\n",
        "      dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "      outputs = (dec_output, w_att, s_att)\n",
        "      targets = (target, w_gold, s_gold)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "      \n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "RpyI0qRvC0cF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting must be a problem, otherwise word_level attention doesn't get the correct supervision.\n",
        "Same goes for sentences"
      ],
      "metadata": {
        "id": "ntMCKJDtuEnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_LBSA_new(encoder, decoder, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "\n",
        "  for batch_idx, (inputs, target, w_gold, s_gold, sent_indexes) in enumerate(data_loader):\n",
        "    \n",
        "    in_size = len(target)\n",
        "\n",
        "    enc_output, w_att = encoder(inputs)\n",
        "    # Sorting the sentences of the encoder to their original position\n",
        "    # Inverting the output because indexes are in ascending order, output is in descending\n",
        "    enc_output = [torch.flip(enc_output[doc_idx], dims = [0]) for doc_idx, _ in enumerate(enc_output)]\n",
        "    w_att = [w_att[doc_idx][::-1] for doc_idx, _ in enumerate(w_att)]\n",
        "    # Using argsort on the indexes reverses the previous argsort\n",
        "    inverted_indexes = [np.argsort(np.array(doc)) for doc in sent_indexes]\n",
        "    inverted_indexes = [el.tolist() for el in inverted_indexes]\n",
        "    # Sort the sentences with original sorting:\n",
        "    # n_doc, n_sents, hidden*2\n",
        "\n",
        "    enc_output = [enc_output[doc_idx][sent_idx] for doc_idx, sent_idx in enumerate(inverted_indexes)]\n",
        "    w_att = [[doc[idx2] for idx2 in inverted_indexes[idx]] for idx, doc in enumerate(w_att)]\n",
        "\n",
        "    \n",
        "    batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "    dec_input, target, indexes = collate(batch)\n",
        "    \n",
        "    target = target.to(device)\n",
        "    for idx1, doc in enumerate(w_gold):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "    for idx, doc in enumerate(s_gold):\n",
        "       s_gold[idx] = doc.to(device)\n",
        "    \n",
        "\n",
        "    dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "    dec_output = torch.flip(dec_output, dims = [0])\n",
        "    s_att = s_att[::-1]\n",
        "    target = torch.flip(target, dims = [0])\n",
        "\n",
        "\n",
        "    inverted_indexes = np.argsort(np.array(indexes))\n",
        "    inverted_indexes = inverted_indexes.tolist()\n",
        "    # Sort the sentences with original sorting:\n",
        "    # n_doc, n_sents, hidden*2\n",
        "    dec_output = dec_output[inverted_indexes]\n",
        "    s_att = [s_att[idx] for idx in inverted_indexes]\n",
        "    target = target[inverted_indexes]\n",
        "\n",
        "\n",
        "    outputs = (dec_output, w_att, s_att)\n",
        "    targets = (target, w_gold, s_gold)\n",
        "    \n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "RN2AM7tLgIDm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_LBSA_new(encoder, decoder, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, target, w_gold, s_gold, sent_indexes) in enumerate(data_loader):\n",
        "      in_size = len(target)\n",
        "\n",
        "      enc_output, w_att = encoder(inputs)\n",
        "\n",
        "      # First flip (go to ascending order thats because in the dataloader collate\n",
        "      # orders sentences in descending order)\n",
        "      enc_output = [torch.flip(enc_output[doc_idx], dims = [0]) for doc_idx, _ in enumerate(enc_output)]\n",
        "      w_att = [w_att[doc_idx][::-1] for doc_idx, _ in enumerate(w_att)]\n",
        "\n",
        "      # Second take indexes for getting the original positions\n",
        "      # Using argsort on the indexes reverses the previous argsort\n",
        "      inverted_indexes = [np.argsort(np.array(doc)) for doc in sent_indexes]\n",
        "      inverted_indexes = [el.tolist() for el in inverted_indexes]\n",
        "\n",
        "      # Third Sort the sentences with original sorting:\n",
        "      # n_doc, n_sents, hidden*2\n",
        "      enc_output = [enc_output[doc_idx][sent_idx] for doc_idx, sent_idx in enumerate(inverted_indexes)]\n",
        "      w_att = [[doc[idx2] for idx2 in inverted_indexes[idx]] for idx, doc in enumerate(w_att)]\n",
        "      \n",
        "      # Create batch\n",
        "      batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "      dec_input, target, indexes = collate(batch)\n",
        "\n",
        "      # Not sorting also w_gold becuse in the encoder, documents don't get ordered inside collate\n",
        "\n",
        "      # Send w_gold to device\n",
        "      target = target.to(device)\n",
        "      for idx1, doc in enumerate(w_gold):\n",
        "        for idx2, sent in enumerate(doc):\n",
        "          w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "      # Send s_gold to device\n",
        "      for idx, doc in enumerate(s_gold):\n",
        "        s_gold[idx] = doc.to(device)\n",
        "\n",
        "      dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "\n",
        "      dec_output = torch.flip(dec_output, dims = [0])\n",
        "      s_att = s_att[::-1]\n",
        "      target = torch.flip(target, dims = [0])\n",
        "\n",
        "      inverted_indexes = np.argsort(np.array(indexes))\n",
        "      inverted_indexes = inverted_indexes.tolist()\n",
        "      # Sort the sentences with original sorting:\n",
        "      # n_doc, n_sents, hidden*2\n",
        "      dec_output = dec_output[inverted_indexes]\n",
        "      s_att = [s_att[idx] for idx in inverted_indexes]\n",
        "      target = target[inverted_indexes]\n",
        "\n",
        "      # Not sorting also s_gold becuse in the encoder, documents don't get ordered inside collate\n",
        "\n",
        "      outputs = (dec_output, w_att, s_att)\n",
        "      targets = (target, w_gold, s_gold)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "      \n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "kEf6kIOghmBm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim import RAdam\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def main_LBSA(train_loader, test_loader, embedding_matrix, device = \"cuda\", epochs = 10):\n",
        "  encoder = EncoderLBSA(embedding_matrix = embedding_matrix, device = device, input_size=300, hidden_size=100).to(device)\n",
        "  decoder = BiLSTMAttention(device = device, input_size = 100, context_size = 20).to(device)\n",
        "\n",
        "  optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), 0.001, betas = (0.9, 0.999), amsgrad=True)\n",
        "\n",
        "  scheduler = ExponentialLR(optimizer, 0.8)\n",
        "\n",
        "  cost_function = loss_LBSA\n",
        "\n",
        "  flag = False\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step_LBSA_new(encoder, decoder, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    test_loss, test_accuracy = test_step_LBSA_new(encoder, decoder, test_loader, cost_function, device)\n",
        "    print(f\"Test loss: {test_loss} \\n Test accuracy: {test_accuracy}\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    # Model has converged, so no need to go ahead\n",
        "    if train_accuracy == 100:\n",
        "      if flag == True:\n",
        "        break\n",
        "      else:\n",
        "        flag = True\n",
        "    scheduler.step()\n",
        "\n",
        "  return test_accuracy\n"
      ],
      "metadata": {
        "id": "3Xzpaa7IDN9Z"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polarity Tests\n",
        "### Shallow baseline"
      ],
      "metadata": {
        "id": "gxwWvbEqJoZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from sklearn.decomposition import PCA\n",
        "from nltk.corpus import subjectivity"
      ],
      "metadata": {
        "id": "G26_d58UJxf7"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neg_marking_list2str(sent):\n",
        "  # takes the doc and produces a single list\n",
        "  # negates the whole document\n",
        "  negated_doc = mark_negation(sent, double_neg_flip=True)\n",
        "  return \" \".join([w for w in negated_doc])"
      ],
      "metadata": {
        "id": "sFb5SwmrJ3ND"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
        "\n",
        "corpus = subj + obj\n",
        "\n",
        "corpus = [neg_marking_list2str(d) for d in corpus]\n",
        "vectors = vectorizer.fit_transform(corpus)\n",
        "labels = numpy.array([1] * len(subj) + [0] * len(obj))\n",
        "scores = cross_validate(classifier, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['accuracy'], return_estimator=True)\n",
        "# Taking the best estimator in accuracy\n",
        "pred = scores[\"estimator\"][scores[\"test_accuracy\"].argmax()]"
      ],
      "metadata": {
        "id": "VAYyh_cRKBxd"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_objective_sents(vectorizer, estimator, corpus):\n",
        "  transformed_corpus = [[vectorizer.transform([neg_marking_list2str(sent)]) for sent in doc] for doc in corpus]\n",
        "  res = [[corpus[doc_idx][sent_idx] for sent_idx, sent in enumerate(doc) if estimator.predict(sent).item()]\n",
        "          for doc_idx, doc in enumerate(transformed_corpus)]\n",
        "  return res"
      ],
      "metadata": {
        "id": "hIBGJS_xKNXi"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neg_marking_subj(doc):\n",
        "  # takes the doc and produces a single list\n",
        "  flattened_doc = [w for sent in doc for w in sent]\n",
        "  # negates the whole document\n",
        "  negated_doc = mark_negation(flattened_doc, double_neg_flip=True)\n",
        "  return \" \".join([w for w in negated_doc])"
      ],
      "metadata": {
        "id": "eW1aXfRrLESK"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mr = movie_reviews\n",
        "neg = mr.paras(categories = \"neg\")\n",
        "pos = mr.paras(categories = \"pos\")\n",
        "mr_corpus = pos + neg\n",
        "\n",
        "\n",
        "mr_corpus = remove_objective_sents(vectorizer, pred, mr_corpus)\n",
        "mr_corpus = [neg_marking_subj(d) for d in mr_corpus]"
      ],
      "metadata": {
        "id": "JRShQiEOQXeK"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = vectorizer.fit_transform(mr_corpus)\n",
        "labels = numpy.array([0] * len(pos) + [1] * len(neg))\n",
        "\n",
        "# Redefine vectorizer and classifier since already used for subjectivity\n",
        "# classifier = SVC(kernel = \"linear\")"
      ],
      "metadata": {
        "id": "b-aieBxAKuiw"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_validate(classifier, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['accuracy'], return_estimator=True)\n",
        "average = sum(scores['test_accuracy'])/len(scores['test_accuracy'])\n",
        "print(round(average, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q18zrgMSLeol",
        "outputId": "66c55376-afd3-46db-f561-bafe56386c61"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Models (FastText Emebdding)\n",
        "In this section I am going to analyze perfomances of deep models using fast-text embedding\n",
        "\n",
        "\n",
        "#### BiLSTM with attention mechanism\n",
        "The first model I am going to test is the BiLSTM with attention"
      ],
      "metadata": {
        "id": "qQfY8Ci2J3kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import FastText\n",
        "fast_text = FastText('en', cache = \"/content/gdrive/My Drive/nlu-project/Embeddings/.vector_cache\")"
      ],
      "metadata": {
        "id": "mtCHBADYUXu1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mr_pipeline = Pipeline(UnderscoreRemover(),\n",
        "                       CharacterRepetitionRemover(),\n",
        "                       ApostrophesMerger(),\n",
        "                       ContractionCleaner(),\n",
        "                       SpecialCharsCleaner(),\n",
        "                       Flattener()\n",
        "                      )\n",
        "mr_corpus = MovieReviewsCorpus(mr_pipeline)\n",
        "mr_embedding_matrix = mr_corpus.get_embedding_matrix(fast_text, 300)\n",
        "mr_dataset = MovieReviewsDataset(mr_corpus.get_indexed_corpus())"
      ],
      "metadata": {
        "id": "TYNaA-pJUgQn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Smaller batch sizes are noisy, this means that they are more regularizing and the\n",
        "# generalization error will be lower\n",
        "mean, std = main_cross_validation(main, mr_dataset, mr_embedding_matrix, collate, epochs = 10, batch_size=32)\n",
        "# 82 +- 3 using 128 as bsize\n",
        "# 85.6 +- 3.6 using 32 as bsize\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ],
      "metadata": {
        "id": "C9AyWMOqW9gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -1 adds the element in the penultimate position\n",
        "mr_pipeline.add_pipeline_element(ShallowObjectiveSentsRemover(), -1)\n",
        "mr_corpus = MovieReviewsCorpus(mr_pipeline)\n",
        "mr_embedding_matrix = mr_corpus.get_embedding_matrix(fast_text, 300)\n",
        "mr_dataset = MovieReviewsDataset(mr_corpus.get_indexed_corpus())"
      ],
      "metadata": {
        "id": "xUphVp6zwkqF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = main_cross_validation(main, mr_dataset, mr_embedding_matrix, collate, epochs = 10, batch_size=32)\n",
        "# 87.95 +- 1.45\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nYyoqlKFxsr",
        "outputId": "8f3ad0ce-ae6b-4674-f8bc-2835e2d73945"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold: 0\n",
            "epoch 0:\n",
            "Training loss: 0.021671149101522232 \n",
            " Training accuracy: 53.833333333333336\n",
            "Test loss: 0.021594336926937102 \n",
            " Test accuracy: 66.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.016298895469970175 \n",
            " Training accuracy: 76.5\n",
            "Test loss: 0.016355734765529633 \n",
            " Test accuracy: 77.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.009472410513295068 \n",
            " Training accuracy: 87.6111111111111\n",
            "Test loss: 0.013666114658117295 \n",
            " Test accuracy: 82.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.005179130551922652 \n",
            " Training accuracy: 94.38888888888889\n",
            "Test loss: 0.010311165153980255 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0021231775113847106 \n",
            " Training accuracy: 98.33333333333333\n",
            "Test loss: 0.012304572686553002 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0008688287857835853 \n",
            " Training accuracy: 99.33333333333333\n",
            "Test loss: 0.01505537498742342 \n",
            " Test accuracy: 90.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0004849022262189667 \n",
            " Training accuracy: 99.55555555555556\n",
            "Test loss: 0.012926727496087552 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.00024413818476508217 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.015420963284559547 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 7.113591433153487e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.023588030748069286 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 4.8996127498968336e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.02394921310245991 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 1\n",
            "epoch 0:\n",
            "Training loss: 0.021477518992291555 \n",
            " Training accuracy: 54.833333333333336\n",
            "Test loss: 0.022925446331501006 \n",
            " Test accuracy: 65.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.013824394303891393 \n",
            " Training accuracy: 78.83333333333333\n",
            "Test loss: 0.016284294426441193 \n",
            " Test accuracy: 76.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.00670582190181853 \n",
            " Training accuracy: 92.27777777777779\n",
            "Test loss: 0.01270234614610672 \n",
            " Test accuracy: 83.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.0037037708506815962 \n",
            " Training accuracy: 95.88888888888889\n",
            "Test loss: 0.01513368234038353 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.001286829076707363 \n",
            " Training accuracy: 98.61111111111111\n",
            "Test loss: 0.028391642272472383 \n",
            " Test accuracy: 84.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0009071487946918448 \n",
            " Training accuracy: 99.16666666666667\n",
            "Test loss: 0.035991793274879454 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0006077895712082358 \n",
            " Training accuracy: 99.3888888888889\n",
            "Test loss: 0.06330634128302336 \n",
            " Test accuracy: 82.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.0003222302198224093 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.04322438254952431 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 2.6128965819099096e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.040593757927417755 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 1.8798945501935133e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.04240192631259561 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 2\n",
            "epoch 0:\n",
            "Training loss: 0.021295823454856873 \n",
            " Training accuracy: 55.44444444444444\n",
            "Test loss: 0.020690842866897582 \n",
            " Test accuracy: 69.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.01415598457886113 \n",
            " Training accuracy: 80.22222222222221\n",
            "Test loss: 0.018300453275442122 \n",
            " Test accuracy: 79.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.006114240746117301 \n",
            " Training accuracy: 92.61111111111111\n",
            "Test loss: 0.021760514825582503 \n",
            " Test accuracy: 80.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.003296009348964112 \n",
            " Training accuracy: 96.05555555555556\n",
            "Test loss: 0.012410658225417138 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.001000742691038694 \n",
            " Training accuracy: 98.88888888888889\n",
            "Test loss: 0.018004329726099968 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0007771490333753819 \n",
            " Training accuracy: 99.22222222222223\n",
            "Test loss: 0.020070473551750182 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.000154844341961709 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.024661915153265 \n",
            " Test accuracy: 84.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 6.126085384999492e-05 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.03387901075184345 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 2.7442844278387686e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.029084986227389892 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 1.7441220232902547e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.031608715364709494 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 3\n",
            "epoch 0:\n",
            "Training loss: 0.021047623654206593 \n",
            " Training accuracy: 60.27777777777777\n",
            "Test loss: 0.01829788625240326 \n",
            " Test accuracy: 77.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.014536905744009548 \n",
            " Training accuracy: 78.83333333333333\n",
            "Test loss: 0.012950645014643668 \n",
            " Test accuracy: 79.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.006817207903497749 \n",
            " Training accuracy: 91.61111111111111\n",
            "Test loss: 0.009674328528344631 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.002614443841804233 \n",
            " Training accuracy: 97.16666666666667\n",
            "Test loss: 0.010954872481524944 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0011387231220982763 \n",
            " Training accuracy: 98.88888888888889\n",
            "Test loss: 0.030693608671426773 \n",
            " Test accuracy: 82.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0005631321094551823 \n",
            " Training accuracy: 99.5\n",
            "Test loss: 0.018510869517922402 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 8.283255808338355e-05 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.022217783480882644 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 3.3491397629808896e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.019187418803267065 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 2.0569394743874783e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.019675144109642134 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 4\n",
            "epoch 0:\n",
            "Training loss: 0.021239286892943913 \n",
            " Training accuracy: 56.611111111111114\n",
            "Test loss: 0.023708061277866364 \n",
            " Test accuracy: 63.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.018525074538257386 \n",
            " Training accuracy: 73.27777777777777\n",
            "Test loss: 0.017730089873075484 \n",
            " Test accuracy: 76.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.010929043224702279 \n",
            " Training accuracy: 86.0\n",
            "Test loss: 0.016947838962078094 \n",
            " Test accuracy: 79.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.004316658127483808 \n",
            " Training accuracy: 95.0\n",
            "Test loss: 0.01429269939661026 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0016761517167065499 \n",
            " Training accuracy: 98.38888888888889\n",
            "Test loss: 0.02107157755177468 \n",
            " Test accuracy: 82.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.000982585157245113 \n",
            " Training accuracy: 99.22222222222223\n",
            "Test loss: 0.01650295827537775 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0006558757460637328 \n",
            " Training accuracy: 99.27777777777777\n",
            "Test loss: 0.01661156365167699 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.0005126240255776792 \n",
            " Training accuracy: 99.3888888888889\n",
            "Test loss: 0.020864431709051133 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.00016380929223133636 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.02518298384402442 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.00010008300449751712 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.03648498892784119 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 5\n",
            "epoch 0:\n",
            "Training loss: 0.02180401517285241 \n",
            " Training accuracy: 53.666666666666664\n",
            "Test loss: 0.023380974233150484 \n",
            " Test accuracy: 56.99999999999999\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.016535338047477935 \n",
            " Training accuracy: 73.88888888888889\n",
            "Test loss: 0.014066781550645828 \n",
            " Test accuracy: 81.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.00880651923103465 \n",
            " Training accuracy: 89.05555555555556\n",
            "Test loss: 0.012216739133000374 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.0031435938961739238 \n",
            " Training accuracy: 96.5\n",
            "Test loss: 0.01763928133994341 \n",
            " Test accuracy: 84.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0014560674525435186 \n",
            " Training accuracy: 98.61111111111111\n",
            "Test loss: 0.016124790981411934 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0006528377817853147 \n",
            " Training accuracy: 99.33333333333333\n",
            "Test loss: 0.012369415909051894 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.00030193680249289575 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.013264205185550963 \n",
            " Test accuracy: 90.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.00010332411040331459 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.02169521383009851 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.00016457700098019106 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.01723711421713233 \n",
            " Test accuracy: 90.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 3.110987092289482e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.02343404121696949 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 6\n",
            "epoch 0:\n",
            "Training loss: 0.02083094467719396 \n",
            " Training accuracy: 59.88888888888889\n",
            "Test loss: 0.020639576315879822 \n",
            " Test accuracy: 66.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.015180839995543162 \n",
            " Training accuracy: 78.61111111111111\n",
            "Test loss: 0.015032750517129899 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.006978012199979276 \n",
            " Training accuracy: 91.88888888888889\n",
            "Test loss: 0.010571482214145363 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.0026547081568019672 \n",
            " Training accuracy: 97.38888888888889\n",
            "Test loss: 0.014424393121153116 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0011159005887909897 \n",
            " Training accuracy: 98.88888888888889\n",
            "Test loss: 0.030611815750598906 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0005077766201084286 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.021245350316166877 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.00027359012764160674 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.02037049040198326 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.00011813467524714522 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.027714759409427643 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 3.0995867207618884e-05 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.01980816391784174 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 3.077787815729809e-05 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.02280018988640222 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 7\n",
            "epoch 0:\n",
            "Training loss: 0.021765923864311643 \n",
            " Training accuracy: 52.44444444444445\n",
            "Test loss: 0.02311688244342804 \n",
            " Test accuracy: 53.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.014749346151947975 \n",
            " Training accuracy: 77.27777777777777\n",
            "Test loss: 0.029238054752349852 \n",
            " Test accuracy: 67.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.0073326303478744295 \n",
            " Training accuracy: 91.33333333333333\n",
            "Test loss: 0.015408286303281784 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.00390320908329967 \n",
            " Training accuracy: 95.94444444444444\n",
            "Test loss: 0.016990032494068146 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0014968856128851056 \n",
            " Training accuracy: 98.72222222222223\n",
            "Test loss: 0.0162413268169621 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0008198564924532547 \n",
            " Training accuracy: 99.16666666666667\n",
            "Test loss: 0.022482794374227524 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0004200288895611632 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.022570774778723716 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.0002686855065985583 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.024078410365618766 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.00010011907265127067 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.027213472574949264 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 7.081303161612595e-05 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.027574648648733272 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 8\n",
            "epoch 0:\n",
            "Training loss: 0.021738553080293866 \n",
            " Training accuracy: 56.111111111111114\n",
            "Test loss: 0.024897281974554063 \n",
            " Test accuracy: 59.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.01630184117290709 \n",
            " Training accuracy: 77.0\n",
            "Test loss: 0.018149427473545074 \n",
            " Test accuracy: 75.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.009534702108293358 \n",
            " Training accuracy: 88.44444444444444\n",
            "Test loss: 0.012813092917203903 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.003828376942417688 \n",
            " Training accuracy: 95.94444444444444\n",
            "Test loss: 0.010381483566015958 \n",
            " Test accuracy: 90.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0018835019002047678 \n",
            " Training accuracy: 97.77777777777777\n",
            "Test loss: 0.013902823217213154 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0009059262965779958 \n",
            " Training accuracy: 99.27777777777777\n",
            "Test loss: 0.01545510694384575 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.00028386981514308396 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.026269213035702704 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.0002750168320699231 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.020166427791118623 \n",
            " Test accuracy: 90.0\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.00017310129830851414 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.01885815344750881 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 6.159790085853779e-05 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.028367350269109012 \n",
            " Test accuracy: 91.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 9\n",
            "epoch 0:\n",
            "Training loss: 0.021935979856385124 \n",
            " Training accuracy: 52.11111111111111\n",
            "Test loss: 0.023998461663722992 \n",
            " Test accuracy: 65.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.019582587596442963 \n",
            " Training accuracy: 66.44444444444444\n",
            "Test loss: 0.017594826221466065 \n",
            " Test accuracy: 77.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.011980440757340855 \n",
            " Training accuracy: 83.55555555555556\n",
            "Test loss: 0.017266833037137986 \n",
            " Test accuracy: 80.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.006238445318821404 \n",
            " Training accuracy: 93.05555555555556\n",
            "Test loss: 0.015811970829963683 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.003405710274560584 \n",
            " Training accuracy: 96.55555555555554\n",
            "Test loss: 0.015312719941139221 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0012943303632265372 \n",
            " Training accuracy: 98.77777777777777\n",
            "Test loss: 0.018423193246126176 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0007196355171941428 \n",
            " Training accuracy: 99.5\n",
            "Test loss: 0.02050257851020433 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.00045061532320687546 \n",
            " Training accuracy: 99.66666666666667\n",
            "Test loss: 0.024190893098711967 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.0002756128265092331 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.019781185239553453 \n",
            " Test accuracy: 90.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.00022071248973790919 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.02146729402244091 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "Folds statistics:\n",
            "----------------\n",
            " - mean: 87.95 \n",
            " - standard deviation: 1.540292180074936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LBSA Method"
      ],
      "metadata": {
        "id": "Xv3pNRwIwgSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LBSA_pipeline = Pipeline(UnderscoreRemover(),\n",
        "                         CharacterRepetitionRemover(),\n",
        "                         ApostrophesMerger(),\n",
        "                         ContractionCleaner(),\n",
        "                         SpecialCharsCleaner(),\n",
        "                         )\n",
        "LBSA_corpus = MovieReviewsCorpusLBSA(LBSA_pipeline)\n",
        "LBSA_embedding_matrix = LBSA_corpus.get_embedding_matrix(fast_text, 300)\n",
        "LBSA_dataset = MovieReviewsDatasetLBSA(LBSA_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY34RPgshVi9",
        "outputId": "bc0780d0-a902-4817-d445-de85232d5b36"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = main_cross_validation(main_LBSA, LBSA_dataset, LBSA_embedding_matrix, collateLBSA, epochs = 10, batch_size=32)\n",
        "# 86.2 +- 2.21\n",
        "# 86.1 +- 3\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ],
      "metadata": {
        "id": "ynwJzSF7hlJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LBSA_pipeline.add_pipeline_element(ShallowObjectiveSentsRemover())\n",
        "LBSA_corpus = MovieReviewsCorpusLBSA(LBSA_pipeline)\n",
        "LBSA_embedding_matrix = LBSA_corpus.get_embedding_matrix(fast_text, 300)\n",
        "LBSA_dataset = MovieReviewsDatasetLBSA(LBSA_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXAMZFhNsYVa",
        "outputId": "657efbe7-3e69-46d1-c0c1-ef87a9ff7961"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = main_cross_validation(main_LBSA, LBSA_dataset, LBSA_embedding_matrix, collateLBSA, epochs = 15, batch_size=32)\n",
        "# - mean: 88.5 \n",
        "# - standard deviation: 1.9235384061671346\n",
        "\n",
        "# Good Run:\n",
        "# - mean: 89.35 \n",
        "# - standard deviation: 1.285496013218244\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ],
      "metadata": {
        "id": "X8-bi60es0KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Models (Glove 840B Embedding)"
      ],
      "metadata": {
        "id": "5Rnr63YbU5Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_vectors = GloVe(name='840B', dim=300, cache = \"/content/gdrive/My Drive/nlu-project/Embeddings/.vector_cache\")"
      ],
      "metadata": {
        "id": "hgNE3lxXrW4M"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LBSA_pipeline = Pipeline(UnderscoreRemover(),\n",
        "                         CharacterRepetitionRemover(),\n",
        "                         ApostrophesMerger(),\n",
        "                         ContractionCleaner(),\n",
        "                         SpecialCharsCleaner(),\n",
        "                         )\n",
        "LBSA_corpus = MovieReviewsCorpusLBSA(LBSA_pipeline)\n",
        "LBSA_embedding_matrix = LBSA_corpus.get_embedding_matrix(global_vectors, 300)\n",
        "LBSA_dataset = MovieReviewsDatasetLBSA(LBSA_corpus)"
      ],
      "metadata": {
        "id": "0TtoFzYDsGHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a527579-16da-4ce9-8649-cc4d0ae225ef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDrwHvOIEsl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9625b00-26c6-46e7-e6a6-a2389caee8eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold: 0\n",
            "epoch 0:\n",
            "Training loss: 0.03088712328010135 \n",
            " Training accuracy: 52.888888888888886\n",
            "Test loss: 0.033306020200252535 \n",
            " Test accuracy: 71.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.023796848704417548 \n",
            " Training accuracy: 78.83333333333333\n",
            "Test loss: 0.022833701968193055 \n",
            " Test accuracy: 84.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.01700952697131369 \n",
            " Training accuracy: 89.66666666666666\n",
            "Test loss: 0.0197608944773674 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.012782681518130832 \n",
            " Training accuracy: 96.22222222222221\n",
            "Test loss: 0.026397609114646912 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.011112581921948327 \n",
            " Training accuracy: 99.27777777777777\n",
            "Test loss: 0.024816671013832094 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.011317659897936715 \n",
            " Training accuracy: 99.5\n",
            "Test loss: 0.03286577194929123 \n",
            " Test accuracy: 91.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.011155783070458307 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.027676002979278566 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.011136825415823195 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.03186442345380783 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.011168492535750071 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.029534547477960586 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.011164532452821731 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.03129702880978584 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 1\n",
            "epoch 0:\n"
          ]
        }
      ],
      "source": [
        "# 87.3, 86.75 e qualcosa - 88.15 lbsa\n",
        "\n",
        "mean, std = main_cross_validation(main_LBSA, LBSA_dataset, LBSA_embedding_matrix, collateLBSA, epochs = 15, batch_size=32)\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subjectivity Detection\n",
        "Now I'm going to implement a subjectivity detector, in order to find objective sentences. This task allows me to remove objective sentences from the subjectivity dataset, so I am left only (almost only, since the model will not be 100% accurate) with objective sentences.\n",
        "\n",
        "## Dataset exploration\n",
        "First I am going to see how the dataset is composed:"
      ],
      "metadata": {
        "id": "TaOnHvIo3X2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import subjectivity\n",
        "\n",
        "\n",
        "subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "obj = [sent for sent in subjectivity.sents(categories = 'obj')]"
      ],
      "metadata": {
        "id": "mFZY-nnW3RzT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I will print the first 10 sentences of each class (subjective and objective) in order to see how the dataset is arranged:"
      ],
      "metadata": {
        "id": "dejtwm5CYsb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mK4Rmfr4ziT"
      },
      "outputs": [],
      "source": [
        "subj[:10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obj[:10]"
      ],
      "metadata": {
        "id": "MMC78B6bYoLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be clearly seen that the daataset is composed of single phrases, instead of documents as opposed to the movie reviews dataset.\n",
        "Now I am going to compare the length of data from the two classes."
      ],
      "metadata": {
        "id": "grLJbs8ehlzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(obj))\n",
        "print(len(subj))"
      ],
      "metadata": {
        "id": "tknJRLrZhlUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's easy to see that the length of both objective and subjective datasets are the same length, so accuracy can be used as a metric since the dataset is balanced"
      ],
      "metadata": {
        "id": "0AxGbL1JiLrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SubjectivityCorpus():\n",
        "  def __init__(self, preprocess_pipeline = None):\n",
        "    # list of documents, each document is a list containing words of that document\n",
        "    self.pipeline = preprocess_pipeline\n",
        "    # Corpus as list of documents. Documents as list of sentences. Sentences as list of tokens\n",
        "    self.corpus, self.labels = self._get_corpus()\n",
        "    if preprocess_pipeline == None:\n",
        "      self.processed_corpus = self.corpus\n",
        "    else:\n",
        "      self.processed_corpus = self._preprocess()\n",
        "\n",
        "    #for optimization purposes\n",
        "    self._vocab_index = 0\n",
        "\n",
        "\n",
        "    self.corpus_words = self.get_corpus_words()\n",
        "    self.vocab = self._create_vocab()\n",
        "\n",
        "\n",
        "\n",
        "  def _list_to_str(self, doc) -> str:\n",
        "      \"\"\"\n",
        "      Put all elements of the list into a single string, separating each element with a space.\n",
        "      \"\"\"\n",
        "      return \" \".join([w for sent in doc for w in sent])\n",
        "\n",
        "  def _preprocess(self):\n",
        "      return self.pipeline(self.corpus)\n",
        "\n",
        "  def _get_corpus(self):\n",
        "    subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "    obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
        "    labels = [1] * len(subj) + [0] * len(obj)\n",
        "    return subj + obj, labels\n",
        "\n",
        "  def subjectivity_dataset_raw(self):\n",
        "      \"\"\"\n",
        "      Returns the dataset containing:\n",
        "\n",
        "      - A list of all the sentences\n",
        "      - The corresponding label for each sentence\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      tuple(list, list)\n",
        "          The dataset: first element is the list of the sentence, the second element of the tuple is the associated label (positive or negative) for each sentence\n",
        "      \"\"\"\n",
        "\n",
        "      return self.corpus, self.labels\n",
        "\n",
        "\n",
        "  def get_corpus_words(self) -> list:\n",
        "      return [w for sent in self.processed_corpus for w in sent]\n",
        "  \n",
        "  def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      np.ndarray\n",
        "          A 2D which each row has the corresponding embedding from the vocabulary\n",
        "      \"\"\"\n",
        "      matrix_length = len(self.vocab)\n",
        "      embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "      # If I use torch.zeros directly it crashes (don't know why)\n",
        "      embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "      null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "      for key in self.vocab.keys():\n",
        "          if torch.equal(embedding[key], null_embedding):\n",
        "              embedding_matrix[self.vocab[key]] = torch.randn(embedding_dim)\n",
        "          else:\n",
        "              embedding_matrix[self.vocab[key]] = embedding[key]\n",
        "              \n",
        "      return embedding_matrix\n",
        "\n",
        "  \n",
        "  def get_fasttext_embedding_matrix(self, embedding, embedding_dim):\n",
        "      matrix_length = len(self.vocab)\n",
        "      embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "      # If I use torch.zeros directly it crashes (don't know why)\n",
        "      embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "      null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "      for key in self.vocab.keys():\n",
        "          tensor_embedding = torch.from_numpy(embedding[key].copy())\n",
        "          if torch.equal(tensor_embedding, null_embedding):\n",
        "              embedding_matrix[self.vocab[key]] = torch.randn(embedding_dim)\n",
        "          else:\n",
        "              embedding_matrix[self.vocab[key]] = tensor_embedding\n",
        "              \n",
        "      return embedding_matrix\n",
        "  \n",
        "  def get_indexed_corpus(self):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      Dictionary\n",
        "          Containing correspondences word -> index\n",
        "      \n",
        "      list(list(torch.tensor))\n",
        "          The corpus represented as indexes corresponding to each word\n",
        "      \"\"\"\n",
        "      vocab = {}\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "          vocab[key] = idx\n",
        "      \n",
        "      indexed_corpus = [torch.tensor([vocab[word] for word in sent]) for sent in self.processed_corpus]\n",
        "      return indexed_corpus, self.labels\n",
        "\n",
        "  def embed_vocab(self, vocab):\n",
        "    for word in vocab.keys():\n",
        "      try:\n",
        "          self.vocab[word]\n",
        "      except:\n",
        "          self.vocab[word] = self._vocab_index\n",
        "          self._vocab_index += 1\n",
        "\n",
        "  def _create_vocab(self):\n",
        "      vocab = dict()\n",
        "      for word in self.corpus_words:\n",
        "        try:\n",
        "          vocab[word]\n",
        "        except:\n",
        "          vocab[word] = self._vocab_index\n",
        "          self._vocab_index += 1\n",
        "      return vocab\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.corpus)\n"
      ],
      "metadata": {
        "id": "GHTqA_vaA2Vm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class SubjectivityDataset(Dataset):\n",
        "  def __init__(self, raw_dataset):\n",
        "    super(SubjectivityDataset, self).__init__()\n",
        "    self.corpus = raw_dataset[0]\n",
        "    self.targets = raw_dataset[1]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    item = self.corpus[index]\n",
        "    label = self.targets[index]\n",
        "    return (item, label)"
      ],
      "metadata": {
        "id": "3NXv420XEUR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_sub = pipeline = Pipeline(UnderscoreRemoverFlat(),\n",
        "                                    CharacterRepetitionRemoverFlat(),\n",
        "                                    ApostrophesMergerFlat(),\n",
        "                                    ContractionCleanerFlat(),\n",
        "                                    SpecialCharsCleanerFlat(),\n",
        "                                    )\n",
        "corpus_subj = SubjectivityCorpus(pipeline)\n",
        "corpus_subj.embed_vocab(corpus.vocab)"
      ],
      "metadata": {
        "id": "B_18YMFwpe9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_subj = SubjectivityDataset(corpus_subj.get_indexed_corpus())\n",
        "train_loader_subj, test_loader_subj = get_data(128, dataset_subj, collate)"
      ],
      "metadata": {
        "id": "kTOap0UOpxhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_subj = corpus_subj.get_embedding_matrix(global_vectors, 300)"
      ],
      "metadata": {
        "id": "T7l4AVLtqsaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, net_obj= main(train_loader_subj, test_loader_subj, embedding_matrix_subj, device = \"cuda\", epochs = 10)\n",
        "\n",
        "# print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ],
      "metadata": {
        "id": "X1yZv0OksfuA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "dc20f90b-f535-4030-b549-03604f353de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0:\n",
            "Training loss: 0.0013972479817457497 \n",
            " Training accuracy: 81.75\n",
            "Test loss: 0.0011940113492310048 \n",
            " Test accuracy: 80.7\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.00021139853063505143 \n",
            " Training accuracy: 79.725\n",
            "Test loss: 0.0002531803525052965 \n",
            " Test accuracy: 81.2\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-279e90f32327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_obj\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-65e4408686ce>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_loader, test_loader, embedding_matrix, device, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch {e}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-7956307cf77e>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(net, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try to use trained subjectivity in order to improve polarity"
      ],
      "metadata": {
        "id": "mUKX632yBajt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_testing(X, device = \"cuda\"):\n",
        "  if type(X) == list:\n",
        "    X = torch.tensor(X)\n",
        "  if len(X.size()) == 1:\n",
        "    X = torch.unsqueeze(X, 0)\n",
        "  \n",
        "  X_tensor = X.to(device)\n",
        "  X_final = pack_padded_sequence(X_tensor, torch.tensor([1]), batch_first=True)\n",
        "  return X_final\n"
      ],
      "metadata": {
        "id": "6bEcAGPHBy0o"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.sentiment.util import mark_negation\n",
        "\n",
        "class ObjectiveSentsRemover(PipelineElement):\n",
        "  def __init__(self, net = None, vocab = None):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    net\n",
        "      Already trained network in order to classify objective sentences\n",
        "    \"\"\"\n",
        "    super(ObjectiveSentsRemover, self).__init__()\n",
        "    if net == None:\n",
        "      raise TypeError(\"Network cannot be None-type\")\n",
        "    self.net = net\n",
        "    self.vocab = vocab\n",
        "  \n",
        "  def _indexed_corpus(self, corpus):\n",
        "    indexed_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for sent in doc:\n",
        "        new_sent = []\n",
        "        for word in sent:\n",
        "          new_sent.append(self.vocab[word])\n",
        "        new_doc.append(new_sent)\n",
        "      indexed_corpus.append(new_doc)\n",
        "    return indexed_corpus\n",
        "\n",
        "\n",
        "  def remove_objective_sents(self, corpus):\n",
        "    indexed_corpus = self._indexed_corpus(corpus)\n",
        "    print(len(corpus[0]))\n",
        "    with torch.no_grad():\n",
        "      self.net.eval()\n",
        "      # I want to keep the sentence if it is subjective i.e. when result of classification = 1\n",
        "      res = [[corpus[doc_idx][sent_idx] for sent_idx, sent in enumerate(doc) if self.net(collate_testing(sent))[0].max(dim = 1)[1].item() == 1]\n",
        "             for doc_idx, doc in enumerate(indexed_corpus)]\n",
        "      print(len(res[0]))\n",
        "    return res\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_objective_sents(corpus)\n",
        "\n",
        "class ShallowObjectiveSentsRemover(PipelineElement):\n",
        "  def __init__(self, threshold = .5, clf = MultinomialNB, trained = False):\n",
        "    self.vectorizer = CountVectorizer()\n",
        "    self.classifier = clf()\n",
        "    if not trained:\n",
        "      self.best_estimator = self._train()\n",
        "    else:\n",
        "      self.best_estimator = self.classifier\n",
        "  \n",
        "  def _train(self):\n",
        "    subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "    obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
        "\n",
        "    corpus = [self.neg_marking_list2str(d) for d in subj] + [self.neg_marking_list2str(d) for d in obj]\n",
        "    vectors = self.vectorizer.fit_transform(corpus)\n",
        "    labels = np.array([1] * len(subj) + [0] * len(obj))\n",
        "    scores = cross_validate(self.classifier, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['accuracy'], return_estimator=True)\n",
        "    estimator = scores[\"estimator\"][scores[\"test_accuracy\"].argmax()]\n",
        "    return estimator\n",
        "\n",
        "  def neg_marking_list2str(self, sent):\n",
        "    # takes the doc and produces a single list\n",
        "    # negates the whole document\n",
        "    negated_doc = mark_negation(sent, double_neg_flip=True)\n",
        "    return \" \".join([w for w in negated_doc])\n",
        "    \n",
        "\n",
        "  def remove_objective_sents(self, corpus):\n",
        "    transformed_corpus = [[self.vectorizer.transform([self.neg_marking_list2str(sent)]) for sent in doc] for doc in corpus]\n",
        "    print(len(corpus[0]))\n",
        "    res = [[corpus[doc_idx][sent_idx] for sent_idx, sent in enumerate(doc) if self.best_estimator.predict(sent).item()]\n",
        "           for doc_idx, doc in enumerate(transformed_corpus)]\n",
        "    print(len(res[0]))\n",
        "    return res\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_objective_sents(corpus)\n"
      ],
      "metadata": {
        "id": "d-wAhSa0ihJg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(UnderscoreRemover(),\n",
        "                    CharacterRepetitionRemover(),\n",
        "                    ApostrophesMerger(),\n",
        "                    ContractionCleaner(),\n",
        "                    SpecialCharsCleaner(),\n",
        "                    ShallowObjectiveSentsRemover(),\n",
        "                    )\n",
        "corpus = MovieReviewsCorpusLBSA(pipeline)\n",
        "# 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42sw8MWVfKpN",
        "outputId": "eafb9da4-43b6-4ecb-bc74-9b9e467ed3cf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = corpus.get_embedding_matrix(global_vectors, 300)\n",
        "dataset = MovieReviewsDatasetLBSA(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A6Hxlu_fL05",
        "outputId": "f8a3166e-b31b-4b15-e4e9-6aa07b0d3447"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 epochs because of the warmup\n",
        "mean, std = main_cross_validation(main_LBSA, dataset, embedding_matrix, collateLBSA, epochs = 15)\n",
        "## 88.65 +- 1.24 no residual neither batchnorm\n",
        "## 89.15 +- 2.16             Residual with reg = False\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CU07k0-tjbx5",
        "outputId": "a3cc5efb-a1b1-402a-850c-f646d77aa38f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold: 0\n",
            "epoch 0:\n",
            "Training loss: 0.007959709366162618 \n",
            " Training accuracy: 61.05555555555555\n",
            "Test loss: 0.009188937544822693 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.005274999472830031 \n",
            " Training accuracy: 78.38888888888889\n",
            "Test loss: 0.00855442076921463 \n",
            " Test accuracy: 56.49999999999999\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.003910372091664208 \n",
            " Training accuracy: 89.77777777777777\n",
            "Test loss: 0.0074517691135406496 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.002459457086192237 \n",
            " Training accuracy: 96.38888888888889\n",
            "Test loss: 0.0054387211799621586 \n",
            " Test accuracy: 84.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0018047150638368394 \n",
            " Training accuracy: 98.83333333333333\n",
            "Test loss: 0.0091081303358078 \n",
            " Test accuracy: 79.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0017057364765140745 \n",
            " Training accuracy: 99.3888888888889\n",
            "Test loss: 0.013652061820030212 \n",
            " Test accuracy: 79.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0016082441558440527 \n",
            " Training accuracy: 99.77777777777777\n",
            "Test loss: 0.01578713774681091 \n",
            " Test accuracy: 75.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.0016047549413310156 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.010998684465885162 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.0015644259171353446 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.011297451257705688 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.0016350032389163972 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.010835739374160767 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.0016054754869805443 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.025587269067764283 \n",
            " Test accuracy: 75.0\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.0016089071333408355 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.024370667934417726 \n",
            " Test accuracy: 72.0\n",
            "------------------------------------------------------------------\n",
            "epoch 12:\n",
            "Training loss: 0.0016930692394574484 \n",
            " Training accuracy: 99.66666666666667\n",
            "Test loss: 0.02161523461341858 \n",
            " Test accuracy: 76.0\n",
            "------------------------------------------------------------------\n",
            "epoch 13:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a2c47ea2af9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 20 epochs because of the warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_LBSA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollateLBSA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m## 88.65 +- 1.24 no residual neither batchnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## 89.15 +- 2.16             Residual with reg = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-519a3c497c07>\u001b[0m in \u001b[0;36mmain_cross_validation\u001b[0;34m(main_fn, dataset, embedding_matrix, collate_fn, device, epochs, random_state, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mfold_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-f188fda874d1>\u001b[0m in \u001b[0;36mmain_LBSA\u001b[0;34m(train_loader, test_loader, embedding_matrix, device, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch {e}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step_LBSA_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step_LBSA_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-59d9cd3a367f>\u001b[0m in \u001b[0;36mtraining_step_LBSA_new\u001b[0;34m(encoder, decoder, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0min_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Sorting the sentences of the encoder to their original position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Inverting the output because indexes are in ascending order, output is in descending\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-b7c96e878766>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_elementwise_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# n_sents, n_words_per_sent, hidden_size * 2 (since is bilstm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 773\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZgEYeACSZha"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('nlu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e0262c2e7a08424d65c968f8ecfc5afb6b5a99089f86fd0fa27478ea619b0ef2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}