{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinni98/Sentiment-analysis-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NBg5KtW5AMy"
      },
      "source": [
        "## Polarity Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oGXvFNN46OT"
      },
      "source": [
        "### Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvHgFZJe4_Xq",
        "outputId": "5b46921e-28d8-435c-84da-3b7b4d370446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Package subjectivity is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import torch\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"movie_reviews\")\n",
        "nltk.download(\"subjectivity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApFi70um_WPB"
      },
      "source": [
        "## TODO: Use spacy negation marking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "C3PyvlOV60V7",
        "outputId": "750ca3be-cfbf-4c35-ae07-cf14e3dc75cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'class MovieReviewsCorpusNegMarked(MovieReviewsCorpus):\\n  def __init__(self):\\n    super().__init__()\\n    self._neg_marking()\\n    self.corpus_words = self.get_corpus_words()\\n    self.vocab = self._create_vocab()\\n  \\n  def _neg_marking(self):\\n    negated_corpus = [self._mark(doc) for doc in self.corpus]\\n    self.corpus = negated_corpus\\n  \\n  def _mark(self, doc):\\n    # negates the whole document\\n    negated_doc = mark_negation(doc, double_neg_flip=True)\\n    return negated_doc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "from nltk.sentiment.util import mark_negation\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MovieReviewsCorpus():\n",
        "  def __init__(self):\n",
        "    # list of documents, each document is a list containing words of that document\n",
        "    self.mr = movie_reviews\n",
        "    self.corpus, self.labels = self._flatten()\n",
        "    self.unprocessed_corpus = self._get_corpus()\n",
        "    self.corpus_words = self.get_corpus_words()\n",
        "    self.vocab = self._create_vocab()\n",
        "\n",
        "  \n",
        "  def _list_to_str(self, doc) -> str:\n",
        "    \"\"\"\n",
        "    Put all elements of the list into a single string, separating each element with a space.\n",
        "    \"\"\"\n",
        "    return \" \".join([w for sent in doc for w in sent])\n",
        "  \n",
        "  \n",
        "  def _flatten(self):\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    list[list[str]]\n",
        "      Each inner list represents a document. Each document is a list of tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # 3 nested list: each list contain a document, each inner list contains a phrase (until fullstop), each phrase contains words.\n",
        "    neg = self.mr.paras(categories = \"neg\")\n",
        "    pos = self.mr.paras(categories = \"pos\")\n",
        "\n",
        "    corpus = [[w for w in self._list_to_str(d).split(\" \")] for d in pos] + [[w for w in self._list_to_str(d).split(\" \")] for d in neg]\n",
        "    labels = np.array([0] * len(pos) + [1] * len(neg))\n",
        "    return corpus, labels\n",
        "  \n",
        "  def _get_corpus(self):\n",
        "    neg = self.mr.paras(categories = \"neg\")\n",
        "    pos = self.mr.paras(categories = \"pos\")\n",
        "\n",
        "    return neg + pos\n",
        "\n",
        "  def movie_reviews_dataset_raw(self):\n",
        "    \"\"\"\n",
        "    Returns the dataset containing:\n",
        "    \n",
        "    - A list of all the documents\n",
        "    - The corresponding label for each document\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple(list, np.array)\n",
        "      The dataset: first element is the list of the document, the second element of the tuple is the associated label (positive or negative) for each document\n",
        "    \"\"\"\n",
        "    \n",
        "    return self.corpus, self.labels\n",
        "  \n",
        "  def get_sentence_ds(self):\n",
        "    neg = self.mr.paras(categories = \"neg\")\n",
        "    pos = self.mr.paras(categories = \"pos\")\n",
        "\n",
        "    pos = [phrase for doc in pos for phrase in doc]\n",
        "    neg = [phrase for doc in neg for phrase in doc]\n",
        "\n",
        "    labels = np.array([0] * len(pos) + [1] * len(neg))\n",
        "    corpus = neg+pos\n",
        "    return corpus, labels\n",
        "\n",
        "  \n",
        "  def get_corpus_words(self) -> list:\n",
        "    return [w for doc in self.corpus for w in doc]\n",
        "\n",
        "  def _create_vocab(self):\n",
        "    vocab = dict()\n",
        "    for word in self.corpus_words:\n",
        "      try:\n",
        "        vocab[word] += 1\n",
        "      except:\n",
        "        vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "\n",
        "\n",
        "\"\"\"class MovieReviewsCorpusNegMarked(MovieReviewsCorpus):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self._neg_marking()\n",
        "    self.corpus_words = self.get_corpus_words()\n",
        "    self.vocab = self._create_vocab()\n",
        "  \n",
        "  def _neg_marking(self):\n",
        "    negated_corpus = [self._mark(doc) for doc in self.corpus]\n",
        "    self.corpus = negated_corpus\n",
        "  \n",
        "  def _mark(self, doc):\n",
        "    # negates the whole document\n",
        "    negated_doc = mark_negation(doc, double_neg_flip=True)\n",
        "    return negated_doc\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zZaF5yV9rlo-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class MovieReviewsDataset(Dataset):\n",
        "  def __init__(self, raw_dataset):\n",
        "    self.corpus = np.array(raw_dataset[0])\n",
        "    self.elements_to_tensor()\n",
        "    self.labels = np.array(raw_dataset[1])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "  \n",
        "  def elements_to_tensor(self):\n",
        "    global_vectors = GloVe(name='840B', dim=300)\n",
        "    for idx, item in enumerate(self.corpus):\n",
        "      item_tensor = torch.empty(len(item), 300)\n",
        "      for i in range(len(item)):\n",
        "        token = item[i]\n",
        "        item_tensor[i] = global_vectors.get_vecs_by_tokens(token)\n",
        "      self.corpus[idx] = item_tensor\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    item = self.corpus[index]\n",
        "    label = self.labels[index]\n",
        "    return (item, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr7PPG1k4gFq"
      },
      "source": [
        "### Create the model class\n",
        "Let's first try with a simple BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bCvqaJ8-hKmT"
      },
      "outputs": [],
      "source": [
        "from unicodedata import bidirectional\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, device = \"cuda\", input_size = 300, hidden_size = 128, output_size = 2):\n",
        "    super(BiLSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True, bidirectional=True, num_layers = 2)\n",
        "    self.source_classifier = nn.Sequential(nn.ReLU(),\n",
        "                                           nn.BatchNorm1d(hidden_size*2, eps = 1e-08),\n",
        "                                           nn.Dropout(0.3),\n",
        "                                           nn.Linear(hidden_size*2, output_size)\n",
        "                                           )\n",
        "    self.target_classifier = nn.Sequential(nn.ReLU(),\n",
        "                                           nn.BatchNorm1d(hidden_size*2, eps = 1e-08),\n",
        "                                           nn.Dropout(0.3),\n",
        "                                           nn.Linear(hidden_size*2, output_size)\n",
        "                                           )\n",
        "    \n",
        "    \n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "      if self.cuda:\n",
        "        return (torch.zeros(4, batch_size, self.hidden_size).to(self.device),\n",
        "                torch.zeros(4, batch_size, self.hidden_size).to(self.device),)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.batch_sizes[0].item()\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "\n",
        "    features, _ = self.lstm(x)\n",
        "    features, input_sizes = pad_packed_sequence(features, batch_first=True)\n",
        "    features = features[list(range(batch_size)), input_sizes - 1, :]\n",
        "    source_output = self.source_classifier(features)\n",
        "    # source_output = nn.Softmax(source_output)\n",
        "\n",
        "    target_output = self.target_classifier(features)\n",
        "    # target_output = nn.Softmax(target_output)\n",
        "\n",
        "    source_target_output = torch.cat((source_output, target_output), dim=1)\n",
        "    \n",
        "    return source_output , target_output, source_target_output\n",
        "\n",
        "  def classifier_parameters(self) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Parameters of the classification layer\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    torch.Tensor\n",
        "      Classification layer parameter\n",
        "    \"\"\"\n",
        "    sc = list(self.source_classifier.parameters())\n",
        "    tc = list(self.target_classifier.parameters())\n",
        "    tot = sc + tc\n",
        "    for param in tot:\n",
        "      yield param\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import torch.optim as optim\n",
        "\n",
        "class AnnealingOptimizer(torch.optim.Optimizer, ABC):\n",
        "  \"\"\"\n",
        "  Defines and abstract class in order to implement an sgd optimizer using an annealing strategy\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0) -> None:\n",
        "    if not 0.0 <= lr:\n",
        "      raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "    if not 0 <= epoch:\n",
        "      raise ValueError(f\"Invalid epoch value: {epoch}\")\n",
        "    \n",
        "    self.nr_epochs = nr_epochs\n",
        "    self.epoch = epoch\n",
        "    self._alpha = 10\n",
        "    self._beta = 0.75\n",
        "    self._base_lr = lr\n",
        "\n",
        "  def update_lr(self):\n",
        "    \"\"\"\n",
        "    Updates the learning rate using the annealing strategy.\n",
        "    In order to let the annealing strategy to work correctly, this method should be called at every epoch during the network training\n",
        "\n",
        "    The learning rate for the classifier is 10 times bigger as proposed in the [Symnet paper](https://arxiv.org/pdf/1904.04663.pdf)\n",
        "    \"\"\"\n",
        "    self.epoch += 1\n",
        "    new_lr = self._compute_lr()\n",
        "    for g in self.optimizer.param_groups:\n",
        "      if g[\"name\"] == \"fe\":\n",
        "        g[\"lr\"] = new_lr\n",
        "      else:\n",
        "        g[\"lr\"] = new_lr*10\n",
        "\n",
        "    \n",
        "  def _compute_lr(self):\n",
        "    \"\"\"\n",
        "    Computes the learning rate using the proposed annealing strategy\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      updated learning rate\n",
        "    \"\"\"\n",
        "    etap = 1 / ((1 + self._alpha * self.epoch / self.nr_epochs ) ** self._beta)\n",
        "    return self._base_lr * etap\n",
        "\n",
        "  def step(self):\n",
        "    self.optimizer.step()\n",
        "  \n",
        "  def zero_grad(self):\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "class BiLSTMOptimizer(AnnealingOptimizer):\n",
        "  \"\"\"\n",
        "  Implements an annealing optimizer for Resnet\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0, momentum: float = 0.9) -> None:\n",
        "    super(BiLSTMOptimizer ,self).__init__(model, nr_epochs, lr, epoch)\n",
        "    \n",
        "    # Note that names for parameters group are important in order to update each group differently\n",
        "    self.optimizer = optim.SGD([\n",
        "                {'params': model.lstm.parameters(), \"name\": \"fe\"},\n",
        "                {'params': model.classifier_parameters(), \"lr\": self._compute_lr()*10, \"name\": \"classifier\"}\n",
        "            ], lr=lr, momentum=momentum)\n",
        "    "
      ],
      "metadata": {
        "id": "boz2XicVi4aT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def source_loss(output_source, label):\n",
        "  \"\"\"\n",
        "   Cross entropy loss of source classifier C_s for source samples (equation 5 of the paper)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  output: torch.Tensor\n",
        "    Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "    be the output of the source classifier\n",
        "  \n",
        "  label: torch.Tensor\n",
        "    Labels corresponding to the samples whose output is computed\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  torch.Tensor\n",
        "    The result of the computed loss for the entire batch\n",
        "  \"\"\"\n",
        "  loss_fun = nn.CrossEntropyLoss()\n",
        "  loss = loss_fun(output_source, label)\n",
        "  return loss\n",
        "\n",
        "def target_loss(output_target, label):\n",
        "  \"\"\"\n",
        "  Cross entropy loss of target classifier C_t for source samples (equation 6 of the paper)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  output: torch.Tensor\n",
        "    Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "    be the output of the target classifier\n",
        "  \n",
        "  label: torch.Tensor\n",
        "    Labels corresponding to the samples whose output is computed\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  torch.Tensor\n",
        "    The result of the computed loss for the entire batch\n",
        "  \"\"\"\n",
        "  return source_loss(output_target, label)\n",
        "\n",
        "def source_target_loss(output, st = True):\n",
        "  \"\"\"\n",
        "  Two-way cross-entropy loss for the joint classifier C_st (equation 7 of the paper)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  output: torch.Tensor\n",
        "    Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "    be the output of the combined source-target classifier\n",
        "  st: bool\n",
        "    True if train batch belongs to source, False if belongs to target\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  torch.Tensor\n",
        "    The result of the computed loss for the entire batch\n",
        "\n",
        "  \"\"\"\n",
        "  n_classes = int(output.size(1)/2)\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  prob_out = soft(output)\n",
        "  if st:\n",
        "    loss = -(prob_out[:,:n_classes].sum(1).log().mean())\n",
        "  else:\n",
        "    loss = -(prob_out[:,n_classes:].sum(1).log().mean())\n",
        "  return loss\n",
        "\n",
        "def feature_category_loss(output_st, label):\n",
        "  \"\"\"\n",
        "  Category level confusion loss (equation 8 of the Symnet paper)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  output_st: torch.Tensor\n",
        "    Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "    be the output of the combined source-target classifier\n",
        "  \n",
        "  label: torch.Tensor\n",
        "    Labels corresponding to the samples whose output is computed\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  torch.Tensor\n",
        "    The result of the computed loss for the entire batch\n",
        "\n",
        "  \"\"\"\n",
        "  n_classes = int(output_st.size(1)/2)\n",
        "\n",
        "  loss_fun_1 = nn.CrossEntropyLoss()\n",
        "  loss_fun_2 = nn.CrossEntropyLoss()\n",
        "\n",
        "  loss_1 = loss_fun_1(output_st[:, :n_classes], label)/2\n",
        "  loss_2 = loss_fun_2(output_st[:,n_classes:], label)/2\n",
        "  return loss_1 + loss_2\n",
        "\n",
        "def feature_domain_loss(output_st):\n",
        "  \"\"\"\n",
        "  Domain level confusion loss (equation 9 of the Symnet paper)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  output: torch.Tensor\n",
        "    Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "    be the output of the combined source-target classifier\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  torch.Tensor\n",
        "    The result of the computed loss for the entire batch\n",
        "\n",
        "  \"\"\"\n",
        "  n_classes = int(output_st.size(1)/2)\n",
        "\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  prob_out = soft(output_st)\n",
        "\n",
        "  loss_1 = -(prob_out[:,:n_classes]).sum(1).log().mean()/2\n",
        "  loss_2 = -(prob_out[:,n_classes:]).sum(1).log().mean()/2\n",
        "\n",
        "  return loss_1 + loss_2\n",
        "\n",
        "\n",
        "\n",
        "def entropyMinimizationPrinciple(output_st):\n",
        "    \"\"\"\n",
        "    Entropy minimization principle (equation 10 of the Symnet paper)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output: torch.Tensor\n",
        "      Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "      be the output of the combined source-target classifier\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      The corresponding entropy minimization loss for the entire batch\n",
        "    \"\"\"\n",
        "    nr_classes = int(output_st.size(1)/2)\n",
        "    soft = nn.Softmax(dim=1)\n",
        "    prob_out = soft(output_st)\n",
        "\n",
        "    p_st_source = prob_out[:, :nr_classes]\n",
        "    p_st_target = prob_out[:, nr_classes:]\n",
        "    qst = p_st_source + p_st_target\n",
        "\n",
        "    emp = -qst.log().mul(qst).sum(1).mean()\n",
        "\n",
        "    return emp"
      ],
      "metadata": {
        "id": "B7GvAvTGjFYe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yZFg9BvQdZHB"
      },
      "outputs": [],
      "source": [
        "def argsort(X, Y, document_lengths):\n",
        "  \"\"\"\n",
        "  Sort inputs by document lengths\n",
        "  \"\"\"\n",
        "  indexes = np.argsort(document_lengths)\n",
        "\n",
        "  X_sorted = X[indexes][::-1]\n",
        "  Y_sorted = Y[indexes][::-1]\n",
        "  document_lengths = torch.from_numpy(document_lengths[indexes][::-1].copy())\n",
        "\n",
        "  return X_sorted, Y_sorted, document_lengths\n",
        "\n",
        "def collate_lengths(batch, lengths):\n",
        "  X, Y = list(zip(*batch))\n",
        "  Y = np.array(list(Y))\n",
        "  X = np.array(list(X))\n",
        "\n",
        "  # Sort dataset\n",
        "  X, Y, document_lengths = argsort(X, Y, lengths)\n",
        "\n",
        "  # Get tensor sizes\n",
        "  max_size = torch.max(document_lengths).item()\n",
        "\n",
        "  # Pad tensor each element\n",
        "  X = pad(X, max_size)\n",
        "\n",
        "  # Transform the batch to a tensor\n",
        "  X_tensor = batch_to_tensor(X, max_size)\n",
        "  \n",
        "  # Return the padded sequence object\n",
        "  return X_tensor, torch.from_numpy(Y.copy()), document_lengths\n",
        "\n",
        "def training_step_uda(net, src_data_loader, target_data_loader, optimizer, lam, e, device = 'cuda'):\n",
        "  n_source_samples = 0.\n",
        "  n_target_samples = 0.\n",
        "  cumulative_classifier_loss = 0.\n",
        "  cumulative_feature_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  target_iter = iter(target_data_loader)\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs_source, labels, source_document_lengths) in enumerate(src_data_loader):\n",
        "    try:\n",
        "      inputs_target, _ , target_document_lengths = next(target_iter)\n",
        "    except:\n",
        "      target_iter = iter(target_data_loader)\n",
        "      inputs_target, _, target_document_lengths = next(target_iter)\n",
        "  \n",
        "    \n",
        "    # Cannot simply concatenate because they have different lengths in dimensions that are not batch dim.\n",
        "    source_inputs_to_list = []\n",
        "    target_inputs_to_list = []\n",
        "\n",
        "    for i, x in enumerate(inputs_source):\n",
        "      source_inputs_to_list.append((x, labels[i].item()))\n",
        "    for i, x in enumerate(inputs_target):\n",
        "      # Creating fake labels\n",
        "      target_inputs_to_list.append((x, -1))\n",
        "\n",
        "    source_document_lengths = source_document_lengths.numpy()\n",
        "    target_document_lengths = target_document_lengths.numpy()\n",
        "    document_lengths = np.concatenate([source_document_lengths, target_document_lengths])\n",
        "    # Since inputs from source and target have different lengths I have to collate again\n",
        "    inputs, labels, document_lengths = collate_lengths(source_inputs_to_list + target_inputs_to_list,\n",
        "                                      document_lengths)\n",
        "\n",
        "    inputs = pack_padded_sequence(inputs, document_lengths, batch_first=True)\n",
        "\n",
        "    source_samples = [i for i in range(len(labels)) if labels[i]!=-1]\n",
        "    target_samples = [i for i in range(len(labels)) if labels[i]==-1]\n",
        "    labels = torch.tensor([labels[i].item() for i in range(len(labels)) if labels[i]!=-1])\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "\n",
        "    # forward pass\n",
        "    c_s, c_t, c_st = net(inputs)\n",
        "\n",
        "    c_s_source = c_s[source_samples,:]\n",
        "    c_s_target = c_s[target_samples,:]\n",
        "\n",
        "    c_t_source = c_t[source_samples,:]\n",
        "    c_t_target = c_t[target_samples,:]\n",
        "\n",
        "    c_st_source = c_st[source_samples,:]\n",
        "    c_st_target = c_st[target_samples,:]\n",
        "\n",
        "\n",
        "    # Equation 5 of the paper\n",
        "    error_source_task = source_loss(c_s_source, labels)\n",
        "\n",
        "    # Equation 6 of the paper\n",
        "    error_target_task = target_loss(c_t_source, labels)\n",
        "\n",
        "    # Equation 7 of the paper\n",
        "    domain_loss_source = source_target_loss(c_st_source)\n",
        "    domain_loss_target = source_target_loss(c_st_target, st = False)\n",
        "    error_domain = domain_loss_source + domain_loss_target\n",
        "\n",
        "    classifier_total_loss = error_source_task + error_target_task + error_domain\n",
        "\n",
        "    # Retain graph needed because otherwise the parts of the computation graph\n",
        "    # needed to compute classifier_total_loss will be freed up, but we\n",
        "    # need those parts in order to compute the next loss\n",
        "    classifier_total_loss.backward(retain_graph = True)\n",
        "\n",
        "    for param in net.lstm.parameters():\n",
        "      param.grad.data.zero_()\n",
        "    \n",
        "    class_params = []\n",
        "    for param in net.source_classifier.parameters():\n",
        "      class_params.append(param.grad.data.clone())\n",
        "      param.grad.data.zero_()\n",
        "    for param in net.target_classifier.parameters():\n",
        "      class_params.append(param.grad.data.clone())\n",
        "      param.grad.data.zero_()\n",
        "\n",
        "    # Equation 8 of the paper\n",
        "    error_feature_category = feature_category_loss(c_st_source, labels)\n",
        "\n",
        "    # Equation 9 of the paper\n",
        "    error_feature_domain = feature_domain_loss(c_st_target)\n",
        "\n",
        "    min_entropy = entropyMinimizationPrinciple(c_st_target)\n",
        "\n",
        "    # Equations 11 of the paper\n",
        "    feature_total_loss = error_feature_category + lam * (error_feature_domain + min_entropy)\n",
        "\n",
        "    feature_total_loss.backward()\n",
        "\n",
        "    idx = 0\n",
        "    for param in net.source_classifier.parameters():\n",
        "      param.grad.data = class_params[idx]\n",
        "      idx += 1\n",
        "    for param in net.target_classifier.parameters():\n",
        "      param.grad.data = class_params[idx]\n",
        "      idx += 1\n",
        "\n",
        "    \n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "\n",
        "\n",
        "    # print statistics\n",
        "    n_source_samples+=inputs_source.shape[0]\n",
        "    n_target_samples+=inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_classifier_loss += classifier_total_loss.item()\n",
        "    cumulative_feature_loss += feature_total_loss.item()\n",
        "    _, predicted = c_s_source.max(dim = 1) ## to get the maximum probability\n",
        "    cumulative_accuracy += predicted.eq(labels).sum().item()\n",
        "\n",
        "  return cumulative_classifier_loss/n_source_samples, cumulative_feature_loss/n_target_samples, cumulative_accuracy/n_source_samples*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LJef4h1cfWym"
      },
      "outputs": [],
      "source": [
        "def test_step_uda(net, data_target_test_loader, device='cuda:0'):\n",
        "\n",
        "    '''\n",
        "    Params\n",
        "    ------\n",
        "\n",
        "    net : model \n",
        "    data_loader : DataLoader obj of the domain to test on\n",
        "    cost_function : cost function used to address accuracies (not necessary) -> TargetClassifierLoss\n",
        "    device : GPU or CPU device\n",
        "\n",
        "    '''\n",
        "\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_idx, (inputs, labels, document_lengths) in enumerate(data_target_test_loader):\n",
        "\n",
        "            inputs = pack_padded_sequence(inputs, document_lengths, batch_first=True)\n",
        "            # load data into GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = labels.to(device)\n",
        "        \n",
        "            # forward pass\n",
        "            _, c_t, _ = net(inputs)\n",
        "\n",
        "            # apply the loss\n",
        "            loss = target_loss(c_t, targets)\n",
        "\n",
        "            # print statistics\n",
        "            samples+=c_t.shape[0]\n",
        "            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "            _, predicted = c_t.max(1)\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OOeUmEHZNBvw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "\n",
        "def main_uda(source_train_loader,\n",
        "             target_train_loader,\n",
        "             target_test_loader,\n",
        "             device=\"cuda\",\n",
        "             epochs=15,\n",
        "             nr_classes = 2, \n",
        "            ):\n",
        "    \n",
        "  # writer = SummaryWriter(log_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\")\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  \n",
        "  print('DataLoaders Done')\n",
        "  net = BiLSTM().to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = BiLSTMOptimizer(model = net, nr_epochs = epochs)\n",
        "  print('Got optimizers')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    lam = 2 / (1 + math.exp(-1 * 10 * e / epochs)) - 1\n",
        "\n",
        "    train_ce_loss, train_en_loss, train_accuracy = training_step_uda(net=net, src_data_loader=source_train_loader, \n",
        "                                                        target_data_loader=target_train_loader, \n",
        "                                                        optimizer=optimizer, lam=lam, e=e, device=device)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f'Epoch: {e+1:d}')\n",
        "    print(f'\\t Train: CE loss {train_ce_loss:.5f}, Entropy loss {train_en_loss:.5f}, Accuracy {train_accuracy:.2f}')\n",
        "\n",
        "    test_loss, test_accuracy = test_step_uda(net, target_test_loader, device)\n",
        "    print(f'\\t Test Accuracy {test_accuracy:.2f}')\n",
        "    \n",
        "    optimizer.update_lr()\n",
        "\n",
        "  test_loss, test_accuracy = test_step_uda(net, target_test_loader, device)\n",
        "  return test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GfNtrS8jmATx"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def pad(batch, max_size):\n",
        "  pad = torch.zeros(batch[0].size(dim=1))\n",
        "  for idx in range(len(batch)):\n",
        "    remaining = max_size - batch[idx].size(dim = 0)\n",
        "    batch[idx] = torch.cat((batch[idx], pad.repeat((remaining, 1))), dim = 0)\n",
        "  return batch\n",
        "\n",
        "def batch_to_tensor(X: List[torch.tensor], max_size):\n",
        "  X_tensor = torch.zeros(len(X), max_size, X[0].size(dim = 1))\n",
        "  for i, embed in enumerate(X):\n",
        "    X_tensor[i] = embed\n",
        "  return X_tensor\n",
        "\n",
        "def sort_ds(X, Y):\n",
        "  \"\"\"\n",
        "  Sort inputs by document lengths\n",
        "  \"\"\"\n",
        "  document_lengths = np.array([tens.size(dim = 0) for tens in X])\n",
        "  indexes = np.argsort(document_lengths)\n",
        "\n",
        "  X_sorted = X[indexes][::-1]\n",
        "  Y_sorted = Y[indexes][::-1]\n",
        "  document_lengths = torch.from_numpy(document_lengths[indexes][::-1].copy())\n",
        "\n",
        "  return X_sorted, Y_sorted, document_lengths\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "  X, Y = list(zip(*batch))\n",
        "  Y = np.array(list(Y))\n",
        "  X = np.array(list(X))\n",
        "\n",
        "  # Sort dataset\n",
        "  X, Y, document_lengths = sort_ds(X, Y)\n",
        "\n",
        "  # Get tensor sizes\n",
        "  max_size = torch.max(document_lengths).item()\n",
        "\n",
        "  # Pad tensor each element\n",
        "  X = pad(X, max_size)\n",
        "\n",
        "  # Transform the batch to a tensor\n",
        "  X_tensor = batch_to_tensor(X, max_size)\n",
        "  \n",
        "  # Return the padded sequence object\n",
        "  return X_tensor, torch.from_numpy(Y.copy()), document_lengths\n",
        "\n",
        "def get_data(batch_size: int, collate_fn):\n",
        "  batch = MovieReviewsCorpus()\n",
        "\n",
        "  dataset = MovieReviewsDataset(batch.movie_reviews_dataset_raw())\n",
        "  unsup_ds = MovieReviewsDataset(batch.movie_reviews_dataset_raw())\n",
        "\n",
        "  # Random Split\n",
        "\n",
        "  train_indexes, test_indexes = train_test_split(list(range(len(dataset.labels))), test_size = 0.2,\n",
        "                                                 stratify = dataset.labels, random_state = 42)\n",
        "  \n",
        "  train_ds = Subset(dataset, train_indexes)\n",
        "  test_ds = Subset(dataset, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "  test_loader = DataLoader(test_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "  unsup_loader = DataLoader(test_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "\n",
        "  return train_loader, test_loader, unsup_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader, unsup_loader = get_data(128, collate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ85bpeYi8xN",
        "outputId": "a33638a1-72d8-4ead-a539-92c63df591a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eDrwHvOIEsl2",
        "outputId": "32867e0e-667b-402a-a351-b0223425164e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "\t Train: CE loss 0.02363, Entropy loss 0.00727, Accuracy 52.56\n",
            "\t Test Accuracy 57.50\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02188, Entropy loss 0.00929, Accuracy 59.19\n",
            "\t Test Accuracy 58.25\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.02113, Entropy loss 0.01152, Accuracy 61.44\n",
            "\t Test Accuracy 62.25\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.02113, Entropy loss 0.01365, Accuracy 62.19\n",
            "\t Test Accuracy 62.75\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.02067, Entropy loss 0.01524, Accuracy 63.38\n",
            "\t Test Accuracy 59.00\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.02081, Entropy loss 0.01651, Accuracy 65.25\n",
            "\t Test Accuracy 62.25\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.02058, Entropy loss 0.01740, Accuracy 63.56\n",
            "\t Test Accuracy 60.00\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.02048, Entropy loss 0.01791, Accuracy 64.12\n",
            "\t Test Accuracy 61.00\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.02043, Entropy loss 0.01833, Accuracy 66.25\n",
            "\t Test Accuracy 58.75\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.02045, Entropy loss 0.01865, Accuracy 65.12\n",
            "\t Test Accuracy 58.75\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.02038, Entropy loss 0.01879, Accuracy 64.31\n",
            "\t Test Accuracy 59.50\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.02040, Entropy loss 0.01896, Accuracy 64.62\n",
            "\t Test Accuracy 59.50\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.02036, Entropy loss 0.01896, Accuracy 66.12\n",
            "\t Test Accuracy 59.75\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.02018, Entropy loss 0.01894, Accuracy 66.00\n",
            "\t Test Accuracy 59.25\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.02016, Entropy loss 0.01892, Accuracy 66.56\n",
            "\t Test Accuracy 59.50\n",
            "Epoch: 16\n",
            "\t Train: CE loss 0.02020, Entropy loss 0.01905, Accuracy 66.75\n",
            "\t Test Accuracy 58.50\n",
            "Epoch: 17\n",
            "\t Train: CE loss 0.02000, Entropy loss 0.01886, Accuracy 67.00\n",
            "\t Test Accuracy 59.00\n",
            "Epoch: 18\n",
            "\t Train: CE loss 0.02013, Entropy loss 0.01886, Accuracy 66.12\n",
            "\t Test Accuracy 59.25\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-480faa3fb64d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_uda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsup_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Overall accuracy: {accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d83aefda9d10>\u001b[0m in \u001b[0;36mmain_uda\u001b[0;34m(source_train_loader, target_train_loader, target_test_loader, device, epochs, nr_classes)\u001b[0m\n\u001b[1;32m     24\u001b[0m     train_ce_loss, train_en_loss, train_accuracy = training_step_uda(net=net, src_data_loader=source_train_loader, \n\u001b[1;32m     25\u001b[0m                                                         \u001b[0mtarget_data_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                                         optimizer=optimizer, lam=lam, e=e, device=device)\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-dfb469f8b180>\u001b[0m in \u001b[0;36mtraining_step_uda\u001b[0;34m(net, src_data_loader, target_data_loader, optimizer, lam, e, device)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mc_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mc_s_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a5f199aa2478>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sizes\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 773\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "accuracy = main_uda(train_loader, unsup_loader, test_loader, device = \"cuda\", epochs = 25)\n",
        "\n",
        "print(f\"Overall accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pBBa_vvYSnL"
      },
      "outputs": [],
      "source": [
        " \"\"\"\n",
        "tensor([1315, 1222, 1011, 1010,  936,  862,  814,  807,  807,  764,  718,  515,\n",
        "         495,  388,  344,  323])\n",
        "tensor([1617, 1361, 1311, 1178, 1081, 1068,  958,  941,  925,  768,  688,  619,\n",
        "         604,  573,  484,  405])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0Af9ORg9pFKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P67fhfYpSZA"
      },
      "source": [
        "# First try to parse phrases documet-wise, then try to parse each phrase of a document separately, and then aggregate the result (if there are more positive phrases then positive, otherwise negative). (Try also to give a weight depending on the number of sentiment lexemes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8-04GhL4p89"
      },
      "source": [
        "### Training procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBD3vt1v4r65"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM_dyKuc4sww"
      },
      "source": [
        "### Main function containing also cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mK4Rmfr4ziT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA1Jgp7d_MgC"
      },
      "source": [
        "### (Possible improvement, apply UDA to GLOVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8toXOrs_T2C"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "bMm0vFo00oLS",
        "outputId": "0c6e196d-48e3-4c2b-b5b1-3e1337ae226d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-253956998aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandford\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandfordTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandfordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk.tokenize.standford'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize.stanford import StanfordTokenizer\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class VectorizerPipeline():\n",
        "\n",
        "  def __init__(self, corpus, pipe= {\"tokenizer\": \"stanford\", \"embedding\": \"glove\"}, embedding_size: int = 300):\n",
        "    self.corpus = corpus\n",
        "    if embedding_size:\n",
        "      self.embedding_size = embedding_size\n",
        "    else:\n",
        "      self.embedding_size = 300\n",
        "    \n",
        "    self._allowed = {\n",
        "        \"tokenizer\": [\"stanford\"],\n",
        "        \"embedding\": [\"glove\"],\n",
        "        \"lemmatizer\": [],\n",
        "        \"stop-word-removal\": [],\n",
        "    }\n",
        "    self.pipe = {\n",
        "        \"tokenizer\": None,\n",
        "        \"embedding\": None,\n",
        "        \"lemmatizer\": None,\n",
        "        \"stop-word-removal\": None,\n",
        "    }\n",
        "    if pipe:\n",
        "      for key, value in pipe.items():\n",
        "        try:\n",
        "          if pipe[key] in self._allowed[key]:\n",
        "            self.pipe[key] = value\n",
        "          else:\n",
        "            raise ValueError(f\"Invalid type of {key}. \\n Valid {key}s are {self._allowed[key]}\")\n",
        "        except KeyError:\n",
        "          raise KeyError(f\"Invalid step in the pipeline: {key}. \\n valid steps are {list(self._allowed.keys())}\")\n",
        "\n",
        "\n",
        "  def tokenization(self, batch):\n",
        "    tok = StanfordTokenizer()\n",
        "    X = [tok(x) for x in batch]\n",
        "    return X\n",
        "  \n",
        "  def embedding(self, batch):\n",
        "    max_length = max(batch, key=len)\n",
        "\n",
        "  def vectorize(self, batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqmjnrCt_U2K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "c176b223-e9f6-4af2-9a1c-541577ccb9ad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4cce880bcee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovieReviewsCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovieReviewsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovie_reviews_dataset_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c6998c471427>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# list of documents, each document is a list containing words of that document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munprocessed_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_corpus_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c6998c471427>\u001b[0m in \u001b[0;36m_flatten\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c6998c471427>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# Get everything we can from this piece.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tok\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# Update the offset table.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    308\u001b[0m                 \u001b[0;34m\"block reader %s() should return list or tuple.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36m_read_para_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 [\n\u001b[1;32m    136\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sent_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 ]\n\u001b[1;32m    139\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \"\"\"\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \"\"\"\n\u001b[1;32m   1420\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"next_tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \"\"\"\n\u001b[1;32m   1441\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "corpus = MovieReviewsCorpus()\n",
        "\n",
        "dataset = MovieReviewsDataset(corpus.movie_reviews_dataset_raw())\n",
        "\n",
        "pipeline = VectorizerPipeline(corpus)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size = 64, collate_fn = pipeline.vectorize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO3lxc0l3sxI"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from tqdm import tqdm\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "\n",
        "corpus = MovieReviewsCorpus()\n",
        "\n",
        "global_vectors = GloVe(name='840B', dim=300)\n",
        "\n",
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    null_embedding = torch.tensor([0.0]*300)\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "          if torch.equal(embeddings_index.get_vecs_by_tokens(word), null_embedding):\n",
        "            raise KeyError\n",
        "          a[word] = embeddings_index.get_vecs_by_tokens(word)\n",
        "          k += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print()\n",
        "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
        "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x\n",
        "\n",
        "\n",
        "oov = check_coverage(corpus.vocab, global_vectors)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0wewgv1uXX5W28Bi99a0E",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}