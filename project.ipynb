{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinni98/Sentiment-analysis-project/blob/subjectivity/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NBg5KtW5AMy"
      },
      "source": [
        "## Polarity Classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8msH_nw3rf39",
        "outputId": "2ca153e2-1a55-4d63-adeb-f12105767d96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gdrive/My Drive/nlu-project\")"
      ],
      "metadata": {
        "id": "vdSs1gS1rgh_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvHgFZJe4_Xq",
        "outputId": "813f6e8b-eed2-4d68-f922-69e60e19c935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "import torch\n",
        "from nltk.corpus import movie_reviews\n",
        "import numpy as np\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"movie_reviews\")\n",
        "nltk.download(\"subjectivity\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"sentiwordnet\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fglEQLVLtc9C"
      },
      "source": [
        "## Exploratory analysis\n",
        "\n",
        "Firstly let's explore the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dKetqTKrZFE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "2d7a7c37-ca09-400f-eb85-ae19768fc243"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b7195547899d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"neg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"length of each part of the dataset:\\n - pos: {len(pos)} \\n - neg: {len(neg)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'movie_reviews' is not defined"
          ]
        }
      ],
      "source": [
        "mr = movie_reviews\n",
        "neg = mr.paras(categories = \"neg\")\n",
        "pos = mr.paras(categories = \"pos\")\n",
        "print(f\"length of each part of the dataset:\\n - pos: {len(pos)} \\n - neg: {len(neg)}\")\n",
        "print(pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Y4wuDNt7XW"
      },
      "source": [
        "It's easy to see that data comes in the following format:\n",
        "\n",
        "- pos = [doc1, doc2, ..., doc1000] (the same applies for negative sentiment examples)\n",
        "\n",
        "Where each doc has the following structure:\n",
        "\n",
        "- doc1 = [sentence_1, sentence_2, ..., sentence_k]\n",
        "\n",
        "Each sentence is a list of tokens, so the dataset is already tokenized.\n",
        "\n",
        "### Word embedding\n",
        "Since I'm going to use deep learning models, I'm going to choose a word embedding to transform the text into vectors.\n",
        "I'm going to start with a pretrained version of GloVe word embedding.\n",
        "Since is a pre-trained word embedding (hence basically a lookup table), I'm going to check how many words of the vocabulary are covered by the pretrained word embedding model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_SEn4EFxKTd"
      },
      "outputs": [],
      "source": [
        "def create_vocab(corpus_words):\n",
        "    vocab = dict()\n",
        "    for word in corpus_words:\n",
        "      try:\n",
        "        vocab[word] += 1\n",
        "      except:\n",
        "        vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "def get_corpus_words(corpus) -> list:\n",
        "    return [w for doc in corpus for sent in doc for w in sent]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdofBhKWxQHx"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from tqdm import tqdm\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "\n",
        "global_vectors = GloVe(name='840B', dim=300)\n",
        "\n",
        "# function inspired by https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings/notebook\n",
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    null_embedding = torch.tensor([0.0]*300)\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "          if torch.equal(embeddings_index.get_vecs_by_tokens(word), null_embedding):\n",
        "            raise KeyError\n",
        "          a[word] = embeddings_index.get_vecs_by_tokens(word)\n",
        "          k += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print()\n",
        "    print(f'Found embeddings for {len(a) / len(vocab):.2%} of vocab')\n",
        "    print(f'Found embeddings for  {k / (k + i):.2%} of all text')\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x\n",
        "\n",
        "vocab = create_vocab(get_corpus_words(pos + neg))\n",
        "oov = check_coverage(vocab, global_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K0HUOB2-E1V"
      },
      "outputs": [],
      "source": [
        "oov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEjIRmuG-Quz"
      },
      "source": [
        "I'm going to see which are the words that are not covered by the embedding (Out Of Vocabulary words), so I can try to see if there are some tenchniques that can be applied in order to improve coverage.\n",
        "The majority of OOV words aren't related with a praticular sentiment (they are basically nouns or some type punctuation), so they can be safely removed. That happens because unknown words are encoded as $[0] * embedding.length$, so no useful information is added.\n",
        "Others OOV words are regular words surrounded by underscores, so they are not recognized by the fixed word embedding. To avoid this problem I implemented a procedure in order to clean these words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgvlBpnMB8EO"
      },
      "outputs": [],
      "source": [
        "def remove_underscores(corpus):\n",
        "  for doc in corpus:\n",
        "    for sent in doc:\n",
        "      for idx, word in enumerate(sent):\n",
        "        if \"_\" in word:\n",
        "          cleaned_word = _clean_word(word)\n",
        "          sent[idx] = cleaned_word\n",
        "  return corpus\n",
        "\n",
        "\n",
        "def _clean_word(word: str):\n",
        "  word = word.replace(\"_\", \" \")\n",
        "  word = word.split()\n",
        "  word = \" \".join(word)\n",
        "  return word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUphYnDOa785",
        "outputId": "e0b314d1-9afc-4056-bcd1-d731c2fb907a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 39519/39519 [00:01<00:00, 28083.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Found embeddings for 92.48% of vocab\n",
            "Found embeddings for  99.61% of all text\n"
          ]
        }
      ],
      "source": [
        "corpus = pos + neg\n",
        "clean_corpus = remove_underscores(corpus), oov\n",
        "vocab = create_vocab(get_corpus_words(clean_corpus))\n",
        "oov = check_coverage(vocab, global_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cmath import phase\n",
        "from dis import findlabels\n",
        "from unicodedata import name\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import spacy\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "CONTRACTION_MAP =  {\"ain't\": \"is not\",\n",
        "                        \"aren't\": \"are not\",\n",
        "                        \"can't\": \"cannot\",\n",
        "                        \"can't've\": \"cannot have\",\n",
        "                        \"'cause\": \"because\",\n",
        "                        \"could've\": \"could have\",\n",
        "                        \"couldn't\": \"could not\",\n",
        "                        \"couldn't've\": \"could not have\",\n",
        "                        \"didn't\": \"did not\",\n",
        "                        \"doesn't\": \"does not\",\n",
        "                        \"don't\": \"do not\",\n",
        "                        \"hadn't\": \"had not\",\n",
        "                        \"hadn't've\": \"had not have\",\n",
        "                        \"hasn't\": \"has not\",\n",
        "                        \"haven't\": \"have not\",\n",
        "                        \"he'd\": \"he would\",\n",
        "                        \"he'd've\": \"he would have\",\n",
        "                        \"he'll\": \"he will\",\n",
        "                        \"he'll've\": \"he he will have\",\n",
        "                        \"he's\": \"he is\",\n",
        "                        \"how'd\": \"how did\",\n",
        "                        \"how'd'y\": \"how do you\",\n",
        "                        \"how'll\": \"how will\",\n",
        "                        \"how's\": \"how is\",\n",
        "                        \"i'd\": \"i would\",\n",
        "                        \"i'd've\": \"i would have\",\n",
        "                        \"i'll\": \"i will\",\n",
        "                        \"i'll've\": \"i will have\",\n",
        "                        \"i'm\": \"i am\",\n",
        "                        \"i've\": \"i have\",\n",
        "                        \"isn't\": \"is not\",\n",
        "                        \"it'd\": \"it would\",\n",
        "                        \"it'd've\": \"it would have\",\n",
        "                        \"it'll\": \"it will\",\n",
        "                        \"it'll've\": \"it will have\",\n",
        "                        \"it's\": \"it is\",\n",
        "                        \"let's\": \"let us\",\n",
        "                        \"ma'am\": \"madam\",\n",
        "                        \"mayn't\": \"may not\",\n",
        "                        \"might've\": \"might have\",\n",
        "                        \"mightn't\": \"might not\",\n",
        "                        \"mightn't've\": \"might not have\",\n",
        "                        \"must've\": \"must have\",\n",
        "                        \"mustn't\": \"must not\",\n",
        "                        \"mustn't've\": \"must not have\",\n",
        "                        \"needn't\": \"need not\",\n",
        "                        \"needn't've\": \"need not have\",\n",
        "                        \"o'clock\": \"of the clock\",\n",
        "                        \"oughtn't\": \"ought not\",\n",
        "                        \"oughtn't've\": \"ought not have\",\n",
        "                        \"shan't\": \"shall not\",\n",
        "                        \"sha'n't\": \"shall not\",\n",
        "                        \"shan't've\": \"shall not have\",\n",
        "                        \"she'd\": \"she would\",\n",
        "                        \"she'd've\": \"she would have\",\n",
        "                        \"she'll\": \"she will\",\n",
        "                        \"she'll've\": \"she will have\",\n",
        "                        \"she's\": \"she is\",\n",
        "                        \"should've\": \"should have\",\n",
        "                        \"shouldn't\": \"should not\",\n",
        "                        \"shouldn't've\": \"should not have\",\n",
        "                        \"so've\": \"so have\",\n",
        "                        \"so's\": \"so as\",\n",
        "                        \"that'd\": \"that would\",\n",
        "                        \"that'd've\": \"that would have\",\n",
        "                        \"that's\": \"that is\",\n",
        "                        \"there'd\": \"there would\",\n",
        "                        \"there'd've\": \"there would have\",\n",
        "                        \"there's\": \"there is\",\n",
        "                        \"they'd\": \"they would\",\n",
        "                        \"they'd've\": \"they would have\",\n",
        "                        \"they'll\": \"they will\",\n",
        "                        \"they'll've\": \"they will have\",\n",
        "                        \"they're\": \"they are\",\n",
        "                        \"they've\": \"they have\",\n",
        "                        \"to've\": \"to have\",\n",
        "                        \"wasn't\": \"was not\",\n",
        "                        \"we'd\": \"we would\",\n",
        "                        \"we'd've\": \"we would have\",\n",
        "                        \"we'll\": \"we will\",\n",
        "                        \"we'll've\": \"we will have\",\n",
        "                        \"we're\": \"we are\",\n",
        "                        \"we've\": \"we have\",\n",
        "                        \"weren't\": \"were not\",\n",
        "                        \"what'll\": \"what will\",\n",
        "                        \"what'll've\": \"what will have\",\n",
        "                        \"what're\": \"what are\",\n",
        "                        \"what's\": \"what is\",\n",
        "                        \"what've\": \"what have\",\n",
        "                        \"when's\": \"when is\",\n",
        "                        \"when've\": \"when have\",\n",
        "                        \"where'd\": \"where did\",\n",
        "                        \"where's\": \"where is\",\n",
        "                        \"where've\": \"where have\",\n",
        "                        \"who'll\": \"who will\",\n",
        "                        \"who'll've\": \"who will have\",\n",
        "                        \"who's\": \"who is\",\n",
        "                        \"who've\": \"who have\",\n",
        "                        \"why's\": \"why is\",\n",
        "                        \"why've\": \"why have\",\n",
        "                        \"will've\": \"will have\",\n",
        "                        \"won't\": \"will not\",\n",
        "                        \"won't've\": \"will not have\",\n",
        "                        \"would've\": \"would have\",\n",
        "                        \"wouldn't\": \"would not\",\n",
        "                        \"wouldn't've\": \"would not have\",\n",
        "                        \"y'all\": \"you all\",\n",
        "                        \"y'all'd\": \"you all would\",\n",
        "                        \"y'all'd've\": \"you all would have\",\n",
        "                        \"y'all're\": \"you all are\",\n",
        "                        \"y'all've\": \"you all have\",\n",
        "                        \"you'd\": \"you would\",\n",
        "                        \"you'd've\": \"you would have\",\n",
        "                        \"you'll\": \"you will\",\n",
        "                        \"you'll've\": \"you will have\",\n",
        "                        \"you're\": \"you are\",\n",
        "                        \"you've\": \"you have\",\n",
        "                    }\n",
        "\n",
        "class PipelineElement(ABC):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  @abstractmethod\n",
        "  def __call__(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "class Pipeline():\n",
        "  def __init__(self, *args):\n",
        "    self.pipeline = []\n",
        "    for arg in args:\n",
        "      self.add_pipeline_element(arg)\n",
        "\n",
        "  def add_pipeline_element(self, element: PipelineElement):\n",
        "    if not issubclass(type(element), PipelineElement):\n",
        "      raise TypeError(\"Wrong element type, only Pipeline elements subclasses can be added\")\n",
        "    self.pipeline.append(element)\n",
        "  \n",
        "  def pipe(self, corpus):\n",
        "    for el in self.pipeline:\n",
        "        corpus = el(corpus)\n",
        "    return corpus\n",
        "  \n",
        "  def __call__(self, *args, **kwds):\n",
        "      if args[0] == None:\n",
        "          raise ValueError(\"Need a corpus as argument\")\n",
        "      corpus = args[0]\n",
        "      return self.pipe(corpus)\n",
        "        \n",
        "# Flattened Elements\n",
        "\n",
        "class UnderscoreRemoverFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Assumes the corpus is flat (i.e. it is not )\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(UnderscoreRemoverFlat, self).__init__()\n",
        "\n",
        "  def remove_underscores(self, corpus):\n",
        "    \"\"\"\n",
        "    Solves the problem where some of the words are surrounded by underscores\n",
        "    (e.g. \"_hello_\")\n",
        "    \"\"\"\n",
        "    for doc in corpus:\n",
        "        for idx, word in enumerate(doc):\n",
        "            if \"_\" in word:\n",
        "                cleaned_word = self._clean_word(word)\n",
        "                doc[idx] = cleaned_word\n",
        "    return corpus\n",
        "\n",
        "\n",
        "  def _clean_word(self, word: str):\n",
        "    word = word.replace(\"_\", \" \")\n",
        "    # remove spaces before and after the word\n",
        "    word = word.split()\n",
        "    word = \" \".join(word)\n",
        "    return word\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_underscores(corpus)\n",
        "\n",
        "class CharacterRepetitionRemoverFlat(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(CharacterRepetitionRemoverFlat, self).__init__()\n",
        "\n",
        "  def reducing_character_repetitions(self, corpus):\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "        new_doc = [self._clean_repetitions(w) for w in doc]\n",
        "        new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  # inspired by https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a\n",
        "  def _clean_repetitions(self, word):\n",
        "    \"\"\"\n",
        "    This Function will reduce repetition to two characters \n",
        "    for alphabets and to one character for punctuations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        word: str                \n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Finally formatted text with alphabets repeating to \n",
        "        one characters & punctuations limited to one repetition \n",
        "        \n",
        "    Example:\n",
        "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
        "    Output : Really, Great !?.;:)\n",
        "\n",
        "    \"\"\"\n",
        "    # Pattern matching for all case alphabets\n",
        "    pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "\n",
        "    # Limiting all the repetitions to two characters.\n",
        "    # MODIFIED: keep only one repetition of the character\n",
        "    formatted_text = pattern_alpha.sub(r\"\\1\\1\", word) \n",
        "\n",
        "    # Pattern matching for all the punctuations that can occur\n",
        "    pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "\n",
        "    # Limiting punctuations in previously formatted string to only one.\n",
        "    combined_formatted = pattern_punct.sub(r'\\1', formatted_text)\n",
        "\n",
        "    # The below statement is replacing repetitions of spaces that occur more than two times with that of one occurrence.\n",
        "    final_formatted = re.sub(' {2,}',' ', combined_formatted)\n",
        "    return final_formatted\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.reducing_character_repetitions(corpus)\n",
        "\n",
        "class ApostrophesMergerFlat(PipelineElement):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ApostrophesMergerFlat, self).__init__()\n",
        "\n",
        "  def merge_apostrophes(self, corpus):\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      indexes = self._get_neg_indexes(doc)\n",
        "      for el in indexes:\n",
        "        doc[el[0]:el[1]] = [\"\".join(doc[el[0]:el[1]])]\n",
        "      new_corpus.append(doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def _get_neg_indexes(self, sent):\n",
        "    contr = [\"t\", \"ve\", \"re\", \"ll\", \"d\", \"all\", \"y\", \"cause\", \"m\", \"clock\", \"am\"] #, \"s\"]\n",
        "    indexes = []\n",
        "    for idx, word in enumerate(sent):\n",
        "      # Try-except to avoid out of range indexes (there can be some \"'\" a the beginning or end of the phrase)\n",
        "      try:\n",
        "        if word==\"'\" and sent[idx+1] in contr:\n",
        "          indexes.append((idx-1,idx+2))\n",
        "      except:\n",
        "        pass\n",
        "    return indexes\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.merge_apostrophes(corpus)\n",
        "\n",
        "\n",
        "class ContractionCleanerFlat(PipelineElement):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ContractionCleanerFlat, self).__init__()\n",
        "\n",
        "  def clean_contractions(self, corpus):\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for word in doc:\n",
        "        try:\n",
        "            correct = CONTRACTION_MAP[word]\n",
        "            correct = correct.split()\n",
        "            new_doc += correct\n",
        "        except:\n",
        "            new_doc.append(word)\n",
        "      new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_contractions(corpus)\n",
        "\n",
        "class SpecialCharsCleanerFlat(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(SpecialCharsCleanerFlat, self).__init__()\n",
        "\n",
        "  def clean_special_chars(self, corpus):\n",
        "    new_corpus = [[self._clean_special_word(w) for w in doc] for doc in corpus]\n",
        "    new_corpus = [[w for w in doc] for doc in corpus]\n",
        "    return new_corpus\n",
        "    \n",
        "  def _clean_special_word(self, word):\n",
        "    # The formatted text after removing not necessary punctuations.\n",
        "    formatted_text = re.sub(r\"[^a-zA-Z0-9:€$-,%?!]+\", '', word) \n",
        "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
        "    return formatted_text\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_special_chars(corpus)\n",
        "\n",
        "class StopWordsRemoverFlat(PipelineElement):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(StopWordsRemoverFlat, self).__init__()\n",
        "\n",
        "  def remove_stop_words(self, corpus):\n",
        "    stops = stopwords.words(\"english\")\n",
        "    stops = [word for word in stops if \"'t\" not in word or \"not\" not in word]\n",
        "    return [[word for word in doc if word not in stops] for doc in corpus]\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_stop_words(corpus)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# Non-flattened elements\n",
        "\n",
        "class UnderscoreRemover(PipelineElement):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(UnderscoreRemover, self).__init__()\n",
        "\n",
        "  def remove_underscores(self, corpus):\n",
        "    \"\"\"\n",
        "    Solves the problem where some of the words are surrounded by underscores\n",
        "    (e.g. \"_hello_\")\n",
        "    \"\"\"\n",
        "    for doc in corpus:\n",
        "      for sent_idx, sent in enumerate(doc):\n",
        "        new_sent = []\n",
        "        for idx, word in enumerate(sent):\n",
        "          if \"_\" in word:\n",
        "            cleaned_word = self._clean_word(word)\n",
        "            new_sent += cleaned_word\n",
        "          else:\n",
        "            new_sent.append(word)\n",
        "        if len(new_sent) > 0:\n",
        "          doc[sent_idx] = new_sent\n",
        "    return corpus\n",
        "\n",
        "  def _clean_word(self, word: str):\n",
        "    word = word.replace(\"_\", \" \")\n",
        "    # remove spaces before and after the word\n",
        "    word = word.split()\n",
        "    return word\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_underscores(corpus)\n",
        "\n",
        "\n",
        "\n",
        "class CharacterRepetitionRemover(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(CharacterRepetitionRemover, self).__init__()\n",
        "\n",
        "  def reducing_character_repetitions(self, corpus):\n",
        "      new_corpus = [[[self._clean_repetitions(w) for w in sent] for sent in doc] for doc in corpus]\n",
        "      return new_corpus\n",
        "      # inspired by https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a\n",
        "\n",
        "  def _clean_repetitions(self, word):\n",
        "    \"\"\"\n",
        "    This Function will reduce repetition to two characters \n",
        "    for alphabets and to one character for punctuations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        word: str                \n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Finally formatted text with alphabets repeating to \n",
        "        one characters & punctuations limited to one repetition \n",
        "        \n",
        "    Example:\n",
        "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
        "    Output : Really, Great !?.;:)\n",
        "\n",
        "    \"\"\"\n",
        "    # Pattern matching for all case alphabets\n",
        "    pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "\n",
        "    # Limiting all the repetitions to two characters.\n",
        "    # MODIFIED: keep only one repetition of the character\n",
        "    formatted_text = pattern_alpha.sub(r\"\\1\\1\", word) \n",
        "\n",
        "    # Pattern matching for all the punctuations that can occur\n",
        "    pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "\n",
        "    # Limiting punctuations in previously formatted string to only one.\n",
        "    combined_formatted = pattern_punct.sub(r'\\1', formatted_text)\n",
        "\n",
        "    # The below statement is replacing repetitions of spaces that occur more than two times with that of one occurrence.\n",
        "    final_formatted = re.sub(' {2,}',' ', combined_formatted)\n",
        "    return final_formatted\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.reducing_character_repetitions(corpus)\n",
        "\n",
        "\n",
        "class ApostrophesMerger(PipelineElement):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ApostrophesMerger, self).__init__()\n",
        "\n",
        "  def merge_apostrophes(self, corpus):\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for sent in doc:\n",
        "        indexes = self._get_neg_indexes(sent)\n",
        "        for el in indexes:\n",
        "          sent[el[0]:el[1]] = [\"\".join(sent[el[0]:el[1]])]\n",
        "        new_doc.append(sent)\n",
        "      new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def _get_neg_indexes(self, sent):\n",
        "    contr = [\"t\", \"ve\", \"re\", \"ll\", \"d\", \"all\", \"y\", \"cause\", \"m\", \"clock\", \"am\"]#, \"s\"]\n",
        "    indexes = []\n",
        "    for idx, word in enumerate(sent):\n",
        "      # Try-except to avoid out of range indexes (there can be some \"'\" a the beginning or end of the phrase)\n",
        "      try:\n",
        "        if word==\"'\" and sent[idx+1] in contr:\n",
        "          indexes.append((idx-1,idx+2))\n",
        "      except:\n",
        "        pass\n",
        "      return indexes\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.merge_apostrophes(corpus)\n",
        "\n",
        "\n",
        "class ContractionCleaner(PipelineElement):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ContractionCleaner, self).__init__()\n",
        "\n",
        "  def clean_contractions(self, corpus):\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for sent in doc:\n",
        "        new_sent = []\n",
        "        for word in sent:\n",
        "          try:\n",
        "            correct = CONTRACTION_MAP[word]\n",
        "            correct = correct.split()\n",
        "            new_sent += correct\n",
        "          except:\n",
        "            new_sent.append(word)\n",
        "        new_doc.append(new_sent)\n",
        "      new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_contractions(corpus)\n",
        "\n",
        "\n",
        "class SpecialCharsCleaner(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(SpecialCharsCleaner, self).__init__()\n",
        "  \n",
        "  def clean_special_chars(self, corpus):\n",
        "      for idx_doc, doc in enumerate(corpus):\n",
        "        for sent_idx, sent in enumerate(doc):\n",
        "          new_sent = []\n",
        "          for word_idx, word in enumerate(sent):\n",
        "            new_word = self._clean_special_word(word)\n",
        "            if new_word != \" \":\n",
        "              new_sent += new_word.split()\n",
        "          if len(new_sent) > 0:\n",
        "            doc[sent_idx] = new_sent\n",
        "      return corpus\n",
        "    \n",
        "  def _clean_special_word(self, word):\n",
        "    # The formatted text after removing not necessary punctuations.\n",
        "    formatted_text = re.sub(r\"[^a-zA-Z0-9:€$-,%?!]+\", ' ', word) \n",
        "    return formatted_text\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_special_chars(corpus)\n",
        "\n",
        "class StopWordsRemover(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(StopWordsRemover, self).__init__()\n",
        "\n",
        "  def remove_stop_words(self, corpus):\n",
        "    stops = stopwords.words(\"english\")\n",
        "    # Don't want to remove stop words associated with negations\n",
        "    stops = [word for word in stops if \"'t\" not in word or \"not\" not in word]\n",
        "    return [[[word for word in sent if word not in stops] for sent in doc] for doc in corpus]\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_stop_words(corpus)\n"
      ],
      "metadata": {
        "id": "spYv4QqyqJ0H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skv2-rEQwMtR"
      },
      "source": [
        "### Corpus class\n",
        "I'm going to create a class for the representation of the corpus in order to have a self contained way to have all the functions that may be useful for the processing of the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C3PyvlOV60V7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "import numpy as np\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "\n",
        "class MovieReviewsCorpusPhrases():\n",
        "    def __init__(self, preprocess_pipeline = None):\n",
        "        \"\"\"\n",
        "        If non preprocess_pipeline is given, the text gets tokenized by default\n",
        "        using spacy tokenizer\n",
        "        \"\"\"\n",
        "        self.mr = movie_reviews\n",
        "        if preprocess_pipeline != None and not isinstance(preprocess_pipeline, MRPipelinePhrases):\n",
        "            raise ValueError(f\"preprocess_pipeline is not valid, you should pass \\\n",
        "                                a MRPipelinePhrases object or None\")\n",
        "        self.pipeline = preprocess_pipeline\n",
        "        self.raw_corpus, self.labels = self._get_raw_corpus()\n",
        "        if self.pipeline == None:\n",
        "            self.processed_corpus = self.raw_corpus\n",
        "        else:\n",
        "            # Flattened and preprocessed corpus\n",
        "            self.processed_corpus = self._preprocess()\n",
        "        \n",
        "        self.vocab = self._create_vocab()\n",
        "        \n",
        "\n",
        "    def _get_raw_corpus(self):\n",
        "        neg = [self.mr.raw(doc) for doc in self.mr.fileids()[:1000]]\n",
        "        pos = [self.mr.raw(doc) for doc in self.mr.fileids()[1000:]]\n",
        "        labels = [0]*len(neg) + [1]*len(pos)\n",
        "        return neg + pos, labels\n",
        "    \n",
        "    def _preprocess(self):\n",
        "        if self.pipeline != None:\n",
        "            return self.pipeline(self.raw_corpus)\n",
        "        else:\n",
        "            return self.raw_corpus\n",
        "        \n",
        "    def _create_vocab(self):\n",
        "        vocab = dict()\n",
        "        corpus_words = [w for doc in self.processed_corpus for w in doc]\n",
        "        for word in corpus_words:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except:\n",
        "                vocab[word] = 1\n",
        "        return vocab\n",
        "\n",
        "    def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            A 2D which each row has the corresponding embedding from the vocabulary\n",
        "        \"\"\"\n",
        "        matrix_length = len(self.vocab)\n",
        "        embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "        # If I use torch.zeros directly it crashes (don't know why)\n",
        "        embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "        null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "        for idx, key in enumerate(self.vocab.keys()):\n",
        "            if torch.equal(embedding[key], null_embedding):\n",
        "                embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "            else:\n",
        "                embedding_matrix[idx] = embedding[key]\n",
        "                \n",
        "        return embedding_matrix\n",
        "    \n",
        "    def get_indexed_corpus(self):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        Dictionary\n",
        "            Containing correspondences word -> index\n",
        "        \n",
        "        list(list(torch.tensor))\n",
        "            The corpus represented as indexes corresponding to each word\n",
        "        \"\"\"\n",
        "        vocab = {}\n",
        "        for idx, key in enumerate(self.vocab.keys()):\n",
        "            vocab[key] = idx\n",
        "        \n",
        "        indexed_corpus = [torch.tensor([torch.tensor(vocab[w], dtype=torch.int32) for w in doc]) for doc in self.processed_corpus]\n",
        "        return indexed_corpus, self.labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.processed_corpus)\n",
        "\n",
        "\n",
        "class MovieReviewsCorpus():\n",
        "    def __init__(self, preprocess_pipeline = None):\n",
        "        # list of documents, each document is a list containing words of that document\n",
        "        self.mr = movie_reviews\n",
        "        self.pipeline = preprocess_pipeline\n",
        "        # Corpus as list of documents. Documents as list of sentences. Sentences as list of tokens\n",
        "        self.unprocessed_corpus, self.labels = self._get_corpus()\n",
        "        # Corpus as list of documents. Documents as list of tokens\n",
        "        self.flattened_corpus = self._flatten()\n",
        "        if preprocess_pipeline == None:\n",
        "            self.processed_corpus = self.flattened_corpus\n",
        "        else:\n",
        "            # Flattened and preprocessed corpus\n",
        "            self.processed_corpus = self._preprocess()\n",
        "\n",
        "        self.corpus_words = self.get_corpus_words()\n",
        "        self.vocab = self._create_vocab()\n",
        "\n",
        "\n",
        "\n",
        "    def _list_to_str(self, doc) -> str:\n",
        "        \"\"\"\n",
        "        Put all elements of the list into a single string, separating each element with a space.\n",
        "        \"\"\"\n",
        "        return \" \".join([w for sent in doc for w in sent])\n",
        "\n",
        "    def _preprocess(self):\n",
        "        return self.pipeline(self.flattened_corpus)\n",
        "\n",
        "    def _flatten(self):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        list[list[str]]\n",
        "            Each inner list represents a document. Each document is a list of tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        # 3 nested list: each list contain a document, each inner list contains a phrase (until fullstop), each phrase contains words.\n",
        "\n",
        "        corpus = [[w for w in self._list_to_str(d).split(\" \")] for d in self.unprocessed_corpus]\n",
        "        return corpus\n",
        "\n",
        "    def _get_corpus(self):\n",
        "        neg = self.mr.paras(categories = \"neg\")\n",
        "        pos = self.mr.paras(categories = \"pos\")\n",
        "        labels = [0] * len(pos) + [1] * len(neg)\n",
        "        return neg + pos, labels\n",
        "\n",
        "    def movie_reviews_dataset_raw(self):\n",
        "        \"\"\"\n",
        "        Returns the dataset containing:\n",
        "\n",
        "        - A list of all the documents\n",
        "        - The corresponding label for each document\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple(list, list)\n",
        "            The dataset: first element is the list of the document, the second element of the tuple is the associated label (positive or negative) for each document\n",
        "        \"\"\"\n",
        "\n",
        "        return self.flattened_corpus, self.labels\n",
        "\n",
        "    def get_sentence_ds(self):\n",
        "        neg = self.mr.paras(categories = \"neg\")\n",
        "        pos = self.mr.paras(categories = \"pos\")\n",
        "\n",
        "        pos = [phrase for doc in pos for phrase in doc]\n",
        "        neg = [phrase for doc in neg for phrase in doc]\n",
        "\n",
        "        labels = np.array([0] * len(pos) + [1] * len(neg))\n",
        "        corpus = neg+pos\n",
        "        return corpus, labels\n",
        "\n",
        "\n",
        "    def get_corpus_words(self) -> list:\n",
        "        return [w for doc in self.processed_corpus for w in doc]\n",
        "    \n",
        "    def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            A 2D which each row has the corresponding embedding from the vocabulary\n",
        "        \"\"\"\n",
        "        matrix_length = len(self.vocab)\n",
        "        embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "        # If I use torch.zeros directly it crashes (don't know why)\n",
        "        embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "        null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "        for idx, key in enumerate(self.vocab.keys()):\n",
        "            if torch.equal(embedding[key], null_embedding):\n",
        "                embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "            else:\n",
        "                embedding_matrix[idx] = embedding[key]\n",
        "                \n",
        "        return embedding_matrix\n",
        "    \n",
        "    def get_fasttext_embedding_matrix(self, embedding, embedding_dim):\n",
        "        matrix_length = len(self.vocab)\n",
        "        embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "        # If I use torch.zeros directly it crashes (don't know why)\n",
        "        embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "        null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "        for idx, key in enumerate(self.vocab.keys()):\n",
        "            tensor_embedding = torch.from_numpy(embedding[key].copy())\n",
        "            if torch.equal(tensor_embedding, null_embedding):\n",
        "                embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "            else:\n",
        "                embedding_matrix[idx] = tensor_embedding\n",
        "                \n",
        "        return embedding_matrix\n",
        "    \n",
        "    def get_indexed_corpus(self):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        Dictionary\n",
        "            Containing correspondences word -> index\n",
        "        \n",
        "        list(list(torch.tensor))\n",
        "            The corpus represented as indexes corresponding to each word\n",
        "        \"\"\"\n",
        "        vocab = {}\n",
        "        for idx, key in enumerate(self.vocab.keys()):\n",
        "            vocab[key] = idx\n",
        "        \n",
        "        indexed_corpus = [torch.tensor([torch.tensor(vocab[w], dtype=torch.int32) for w in doc]) for doc in self.processed_corpus]\n",
        "        return indexed_corpus, self.labels\n",
        "\n",
        "\n",
        "    def _create_vocab(self):\n",
        "        vocab = dict()\n",
        "        for word in self.corpus_words:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except:\n",
        "                vocab[word] = 1\n",
        "        return vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.flattened_corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zZaF5yV9rlo-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class MovieReviewsDataset(Dataset):\n",
        "  def __init__(self, raw_dataset):\n",
        "    super(MovieReviewsDataset, self).__init__()\n",
        "    self.corpus = np.array(raw_dataset[0], dtype = object)\n",
        "    self.targets = np.array(raw_dataset[1], dtype = np.int64)\n",
        "    self.max_element = len(max(self.corpus, key=lambda x: len(x)))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    item = self.corpus[index]\n",
        "    label = self.targets[index]\n",
        "    return (item, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr7PPG1k4gFq"
      },
      "source": [
        "### Create the model class\n",
        "Let's first try with a simple BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bCvqaJ8-hKmT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix = None, device = \"cuda\", input_size = 300, hidden_size = 128, output_size = 2):\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        if embedding_matrix != None:\n",
        "          self.embedding = self.create_embedding_layer(embedding_matrix)\n",
        "        else:\n",
        "          self.embedding = None\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True, bidirectional=True)\n",
        "        self.fc = nn.Sequential(nn.ReLU(),\n",
        "                                nn.BatchNorm1d(hidden_size*2, eps = 1e-08),\n",
        "                                nn.Dropout(0.3),\n",
        "                                nn.Linear(hidden_size*2, output_size)\n",
        "                                )\n",
        "\n",
        "    def create_embedding_layer(self, embedding_matrix):\n",
        "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "        emb_layer = nn.Embedding(num_embeddings, embedding_dim, -1)\n",
        "        emb_layer.load_state_dict({\"weight\": embedding_matrix})\n",
        "        return emb_layer\n",
        "\n",
        "    # function taken from https://discuss.pytorch.org/t/how-to-use-pack-sequence-if-we-are-going-to-use-word-embedding-and-bilstm/28184/4\n",
        "    def simple_elementwise_apply(self, fn, packed_sequence):\n",
        "        \"\"\"applies a pointwise function fn to each element in packed_sequence\"\"\"\n",
        "        return torch.nn.utils.rnn.PackedSequence(fn(packed_sequence.data), packed_sequence.batch_sizes)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        if self.cuda:\n",
        "            return (torch.zeros(2, batch_size, self.hidden_size).to(self.device),\n",
        "                    torch.zeros(2, batch_size, self.hidden_size).to(self.device),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.batch_sizes[0].item()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        x = self.simple_elementwise_apply(self.embedding, x)\n",
        "\n",
        "        # output: batch_size, sequence_length, hidden_size * 2 (since is bilstm)\n",
        "        out, _ = self.lstm(x, hidden)\n",
        "        out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "        # Interested only in the last layer\n",
        "        out = out[list(range(batch_size)), input_sizes - 1, :]\n",
        "        out = self.fc(out)\n",
        "        out = out.squeeze()\n",
        "        return out\n",
        "\n",
        "class BiLSTMAttention(BiLSTM):\n",
        "    # BiLSTM with attention inspired by the following paper: https://aclanthology.org/S18-1040.pdf\n",
        "    def __init__(self, embedding_matrix = None, device=\"cuda\", input_size=300,\n",
        "                 hidden_size=128, context_size = None, output_size=2):\n",
        "        super(BiLSTMAttention, self).__init__(embedding_matrix, device, input_size, hidden_size, output_size)\n",
        "        # Not self attention :)\n",
        "        if context_size != None:\n",
        "          self.attention = nn.Linear(self.hidden_size * 2, context_size)\n",
        "          self.history = nn.Parameter(torch.randn(context_size))\n",
        "        else:\n",
        "          self.attention = nn.Linear(self.hidden_size * 2, 1)\n",
        "          self.history = None\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.batch_sizes[0].item()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        if self.embedding != None:\n",
        "          x = self.simple_elementwise_apply(self.embedding, x)\n",
        "\n",
        "        # output: batch_size, sequence_length, hidden_size * 2 (since is bilstm)\n",
        "        out, _ = self.lstm(x, hidden)\n",
        "        out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "\n",
        "        if self.history == None:\n",
        "          attention_values = torch.tanh(self.attention(out)).squeeze(dim = 2)\n",
        "          attention_weights = torch.softmax(attention_values, dim = 1).unsqueeze(1)\n",
        "          # n_docs, sequence_length\n",
        "        else:\n",
        "          attention_values = torch.tanh(self.attention(out))\n",
        "          attention_weights = torch.softmax(attention_values.matmul(self.history), dim = 1).unsqueeze(1)\n",
        "          # n_docs, sequence_length\n",
        "\n",
        "        out = torch.sum(attention_weights.matmul(out), dim = 1)\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        attention_weights = attention_weights.squeeze(dim = 1)\n",
        "        att = [doc[:input_sizes[idx]] for idx, doc in enumerate(attention_weights)]\n",
        "\n",
        "        return out, att\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yZFg9BvQdZHB"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Sigmoid\n",
        "\n",
        "def training_step(net, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for batch_idx, (inputs, targets, _) in enumerate(data_loader):\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    in_size = targets.size(dim=0)\n",
        "\n",
        "    outputs, _ = net(inputs)\n",
        "\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LJef4h1cfWym"
      },
      "outputs": [],
      "source": [
        "def test_step(net, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets, _) in enumerate(data_loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      in_size = targets.size(dim=0)\n",
        "\n",
        "      outputs, _ = net(inputs)\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = outputs.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OOeUmEHZNBvw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "def main(train_loader, test_loader, embedding_matrix, device = \"cuda\", epochs = 10):\n",
        "\n",
        "  net = BiLSTMAttention(embedding_matrix, device = device, input_size=300).to(device)\n",
        "\n",
        "  optimizer = Adam(net.parameters(), 0.001, betas = (0.9, 0.9), amsgrad=True)\n",
        "\n",
        "  cost_function = nn.CrossEntropyLoss(weight = torch.tensor([1.0, 500.0]).to(device))\n",
        "\n",
        "  scheduler = ExponentialLR(optimizer, 0.9)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    test_loss, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "    print(f\"Test loss: {test_loss} \\n Test accuracy: {test_accuracy}\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "\n",
        "    # scheduler.step()\n",
        "  \n",
        "  _, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "\n",
        "  return test_accuracy, net\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GfNtrS8jmATx"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def pad(batch, max_size):\n",
        "    try:\n",
        "      pad = torch.tensor([-1]*batch[0].size(dim=1), dtype = torch.float).to(\"cuda\")\n",
        "      embedded = 1\n",
        "    except:\n",
        "      pad = torch.tensor([-1])\n",
        "      embedded = 0\n",
        "    for idx in range(len(batch)):\n",
        "        remaining = max_size - batch[idx].size(dim = 0)\n",
        "        abc = pad.repeat(remaining)\n",
        "        if embedded:\n",
        "          batch[idx] = torch.cat((batch[idx], pad.repeat(remaining, 1)), dim = 0)\n",
        "        else:\n",
        "          batch[idx] = torch.cat((batch[idx], pad.repeat(remaining)), dim = 0)\n",
        "    return batch\n",
        "\n",
        "def batch_to_tensor(X: List[torch.tensor], max_size):\n",
        "    try:\n",
        "      X_tensor = torch.zeros((len(X), max_size, X[0].size(dim=1)), dtype=torch.float).to(\"cuda\")\n",
        "    except:\n",
        "      X_tensor = torch.zeros((len(X), max_size), dtype=torch.int32)\n",
        "\n",
        "    for i, embed in enumerate(X):\n",
        "        X_tensor[i] = embed\n",
        "    return X_tensor\n",
        "\n",
        "def sort_ds(X, Y):\n",
        "    \"\"\"\n",
        "    Sort inputs by document lengths\n",
        "    \"\"\"\n",
        "    document_lengths = np.array([tens.size(dim = 0) for tens in X])\n",
        "    indexes = np.argsort(document_lengths)\n",
        "    document_lengths = document_lengths.tolist()\n",
        "\n",
        "    X_sorted = [X[idx] for idx in indexes][::-1]\n",
        "    Y_sorted = [Y[idx] for idx in indexes][::-1]\n",
        "    document_lengths = torch.tensor([document_lengths[idx] for idx in indexes][::-1])\n",
        "\n",
        "    return X_sorted, Y_sorted, document_lengths, indexes\n",
        "\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    X, Y = list(zip(*batch))\n",
        "    # Sort dataset\n",
        "    X, Y, document_lengths, indexes = sort_ds(X, Y)\n",
        "\n",
        "    # Get tensor sizes\n",
        "    max_size = torch.max(document_lengths).item()\n",
        "\n",
        "    # Pad tensor each element\n",
        "    X = pad(X, max_size)\n",
        "\n",
        "    # Transform the batch to a tensor\n",
        "    X_tensor = batch_to_tensor(X, max_size)\n",
        "    Y_tensor = torch.tensor(Y)\n",
        "    # Return the padded sequence object\n",
        "    X_final = pack_padded_sequence(X_tensor, document_lengths, batch_first=True)\n",
        "    return X_final, Y_tensor, indexes\n",
        "\n",
        "\n",
        "def get_data(batch_size: int, dataset, collate_fn, random_state = 42):\n",
        "  # Random Split\n",
        "  train_indexes, test_indexes = train_test_split(list(range(len(dataset.targets))), test_size = 0.2,\n",
        "                                                  stratify = dataset.targets, random_state = random_state)\n",
        "\n",
        "  train_ds = Subset(dataset, train_indexes)\n",
        "  test_ds = Subset(dataset, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "  test_loader = DataLoader(test_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "\n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "def main_cross_validation(main, dataset, embedding_matrix, collate_fn,\n",
        "                          device = \"cuda\", epochs = 20, random_state = 42, batch_size = 128):\n",
        "\n",
        "  targets = np.asarray(dataset.targets, dtype=np.int64)\n",
        "\n",
        "  skf = StratifiedKFold(10, shuffle = True, random_state=random_state)\n",
        "\n",
        "  fold_accuracies = []\n",
        "  \n",
        "  for fold, (train_indexes, val_indexes) in enumerate(skf.split(np.zeros(len(dataset)),\n",
        "                                                      targets)):\n",
        "    print(f\"\\n Fold: {fold}\")\n",
        "    train_sampler = SubsetRandomSampler(train_indexes)\n",
        "    val_sampler = SubsetRandomSampler(val_indexes)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size = batch_size, sampler = train_sampler,\n",
        "                              collate_fn = collate_fn, pin_memory=True)\n",
        "    val_loader = DataLoader(dataset, batch_size = batch_size, sampler = val_sampler,\n",
        "                            collate_fn = collate_fn, pin_memory = True)\n",
        "\n",
        "\n",
        "    val_accuracy = main(train_loader, val_loader, embedding_matrix, device, epochs)\n",
        "    \n",
        "    fold_accuracies.append(val_accuracy)\n",
        "\n",
        "\n",
        "  fold_accuracies = np.array(fold_accuracies)\n",
        "\n",
        "  return fold_accuracies.mean(), fold_accuracies.std()\n",
        "\n"
      ],
      "metadata": {
        "id": "FkRBKES24_0v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lexicon Based Supervised Attention Model"
      ],
      "metadata": {
        "id": "MkeZyHH15r6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieReviewsCorpusLBSA():\n",
        "  def __init__(self, preprocess_pipeline = None):\n",
        "      \"\"\"\n",
        "      If non preprocess_pipeline is given, the text gets tokenized by default\n",
        "      using spacy tokenizer\n",
        "      \"\"\"\n",
        "      self.mr = movie_reviews\n",
        "      self.pipeline = preprocess_pipeline\n",
        "      self.raw_corpus, self.labels = self._get_raw_corpus()\n",
        "      if self.pipeline == None:\n",
        "          self.processed_corpus = self.raw_corpus\n",
        "      else:\n",
        "          # Flattened and preprocessed corpus\n",
        "          self.processed_corpus = self._preprocess()\n",
        "      \n",
        "      self.vocab = self._create_vocab()\n",
        "      \n",
        "\n",
        "  def _get_raw_corpus(self):\n",
        "      neg = self.mr.paras(categories = \"neg\")\n",
        "      pos = self.mr.paras(categories = \"pos\")\n",
        "      labels = [0]*len(neg) + [1]*len(pos)\n",
        "      return neg + pos, labels\n",
        "  \n",
        "  def _preprocess(self):\n",
        "      if self.pipeline != None:\n",
        "          return self.pipeline(self.raw_corpus)\n",
        "      else:\n",
        "          return self.raw_corpus\n",
        "      \n",
        "  def _create_vocab(self):\n",
        "      vocab = dict()\n",
        "      corpus_words = [w for doc in self.processed_corpus for sent in doc for w in sent]\n",
        "      for word in corpus_words:\n",
        "          try:\n",
        "              vocab[word] += 1\n",
        "          except:\n",
        "              vocab[word] = 1\n",
        "      return vocab\n",
        "\n",
        "  def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      np.ndarray\n",
        "          A 2D which each row has the corresponding embedding from the vocabulary\n",
        "      \"\"\"\n",
        "      matrix_length = len(self.vocab)\n",
        "      embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "      # If I use torch.zeros directly it crashes (don't know why)\n",
        "      embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "      null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "          if torch.equal(embedding[key], null_embedding):\n",
        "              embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "          else:\n",
        "              embedding_matrix[idx] = embedding[key]\n",
        "              \n",
        "      return embedding_matrix\n",
        "  \n",
        "  def get_indexed_corpus(self):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      Dictionary\n",
        "          Containing correspondences word -> index\n",
        "      \n",
        "      list(int)\n",
        "          labels associated with each document\n",
        "      \"\"\"\n",
        "      vocab = {}\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "          vocab[key] = idx\n",
        "      \n",
        "      # each doc is a list of tensor which represent sentences, each sentence is a tensor of indexed words\n",
        "      indexed_corpus = [[torch.tensor([vocab[w] for w in sent], dtype=torch.int32) \n",
        "                        for sent in doc]\n",
        "                        for doc in self.processed_corpus]\n",
        "      return indexed_corpus, self.labels\n",
        "  \n",
        "  \n",
        "  \n",
        "  def __len__(self):\n",
        "      return len(self.processed_corpus)\n",
        "\n",
        "c = MovieReviewsCorpusLBSA()"
      ],
      "metadata": {
        "id": "n9gmVtdg_Gf1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "\n",
        "class MovieReviewsDatasetLBSA(Dataset):\n",
        "  def __init__(self, corpus):\n",
        "    super(MovieReviewsDatasetLBSA, self).__init__()\n",
        "    self.corpus = corpus\n",
        "    indexed_corpus = self.corpus.get_indexed_corpus()\n",
        "    # Word level gold attention vector\n",
        "    self.word_lambda = 3\n",
        "    self.sentence_lambda = 3\n",
        "    self.sentiment_degree = self._compute_sentiment_degree()\n",
        "    self.wl_gold_av = self._compute_gold_words()\n",
        "    self.sl_gold_av = self._compute_gold_sents()\n",
        "    self.data = indexed_corpus[0]\n",
        "    self.targets = indexed_corpus[1]\n",
        "  \n",
        "  def _compute_sentiment_degree(self):\n",
        "    senti_vocab = self._build_senti_vocab(self.corpus.vocab)\n",
        "    path = '/content/gdrive/My Drive/nlu-project/lexicons/'\n",
        "    mpqa_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'mpqa/mpqa.json')\n",
        "    bingliu_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'bingliu/bingliu.json')\n",
        "    inquirer_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'inquirer/inquirer.json')\n",
        "    concreteness_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'concreteness/concreteness.json')\n",
        "    twitter_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'twitter/twitter.json')\n",
        "    qwn_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'qwn/qwn.json')\n",
        "    social_sent = self._build_social_sent_vocab(self.corpus.vocab, path + 'social_sent')\n",
        "    senticnet_vocab = self.build_sentic_net_vocab(self.corpus.vocab, path + \"sentic_net/senticnet.txt\")\n",
        "    social_sent = self._build_social_sent_vocab(self.corpus.vocab, path + \"social_sent\")\n",
        "    res = self._compute_average_sentiment_degree(senti_vocab,\n",
        "                                                 mpqa_vocab,\n",
        "                                                 bingliu_vocab,\n",
        "                                                 inquirer_vocab,\n",
        "                                                 concreteness_vocab,\n",
        "                                                 twitter_vocab,\n",
        "                                                 qwn_vocab,\n",
        "                                                 senticnet_vocab,\n",
        "                                                 social_sent\n",
        "                                                 )\n",
        "    \n",
        "    corpus = self.corpus.processed_corpus\n",
        "    scores = [[[res[word] for word in sent] for sent in doc] for doc in corpus]\n",
        "    return scores\n",
        "\n",
        "  def _compute_gold_sents(self):\n",
        "    sentence_sentiment_degree  = [[sum(sent)/len(sent) for sent in doc] for doc in self.sentiment_degree]\n",
        "    gold = [self._normalized_softmax(doc, self.sentence_lambda) for doc in sentence_sentiment_degree]\n",
        "    return gold\n",
        "\n",
        "\n",
        "  def _compute_gold_words(self):\n",
        "    gold = [[self._normalized_softmax(sent_scores, self.word_lambda) for sent_scores in doc] for doc in self.sentiment_degree]\n",
        "    return gold\n",
        "\n",
        "  def _normalized_softmax(self, sequence, lam):\n",
        "    multiplied_sequence = [lam * el for el in sequence]\n",
        "    total = sum([math.exp(el) for el in sequence])\n",
        "    res = torch.tensor([math.exp(lam * el)/total for el in sequence])\n",
        "    return res\n",
        "\n",
        "  def _build_0_1_vocab(self, vocab, path):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/williamleif/socialsent/blob/master/socialsent/data/lexicons/mpqa.json\n",
        "\n",
        "    Values:\n",
        "    - 1 = positive\n",
        "    - 0 = neutral\n",
        "    - -1 = negative\n",
        "    \n",
        "    The absolute value will be taken\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "      lexicon = json.load(f)\n",
        "    \n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "    \n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = lexicon[key]\n",
        "        res_vocab[key] = abs(value)\n",
        "      except KeyError:\n",
        "        pass\n",
        "    \n",
        "    return res_vocab\n",
        "\n",
        "\n",
        "  def _build_senti_vocab(self, vocab):\n",
        "    \"\"\"\n",
        "    builds a vocab using senti-wordnet\n",
        "    \"\"\"\n",
        "    senti_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      senti_vocab[key] = 0\n",
        "\n",
        "    max_value = 0\n",
        "    for key in senti_vocab.keys():\n",
        "      senses = list(swn.senti_synsets(key))\n",
        "      pos = 0\n",
        "      neg = 0\n",
        "      for sense in senses:\n",
        "        if sense.synset.name().split(\".\")[0] == key:\n",
        "          pos += sense.pos_score()\n",
        "          neg += sense.neg_score()\n",
        "      if (pos != 0) or (neg != 0):\n",
        "        senti_vocab[key] = max(pos, neg)\n",
        "      if senti_vocab[key] > max_value:\n",
        "        max_value = senti_vocab[key]\n",
        "\n",
        "    # for key in senti_vocab.keys():\n",
        "      # senti_vocab[key] = self.maprange((0, max_value), (0, 1), senti_vocab[key])\n",
        "\n",
        "    return senti_vocab\n",
        "  \n",
        "  def build_sentic_net_vocab(self, vocab, path):\n",
        "\n",
        "    df = pd.read_csv(path, sep=\"\\t+\")\n",
        "\n",
        "    df.replace([\"negative\", \"positive\"], 1, inplace = True)\n",
        "    # df.set_index([\"CONCEPT\"], inplace = True)\n",
        "\n",
        "    df = dict(zip(df.CONCEPT, df.POLARITY))\n",
        "\n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "\n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = df[key]\n",
        "        res_vocab[key] = value\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    return res_vocab\n",
        "  \n",
        "  def _build_social_sent_vocab(self, vocab, path):\n",
        "    word_path = f\"{path}/frequent_words/\"\n",
        "    adj_path = f\"{path}/adjectives\"\n",
        "    word_files = [os.path.join(word_path, filename) for filename in os.listdir(word_path) if \".tsv\" in filename]\n",
        "    adj_files = [os.path.join(adj_path, filename) for filename in os.listdir(adj_path) if \".tsv\" in filename]\n",
        "\n",
        "    word_dfs = [pd.read_csv(f, sep = \"\\t\", names = [\"word\", \"mean\", \"std\"]) for f in word_files]\n",
        "    adj_dfs = [pd.read_csv(f, sep = \"\\t\", names = [\"word\", \"mean\", \"std\"]) for f in adj_files]\n",
        "\n",
        "    #words = pd.concat(word_dfs)\n",
        "    #adjs = pd.concat(adj_dfs)\n",
        "    words = pd.read_csv(f\"{word_path}/2000.tsv\", sep = \"\\t\", names = [\"word\", \"mean\", \"std\"])\n",
        "    adjs = pd.read_csv(f\"{adj_path}/2000.tsv\", sep = \"\\t\", names = [\"word\", \"mean\", \"std\"])\n",
        "    tot = pd.concat([words, adjs])\n",
        "\n",
        "    tot = tot.drop(\"std\", axis = 1)\n",
        "    tot[\"mean\"] = tot[\"mean\"].abs()\n",
        "    tot.sort_values(by=[\"mean\"], inplace = True)\n",
        "    tot.drop_duplicates(subset = \"word\", keep=\"last\", inplace = True)\n",
        "\n",
        "    tot = dict(zip(tot[\"word\"], tot[\"mean\"]))\n",
        "\n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "\n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = tot[key]\n",
        "        res_vocab[key] = value\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    return res_vocab\n",
        "  \n",
        "  def maprange(self, a, b, s):\n",
        "    \"\"\"\n",
        "    Maps the number s from range a = [a1, a2] to range b = [b1, b2]\n",
        "    \"\"\"\n",
        "    # Source: https://rosettacode.org/wiki/Map_range#Python\n",
        "    (a1, a2), (b1, b2) = a, b\n",
        "    return  b1 + ((s - a1) * (b2 - b1) / (a2 - a1))\n",
        "  \n",
        "  def _compute_average_sentiment_degree(self, *args):\n",
        "    \"\"\"\n",
        "    Assumption: all arguments in args are dictionaries containing the same keys\n",
        "    and a numbers as value.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict\n",
        "      average of the sentiment degree across dictionaries for each word\n",
        "    \n",
        "    Example\n",
        "    -------\n",
        "    we have two dictionaries that give a sentiment degree to words:\n",
        "    a = {\"good\": 0.9, \"bad\": 0.7}\n",
        "    b = {\"good\": 0.5, \"bad\": 0.1}\n",
        "\n",
        "    result = {\"good\": 0.7, \"bad\": 0.4}\n",
        "    \"\"\"\n",
        "    # Da correggere!!!! non devo dividere per n_args ma per il numero di vocabolari che trovano una specifica parola\n",
        "    n_args = len(args)\n",
        "    res = {}\n",
        "    for arg in args:\n",
        "      for key in arg.keys():\n",
        "        try:\n",
        "          res[key].append(arg[key])\n",
        "        except KeyError:\n",
        "          res[key] = []\n",
        "    for key in res.keys():\n",
        "      if len(res[key]) != 0:\n",
        "        res[key] = sum(res[key]) / len(res[key])\n",
        "      else:\n",
        "        res[key] = 0\n",
        "    \n",
        "    return res\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = self.data[index]\n",
        "    label = self.targets[index]\n",
        "    gold_word = self.wl_gold_av[index]\n",
        "    gold_sent = self.sl_gold_av[index]\n",
        "    return (item, label, gold_word, gold_sent)\n",
        "\n",
        "#corpus = MovieReviewsCorpusLBSA()\n",
        "#ds = MovieReviewsDatasetLBSA(corpus)\n",
        "#print(type(ds.sl_gold_av[1]))"
      ],
      "metadata": {
        "id": "8lGBLNBxo4Jv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand:\n",
        "- If it is better to introduce intermediate supervision\n",
        "\n",
        "- If it is better to use one hot encoding for the output\n",
        "\n",
        "- If I intepreted well the word-loss"
      ],
      "metadata": {
        "id": "IT_UVsevkmNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLBSA(BiLSTMAttention):\n",
        "    # Lexicon Based Supervised Attention model (LBSA) inspired by the following paper: https://aclanthology.org/C18-1074.pdf\n",
        "    def __init__(self, embedding_matrix, device=\"cuda\", input_size=300,\n",
        "                 hidden_size=128, context_size = 20, output_size=2):\n",
        "\n",
        "        super(EncoderLBSA, self).__init__(embedding_matrix, device, input_size, hidden_size, context_size, output_size)\n",
        "        self.fc = nn.Sequential(\n",
        "                                nn.Linear(hidden_size*2, hidden_size)\n",
        "                                )\n",
        "\n",
        "    # TODO: Pass the part inside for to super.forward()  \n",
        "    def forward(self, x):\n",
        "      att = []\n",
        "      res = []\n",
        "      for i, doc in enumerate(x):\n",
        "        doc = doc.to(self.device)\n",
        "        batch_size = doc.batch_sizes[0].item()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        doc = self.simple_elementwise_apply(self.embedding, doc)\n",
        "\n",
        "        out, _ = self.lstm(doc, hidden)\n",
        "        out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "        # n_sents, n_words_per_sent, hidden_size * 2 (since is bilstm)\n",
        "\n",
        "\n",
        "        attention_values = torch.tanh(self.attention(out))\n",
        "        # n_sents, n_words_per_sent, context_size\n",
        "\n",
        "        attention_weights = torch.softmax(attention_values.matmul(self.history), dim = 1).unsqueeze(1)\n",
        "        # n_sents, n_words_per_sent\n",
        "\n",
        "        out = torch.sum(attention_weights.matmul(out), dim = 1)\n",
        "        # n_sents, hidden*2\n",
        "\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        attention_weights = attention_weights.squeeze(dim=1)\n",
        "\n",
        "        att.append([sent[:input_sizes[idx]] for idx, sent in enumerate(attention_weights)])\n",
        "\n",
        "        res.append(out)\n",
        "      # n_doc, seq_lengths, hidden * 2\n",
        "      return res, att"
      ],
      "metadata": {
        "id": "k_dniRPY5q2A"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sortLBSA(X, w_gold, s_gold):\n",
        "\n",
        "  sentence_lengths = [np.array([sent.size(dim=0) for sent in doc]) for doc in X]\n",
        "  indexes = [np.argsort(doc) for doc in sentence_lengths]\n",
        "  indexes = [el.tolist() for el in indexes]\n",
        "\n",
        "  X_sorted = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(X)]\n",
        "  # w_gold = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(w_gold)]\n",
        "  # s_gold = [torch.tensor([doc[idx2] for idx2 in indexes[idx]][::-1]) for idx, doc in enumerate(s_gold)]\n",
        "  sentence_lengths = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(sentence_lengths)]\n",
        "\n",
        "  return X_sorted, w_gold, s_gold, sentence_lengths, indexes\n",
        "\n",
        "def padLBSA(batch, max_sizes):\n",
        "    pad = torch.tensor([-1])\n",
        "    for idx1, doc in enumerate(batch):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        remaining = max_sizes[idx1] - sent.size(dim = 0)\n",
        "        batch[idx1][idx2] = torch.cat((sent, pad.repeat(remaining)), dim = 0)\n",
        "    return batch\n",
        "\n",
        "def to_tensorLBSA(batch, max_sizes):\n",
        "  res = []\n",
        "  for idx, doc in enumerate(batch):\n",
        "    buff = torch.zeros(len(doc), max_sizes[idx], dtype=torch.int32)\n",
        "    for idx2, sent in enumerate(doc):\n",
        "      buff[idx2] = sent\n",
        "\n",
        "    res.append(buff)\n",
        "  return res\n",
        "\n",
        "def collateLBSA(batch):\n",
        "  X, Y, w_gold, s_gold = list(zip(*batch))\n",
        "\n",
        "  X, w_gold, s_gold, sentence_lengths, indexes = sortLBSA(X, w_gold, s_gold)\n",
        "  # can take doc[0] since senetence_lengths is sorted\n",
        "  max_sizes = [doc[0] for doc in sentence_lengths]\n",
        "\n",
        "  # Pad tensor each element\n",
        "  X = padLBSA(X, max_sizes)\n",
        "  # Transform the batch to a tensor\n",
        "  X = to_tensorLBSA(X, max_sizes)\n",
        "\n",
        "  # Return the padded sequence object\n",
        "  X = [pack_padded_sequence(doc, sentence_lengths[idx], batch_first=True) for idx, doc in enumerate(X)]\n",
        "  return X, Y, w_gold, s_gold, indexes"
      ],
      "metadata": {
        "id": "JyjRmkGAIu94"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def element_wise_log_loss(out, labels):\n",
        "  res = - out.log().mul(labels).sum(dim=0)\n",
        "  return res\n",
        "\n",
        "def loss_LBSA(outputs, targets, mu_w = 0.0005, mu_s = 0.025):\n",
        "  dec_output, w_att, s_att = outputs\n",
        "  target, w_gold, s_gold = targets\n",
        "\n",
        "  total_loss = 0\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "\n",
        "  cross_loss = ce(dec_output, target)\n",
        "  total_loss += cross_loss\n",
        "\n",
        "  # pensare a fare uno scaling della loss se è molto diversa dalla cross entropy\n",
        "\n",
        "  w_loss = torch.mean(torch.tensor([\n",
        "    torch.sum(torch.tensor([\n",
        "        element_wise_log_loss(w_att[idx1][idx2], sent) for idx2, sent in enumerate(doc)\n",
        "    ])) * mu_w for idx1, doc in enumerate(w_gold)\n",
        "  ]))\n",
        "  total_loss += w_loss\n",
        "\n",
        "  s_loss = torch.mean(torch.tensor([\n",
        "      element_wise_log_loss(s_att[idx], doc) * mu_s for idx, doc in enumerate(s_gold)\n",
        "  ]))\n",
        "  total_loss += s_loss\n",
        "\n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "VXw9lvmaWM3B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_LBSA(encoder, decoder, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "\n",
        "  for batch_idx, (inputs, target, w_gold, s_gold, _) in enumerate(data_loader):\n",
        "    \n",
        "    in_size = len(target)\n",
        "\n",
        "    enc_output, w_att = encoder(inputs)\n",
        "    \n",
        "    batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "    dec_input, target, indexes = collate(batch)\n",
        "    \n",
        "    s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "\n",
        "    target = target.to(device)\n",
        "    for idx1, doc in enumerate(w_gold):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "    for idx, doc in enumerate(s_gold):\n",
        "       s_gold[idx] = doc.to(device)\n",
        "    \n",
        "\n",
        "    dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "    outputs = (dec_output, w_att, s_att)\n",
        "    targets = (target, w_gold, s_gold)\n",
        "\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "yZF4uo99VKsh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_LBSA(encoder, decoder, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, target, w_gold, s_gold, _) in enumerate(data_loader):\n",
        "      in_size = len(target)\n",
        "\n",
        "      enc_output, w_att = encoder(inputs)\n",
        "      \n",
        "      batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "      dec_input, target, indexes = collate(batch)\n",
        "\n",
        "      s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "      # Not sorting also w_gold becuse in the encoder, documents don't get shuffled\n",
        "\n",
        "      target = target.to(device)\n",
        "      for idx1, doc in enumerate(w_gold):\n",
        "        for idx2, sent in enumerate(doc):\n",
        "          w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "      for idx, doc in enumerate(s_gold):\n",
        "        s_gold[idx] = doc.to(device)\n",
        "\n",
        "      dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "      outputs = (dec_output, w_att, s_att)\n",
        "      targets = (target, w_gold, s_gold)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "      \n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "RpyI0qRvC0cF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting must be a problem, otherwise word_level attention doesn't get the correct supervision.\n",
        "Same goes for sentences"
      ],
      "metadata": {
        "id": "ntMCKJDtuEnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_LBSA_new(encoder, decoder, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "\n",
        "  for batch_idx, (inputs, target, w_gold, s_gold, sent_indexes) in enumerate(data_loader):\n",
        "    \n",
        "    in_size = len(target)\n",
        "\n",
        "    enc_output, w_att = encoder(inputs)\n",
        "    # Sorting the sentences of the encoder to their original position\n",
        "    # Inverting the output because indexes are in ascending order, output is in descending\n",
        "    enc_output = [torch.flip(enc_output[doc_idx], dims = [0]) for doc_idx, _ in enumerate(enc_output)]\n",
        "    w_att = [w_att[doc_idx][::-1] for doc_idx, _ in enumerate(w_att)]\n",
        "    # Using argsort on the indexes reverses the previous argsort\n",
        "    inverted_indexes = [np.argsort(np.array(doc)) for doc in sent_indexes]\n",
        "    inverted_indexes = [el.tolist() for el in inverted_indexes]\n",
        "    # Sort the sentences with original sorting:\n",
        "    # n_doc, n_sents, hidden*2\n",
        "\n",
        "    enc_output = [enc_output[doc_idx][sent_idx] for doc_idx, sent_idx in enumerate(inverted_indexes)]\n",
        "    w_att = [[doc[idx2] for idx2 in inverted_indexes[idx]] for idx, doc in enumerate(w_att)]\n",
        "\n",
        "\n",
        "    # for i, doc in enumerate(enc_output):\n",
        "        # for j, sent in enumerate(doc):\n",
        "          # enc_output[i][j] = sent.cpu()\n",
        "    \n",
        "    batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "    dec_input, target, indexes = collate(batch)\n",
        "    \n",
        "    # s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "    target = target.to(device)\n",
        "    for idx1, doc in enumerate(w_gold):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "    for idx, doc in enumerate(s_gold):\n",
        "       s_gold[idx] = doc.to(device)\n",
        "    \n",
        "\n",
        "    dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "    dec_output = torch.flip(dec_output, dims = [0])\n",
        "    s_att = s_att[::-1]\n",
        "    target = torch.flip(target, dims = [0])\n",
        "\n",
        "\n",
        "    inverted_indexes = np.argsort(np.array(indexes))\n",
        "    inverted_indexes = inverted_indexes.tolist()\n",
        "    # Sort the sentences with original sorting:\n",
        "    # n_doc, n_sents, hidden*2\n",
        "    dec_output = dec_output[inverted_indexes]\n",
        "    s_att = [s_att[idx] for idx in inverted_indexes]\n",
        "    target = target[inverted_indexes]\n",
        "\n",
        "\n",
        "    outputs = (dec_output, w_att, s_att)\n",
        "    targets = (target, w_gold, s_gold)\n",
        "    \n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "RN2AM7tLgIDm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_LBSA_new(encoder, decoder, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, target, w_gold, s_gold, sent_indexes) in enumerate(data_loader):\n",
        "      in_size = len(target)\n",
        "\n",
        "      enc_output, w_att = encoder(inputs)\n",
        "\n",
        "      # First flip (go to ascending order)\n",
        "      enc_output = [torch.flip(enc_output[doc_idx], dims = [0]) for doc_idx, _ in enumerate(enc_output)]\n",
        "      w_att = [w_att[doc_idx][::-1] for doc_idx, _ in enumerate(w_att)]\n",
        "\n",
        "      # Second take indexes for getting the original positions\n",
        "      # Using argsort on the indexes reverses the previous argsort\n",
        "      inverted_indexes = [np.argsort(np.array(doc)) for doc in sent_indexes]\n",
        "      inverted_indexes = [el.tolist() for el in inverted_indexes]\n",
        "\n",
        "      # Third Sort the sentences with original sorting:\n",
        "      # n_doc, n_sents, hidden*2\n",
        "      enc_output = [enc_output[doc_idx][sent_idx] for doc_idx, sent_idx in enumerate(inverted_indexes)]\n",
        "      w_att = [[doc[idx2] for idx2 in inverted_indexes[idx]] for idx, doc in enumerate(w_att)]\n",
        "      \n",
        "      batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "      dec_input, target, indexes = collate(batch)\n",
        "\n",
        "      # s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "      # Not sorting also w_gold becuse in the encoder, documents don't get shuffled inside\n",
        "\n",
        "      target = target.to(device)\n",
        "      for idx1, doc in enumerate(w_gold):\n",
        "        for idx2, sent in enumerate(doc):\n",
        "          w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "      for idx, doc in enumerate(s_gold):\n",
        "        s_gold[idx] = doc.to(device)\n",
        "\n",
        "      dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "\n",
        "      dec_output = torch.flip(dec_output, dims = [0])\n",
        "      s_att = s_att[::-1]\n",
        "      target = torch.flip(target, dims = [0])\n",
        "\n",
        "      inverted_indexes = np.argsort(np.array(indexes))\n",
        "      inverted_indexes = inverted_indexes.tolist()\n",
        "      # Sort the sentences with original sorting:\n",
        "      # n_doc, n_sents, hidden*2\n",
        "      dec_output = dec_output[inverted_indexes]\n",
        "      s_att = [s_att[idx] for idx in inverted_indexes]\n",
        "      target = target[inverted_indexes]\n",
        "\n",
        "      outputs = (dec_output, w_att, s_att)\n",
        "      targets = (target, w_gold, s_gold)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "      \n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "kEf6kIOghmBm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def main_LBSA(train_loader, test_loader, embedding_matrix, device = \"cuda\", epochs = 10):\n",
        "\n",
        "  encoder = EncoderLBSA(embedding_matrix = embedding_matrix, device = device, input_size=300, hidden_size=100).to(device)\n",
        "  decoder = BiLSTMAttention(device = device, input_size = 100, context_size = 20).to(device)\n",
        "\n",
        "  optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), 0.001, betas = (0.9, 0.999), amsgrad=True)\n",
        "  scheduler = ExponentialLR(optimizer, 0.8)\n",
        "\n",
        "  cost_function = loss_LBSA\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step_LBSA_new(encoder, decoder, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    test_loss, test_accuracy = test_step_LBSA_new(encoder, decoder, test_loader, cost_function, device)\n",
        "    print(f\"Test loss: {test_loss} \\n Test accuracy: {test_accuracy}\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    scheduler.step()\n",
        "\n",
        "  return test_accuracy\n"
      ],
      "metadata": {
        "id": "3Xzpaa7IDN9Z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import FastText\n",
        "global_vectors = GloVe(name='840B', dim=300, cache = \"/content/gdrive/My Drive/nlu-project/.vector_cache\")\n",
        "# fast_text = FastText('en')"
      ],
      "metadata": {
        "id": "hgNE3lxXrW4M"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(UnderscoreRemover(),\n",
        "                    CharacterRepetitionRemover(),\n",
        "                    ApostrophesMerger(),\n",
        "                    ContractionCleaner(),\n",
        "                    SpecialCharsCleaner(),\n",
        "                    )\n",
        "corpus = MovieReviewsCorpusLBSA(pipeline)"
      ],
      "metadata": {
        "id": "Xu7rZw-mrqke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = corpus.get_embedding_matrix(global_vectors, 300)\n",
        "# ds = corpus.get_indexed_corpus()"
      ],
      "metadata": {
        "id": "0TtoFzYDsGHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MovieReviewsDatasetLBSA(corpus)\n",
        "# train_loader, test_loader = get_data(128, dataset, collate_fn=collateLBSA)"
      ],
      "metadata": {
        "id": "QQf9eKAqsIfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64235c66-918a-4f0e-b884-29656a814667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDrwHvOIEsl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fa85a7-f72a-49c2-9d28-0122157eb345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold: 0\n",
            "epoch 0:\n",
            "Training loss: 0.008031133512655894 \n",
            " Training accuracy: 61.72222222222222\n",
            "Test loss: 0.00965051531791687 \n",
            " Test accuracy: 50.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.006437118318345812 \n",
            " Training accuracy: 77.66666666666666\n",
            "Test loss: 0.00931180089712143 \n",
            " Test accuracy: 81.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.005180157721042633 \n",
            " Training accuracy: 87.0\n",
            "Test loss: 0.008433159589767456 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.003672986626625061 \n",
            " Training accuracy: 92.22222222222223\n",
            "Test loss: 0.00724354237318039 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.003197269406583574 \n",
            " Training accuracy: 94.83333333333334\n",
            "Test loss: 0.006924657821655273 \n",
            " Test accuracy: 77.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0028365622295273675 \n",
            " Training accuracy: 97.11111111111111\n",
            "Test loss: 0.005526358187198639 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0028466419213347967 \n",
            " Training accuracy: 98.66666666666667\n",
            "Test loss: 0.005413910448551178 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.002484510276052687 \n",
            " Training accuracy: 98.77777777777777\n",
            "Test loss: 0.008072973489761352 \n",
            " Test accuracy: 80.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.0024069909585846794 \n",
            " Training accuracy: 99.3888888888889\n",
            "Test loss: 0.005674081444740295 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.0023333178295029534 \n",
            " Training accuracy: 99.77777777777777\n",
            "Test loss: 0.005862780809402466 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.002405518607960807 \n",
            " Training accuracy: 99.55555555555556\n",
            "Test loss: 0.005810036659240723 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.002373190058602227 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.005627171099185944 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 1\n",
            "epoch 0:\n",
            "Training loss: 0.008099377552668253 \n",
            " Training accuracy: 59.77777777777777\n",
            "Test loss: 0.009664748311042786 \n",
            " Test accuracy: 65.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.006674894822968377 \n",
            " Training accuracy: 73.77777777777777\n",
            "Test loss: 0.009144609570503235 \n",
            " Test accuracy: 79.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.004928470055262248 \n",
            " Training accuracy: 83.22222222222221\n",
            "Test loss: 0.008289664685726165 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.003881850341955821 \n",
            " Training accuracy: 90.88888888888889\n",
            "Test loss: 0.006481090188026428 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.003135450134674708 \n",
            " Training accuracy: 95.0\n",
            "Test loss: 0.005280214101076126 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.002732145289580027 \n",
            " Training accuracy: 97.11111111111111\n",
            "Test loss: 0.005097413659095764 \n",
            " Test accuracy: 90.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.002597862548298306 \n",
            " Training accuracy: 98.83333333333333\n",
            "Test loss: 0.00552220344543457 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.002421884983778 \n",
            " Training accuracy: 98.66666666666667\n",
            "Test loss: 0.005623431801795959 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.002367226266198688 \n",
            " Training accuracy: 99.33333333333333\n",
            "Test loss: 0.00585359275341034 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.002260117381811142 \n",
            " Training accuracy: 99.55555555555556\n",
            "Test loss: 0.005667220503091812 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.0034746960798899334 \n",
            " Training accuracy: 99.55555555555556\n",
            "Test loss: 0.007274166345596313 \n",
            " Test accuracy: 83.5\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.0022909004820717708 \n",
            " Training accuracy: 99.6111111111111\n",
            "Test loss: 0.006948060691356659 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 2\n",
            "epoch 0:\n",
            "Training loss: 0.00756491697496838 \n",
            " Training accuracy: 61.611111111111114\n",
            "Test loss: 0.009260676503181457 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.005404218782981237 \n",
            " Training accuracy: 81.5\n",
            "Test loss: 0.008935120999813079 \n",
            " Test accuracy: 51.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.00451152879330847 \n",
            " Training accuracy: 88.72222222222223\n",
            "Test loss: 0.008015919923782349 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.0036527291105853186 \n",
            " Training accuracy: 93.88888888888889\n",
            "Test loss: 0.006990261077880859 \n",
            " Test accuracy: 83.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0031258069144354927 \n",
            " Training accuracy: 96.61111111111111\n",
            "Test loss: 0.005720543265342712 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0029423434866799247 \n",
            " Training accuracy: 97.72222222222223\n",
            "Test loss: 0.005130841434001923 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0025241234070724912 \n",
            " Training accuracy: 98.66666666666667\n",
            "Test loss: 0.005251829028129578 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.00237124502658844 \n",
            " Training accuracy: 99.0\n",
            "Test loss: 0.005382995903491974 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.002305089251862632 \n",
            " Training accuracy: 99.33333333333333\n",
            "Test loss: 0.005288335084915161 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.002262120693922043 \n",
            " Training accuracy: 99.66666666666667\n",
            "Test loss: 0.005489327609539032 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.0022310433867904875 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.0052114158868789675 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.0023610502647029028 \n",
            " Training accuracy: 99.77777777777777\n",
            "Test loss: 0.00550024539232254 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 3\n",
            "epoch 0:\n",
            "Training loss: 0.008165444466802809 \n",
            " Training accuracy: 59.5\n",
            "Test loss: 0.00986851692199707 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.006494000951449076 \n",
            " Training accuracy: 75.55555555555556\n",
            "Test loss: 0.009195298850536347 \n",
            " Test accuracy: 57.99999999999999\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.004835662643114726 \n",
            " Training accuracy: 85.5\n",
            "Test loss: 0.008277200162410736 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.003775915602842967 \n",
            " Training accuracy: 92.33333333333333\n",
            "Test loss: 0.006417834162712097 \n",
            " Test accuracy: 93.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0034454747868908776 \n",
            " Training accuracy: 94.88888888888889\n",
            "Test loss: 0.005722534358501435 \n",
            " Test accuracy: 82.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0033552513354354434 \n",
            " Training accuracy: 97.33333333333334\n",
            "Test loss: 0.00451472818851471 \n",
            " Test accuracy: 90.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.002845229423708386 \n",
            " Training accuracy: 97.77777777777777\n",
            "Test loss: 0.006231260597705841 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.0025736345019605426 \n",
            " Training accuracy: 98.72222222222223\n",
            "Test loss: 0.0047819983959197995 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.0024485336740811665 \n",
            " Training accuracy: 98.88888888888889\n",
            "Test loss: 0.0045817729830741885 \n",
            " Test accuracy: 91.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.0023515488704045615 \n",
            " Training accuracy: 99.33333333333333\n",
            "Test loss: 0.004873007237911224 \n",
            " Test accuracy: 90.0\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.002343693673610687 \n",
            " Training accuracy: 99.6111111111111\n",
            "Test loss: 0.004882984459400177 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.0022844540907277 \n",
            " Training accuracy: 99.6111111111111\n",
            "Test loss: 0.005276954472064972 \n",
            " Test accuracy: 90.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 4\n",
            "epoch 0:\n",
            "Training loss: 0.008262833555539448 \n",
            " Training accuracy: 57.611111111111114\n",
            "Test loss: 0.009765831232070922 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.006749696864022149 \n",
            " Training accuracy: 71.83333333333334\n",
            "Test loss: 0.009230851829051971 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.005405141694678201 \n",
            " Training accuracy: 80.27777777777779\n",
            "Test loss: 0.008459931015968323 \n",
            " Test accuracy: 79.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.004327405989170074 \n",
            " Training accuracy: 88.0\n",
            "Test loss: 0.007285939753055573 \n",
            " Test accuracy: 77.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0034067281749513415 \n",
            " Training accuracy: 93.61111111111111\n",
            "Test loss: 0.0057723307609558105 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0029715221789148117 \n",
            " Training accuracy: 96.05555555555556\n",
            "Test loss: 0.005588882565498352 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.002819346437851588 \n",
            " Training accuracy: 97.72222222222223\n",
            "Test loss: 0.005499318689107895 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.0024599408937825097 \n",
            " Training accuracy: 98.72222222222223\n",
            "Test loss: 0.0056538921594619755 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.002333205971452925 \n",
            " Training accuracy: 99.44444444444444\n",
            "Test loss: 0.0054043009877204895 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.0027786331872145333 \n",
            " Training accuracy: 99.66666666666667\n",
            "Test loss: 0.005948487520217895 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.0023481502797868515 \n",
            " Training accuracy: 99.66666666666667\n",
            "Test loss: 0.006545773446559906 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.002307730946275923 \n",
            " Training accuracy: 99.6111111111111\n",
            "Test loss: 0.006614122092723846 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 5\n",
            "epoch 0:\n",
            "Training loss: 0.008073730369408925 \n",
            " Training accuracy: 57.99999999999999\n",
            "Test loss: 0.009678606688976289 \n",
            " Test accuracy: 57.49999999999999\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.0064549153049786884 \n",
            " Training accuracy: 73.55555555555556\n",
            "Test loss: 0.009132999777793884 \n",
            " Test accuracy: 53.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.0046708176698949605 \n",
            " Training accuracy: 85.27777777777777\n",
            "Test loss: 0.00808920532464981 \n",
            " Test accuracy: 77.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.0035288367668787637 \n",
            " Training accuracy: 92.22222222222223\n",
            "Test loss: 0.006194689869880676 \n",
            " Test accuracy: 90.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.003465342124303182 \n",
            " Training accuracy: 95.16666666666667\n",
            "Test loss: 0.005508407354354859 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0027223817507425942 \n",
            " Training accuracy: 97.5\n",
            "Test loss: 0.0053073447942733765 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0026194991171360014 \n",
            " Training accuracy: 98.66666666666667\n",
            "Test loss: 0.00556734561920166 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.0024376318355401358 \n",
            " Training accuracy: 98.72222222222223\n",
            "Test loss: 0.007464842200279236 \n",
            " Test accuracy: 83.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.0023998321923944686 \n",
            " Training accuracy: 99.6111111111111\n",
            "Test loss: 0.005300875753164291 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.002279478675789303 \n",
            " Training accuracy: 99.77777777777777\n",
            "Test loss: 0.005088264495134354 \n",
            " Test accuracy: 91.0\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.002206637528207567 \n",
            " Training accuracy: 99.77777777777777\n",
            "Test loss: 0.0054034945368766785 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.0022114787333541446 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.005304576456546783 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 6\n",
            "epoch 0:\n",
            "Training loss: 0.007496133910285102 \n",
            " Training accuracy: 63.0\n",
            "Test loss: 0.009420571625232696 \n",
            " Test accuracy: 57.49999999999999\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.005408557984564039 \n",
            " Training accuracy: 80.5\n",
            "Test loss: 0.008968406319618226 \n",
            " Test accuracy: 65.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.004396214650736915 \n",
            " Training accuracy: 90.33333333333333\n",
            "Test loss: 0.00829704374074936 \n",
            " Test accuracy: 73.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.0034513997534910837 \n",
            " Training accuracy: 93.33333333333333\n",
            "Test loss: 0.008533415794372558 \n",
            " Test accuracy: 58.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.002897877792517344 \n",
            " Training accuracy: 96.11111111111111\n",
            "Test loss: 0.006474797129631043 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.002533544467555152 \n",
            " Training accuracy: 98.38888888888889\n",
            "Test loss: 0.005789480209350586 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.002371046841144562 \n",
            " Training accuracy: 99.16666666666667\n",
            "Test loss: 0.005287468433380127 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.00228731835881869 \n",
            " Training accuracy: 99.3888888888889\n",
            "Test loss: 0.007914947271347046 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.002192611942688624 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.006561068892478943 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.0021325628542237813 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.006125364005565643 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.002478773080640369 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.005617989599704743 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.002133064965407054 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.005642427504062653 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 7\n",
            "epoch 0:\n",
            "Training loss: 0.007893479069073995 \n",
            " Training accuracy: 60.611111111111114\n",
            "Test loss: 0.009502162039279938 \n",
            " Test accuracy: 50.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.005865033219257991 \n",
            " Training accuracy: 76.16666666666667\n",
            "Test loss: 0.008959717452526092 \n",
            " Test accuracy: 51.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.004444919609361225 \n",
            " Training accuracy: 87.3888888888889\n",
            "Test loss: 0.0081925830245018 \n",
            " Test accuracy: 64.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.003639043950372272 \n",
            " Training accuracy: 92.55555555555556\n",
            "Test loss: 0.007698934674263001 \n",
            " Test accuracy: 64.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.003121648778518041 \n",
            " Training accuracy: 95.94444444444444\n",
            "Test loss: 0.007589755058288575 \n",
            " Test accuracy: 74.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0027159801953368715 \n",
            " Training accuracy: 97.55555555555556\n",
            "Test loss: 0.006345646381378174 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.002520140293571684 \n",
            " Training accuracy: 98.33333333333333\n",
            "Test loss: 0.005545489639043808 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.00271083179447386 \n",
            " Training accuracy: 98.94444444444444\n",
            "Test loss: 0.006019132435321808 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.0023673611051506466 \n",
            " Training accuracy: 99.0\n",
            "Test loss: 0.006136628687381744 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.002284433775477939 \n",
            " Training accuracy: 99.5\n",
            "Test loss: 0.0063860684633255 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.0022811342941390146 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.006644128561019897 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.0022291319072246553 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.006035919785499572 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 8\n",
            "epoch 0:\n",
            "Training loss: 0.00787594738933775 \n",
            " Training accuracy: 61.0\n",
            "Test loss: 0.009549623727798462 \n",
            " Test accuracy: 50.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.005864972240395016 \n",
            " Training accuracy: 76.94444444444444\n",
            "Test loss: 0.008886260092258453 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.004314416282706791 \n",
            " Training accuracy: 87.5\n",
            "Test loss: 0.007824712693691253 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.003420184701681137 \n",
            " Training accuracy: 92.88888888888889\n",
            "Test loss: 0.006678033471107483 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0030345172186692555 \n",
            " Training accuracy: 96.33333333333334\n",
            "Test loss: 0.005969246625900268 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.002594449023405711 \n",
            " Training accuracy: 98.16666666666667\n",
            "Test loss: 0.00489877849817276 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0024071211781766678 \n",
            " Training accuracy: 98.88888888888889\n",
            "Test loss: 0.005323926210403443 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.002249170790115992 \n",
            " Training accuracy: 99.44444444444444\n",
            "Test loss: 0.0054037638008594515 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.0025091783122883904 \n",
            " Training accuracy: 99.6111111111111\n",
            "Test loss: 0.006687251925468445 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.0023753968046771154 \n",
            " Training accuracy: 99.11111111111111\n",
            "Test loss: 0.006749338507652283 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.0022232886652151745 \n",
            " Training accuracy: 99.77777777777777\n",
            "Test loss: 0.0057806396484375 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.0021868993590275446 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.005366512835025788 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 9\n",
            "epoch 0:\n",
            "Training loss: 0.0074948574105898535 \n",
            " Training accuracy: 62.94444444444445\n",
            "Test loss: 0.00941756933927536 \n",
            " Test accuracy: 68.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.005935478210449219 \n",
            " Training accuracy: 77.66666666666666\n",
            "Test loss: 0.008969902396202087 \n",
            " Test accuracy: 59.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.004428708404302597 \n",
            " Training accuracy: 87.5\n",
            "Test loss: 0.008041600286960602 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.003520362459950977 \n",
            " Training accuracy: 93.22222222222221\n",
            "Test loss: 0.006647428572177887 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.003051158835490545 \n",
            " Training accuracy: 95.5\n",
            "Test loss: 0.005787381529808044 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0027067302332984076 \n",
            " Training accuracy: 97.27777777777777\n",
            "Test loss: 0.007037871181964874 \n",
            " Test accuracy: 81.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0025455666416221196 \n",
            " Training accuracy: 98.5\n",
            "Test loss: 0.006066677570343018 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.0023605525659190284 \n",
            " Training accuracy: 99.0\n",
            "Test loss: 0.00609352707862854 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.002257699684964286 \n",
            " Training accuracy: 99.44444444444444\n",
            "Test loss: 0.00557400107383728 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.002193982220358319 \n",
            " Training accuracy: 99.66666666666667\n",
            "Test loss: 0.0057304418087005616 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.002182806647486157 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.00580886572599411 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.0022008492383691998 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.005373736917972565 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "Folds statistics:\n",
            "----------------\n",
            " - mean: 87.05 \n",
            " - standard deviation: 2.079062288629179\n"
          ]
        }
      ],
      "source": [
        "# 87.3, 86.75 e qualcosa - 88.15 lbsa\n",
        "# Da fare una collate nuova direttamente che non vada a toccare i tensori esistenti.\n",
        "# Oppure ragionare un attimo.... servono davvero i gradient per l'input? O posso fare il detach?\n",
        "\n",
        "mean, std = main_cross_validation(main_LBSA, dataset, embedding_matrix, collateLBSA, epochs = 12)\n",
        "\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subjectivity Detection\n",
        "Now I'm going to implement a subjectivity detector, in order to find objective sentences. This task allows me to remove objective sentences from the subjectivity dataset, so I am left only (almost only, since the model will not be 100% accurate) with objective sentences.\n",
        "\n",
        "## Dataset exploration\n",
        "First I am going to see how the dataset is composed:"
      ],
      "metadata": {
        "id": "TaOnHvIo3X2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import subjectivity\n",
        "\n",
        "\n",
        "subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "obj = [sent for sent in subjectivity.sents(categories = 'obj')]"
      ],
      "metadata": {
        "id": "mFZY-nnW3RzT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I will print the first 10 sentences of each class (subjective and objective) in order to see how the dataset is arranged:"
      ],
      "metadata": {
        "id": "dejtwm5CYsb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mK4Rmfr4ziT"
      },
      "outputs": [],
      "source": [
        "subj[:10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obj[:10]"
      ],
      "metadata": {
        "id": "MMC78B6bYoLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be clearly seen that the daataset is composed of single phrases, instead of documents as opposed to the movie reviews dataset.\n",
        "Now I am going to compare the length of data from the two classes."
      ],
      "metadata": {
        "id": "grLJbs8ehlzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(obj))\n",
        "print(len(subj))"
      ],
      "metadata": {
        "id": "tknJRLrZhlUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's easy to see that the length of both objective and subjective datasets are the same length, so accuracy can be used as a metric since the dataset is balanced"
      ],
      "metadata": {
        "id": "0AxGbL1JiLrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SubjectivityCorpus():\n",
        "  def __init__(self, preprocess_pipeline = None):\n",
        "    # list of documents, each document is a list containing words of that document\n",
        "    self.pipeline = preprocess_pipeline\n",
        "    # Corpus as list of documents. Documents as list of sentences. Sentences as list of tokens\n",
        "    self.corpus, self.labels = self._get_corpus()\n",
        "    if preprocess_pipeline == None:\n",
        "        self.processed_corpus = self.corpus\n",
        "    else:\n",
        "        # Flattened and preprocessed corpus\n",
        "        self.processed_corpus = self._preprocess()\n",
        "\n",
        "    #for optimization purposes\n",
        "    self._vocab_index = 0\n",
        "\n",
        "\n",
        "    self.corpus_words = self.get_corpus_words()\n",
        "    self.vocab = self._create_vocab()\n",
        "\n",
        "\n",
        "\n",
        "  def _list_to_str(self, doc) -> str:\n",
        "      \"\"\"\n",
        "      Put all elements of the list into a single string, separating each element with a space.\n",
        "      \"\"\"\n",
        "      return \" \".join([w for sent in doc for w in sent])\n",
        "\n",
        "  def _preprocess(self):\n",
        "      return self.pipeline(self.corpus)\n",
        "\n",
        "  def _get_corpus(self):\n",
        "    subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "    obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
        "    labels = [1] * len(subj) + [0] * len(obj)\n",
        "    return subj + obj, labels\n",
        "\n",
        "  def subjectivity_dataset_raw(self):\n",
        "      \"\"\"\n",
        "      Returns the dataset containing:\n",
        "\n",
        "      - A list of all the sentences\n",
        "      - The corresponding label for each sentence\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      tuple(list, list)\n",
        "          The dataset: first element is the list of the sentence, the second element of the tuple is the associated label (positive or negative) for each sentence\n",
        "      \"\"\"\n",
        "\n",
        "      return self.corpus, self.labels\n",
        "\n",
        "\n",
        "  def get_corpus_words(self) -> list:\n",
        "      return [w for sent in self.processed_corpus for w in sent]\n",
        "  \n",
        "  def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      np.ndarray\n",
        "          A 2D which each row has the corresponding embedding from the vocabulary\n",
        "      \"\"\"\n",
        "      matrix_length = len(self.vocab)\n",
        "      embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "      # If I use torch.zeros directly it crashes (don't know why)\n",
        "      embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "      null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "      for key in self.vocab.keys():\n",
        "          if torch.equal(embedding[key], null_embedding):\n",
        "              embedding_matrix[self.vocab[key]] = torch.randn(embedding_dim)\n",
        "          else:\n",
        "              embedding_matrix[self.vocab[key]] = embedding[key]\n",
        "              \n",
        "      return embedding_matrix\n",
        "\n",
        "  \n",
        "  def get_fasttext_embedding_matrix(self, embedding, embedding_dim):\n",
        "      matrix_length = len(self.vocab)\n",
        "      embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "      # If I use torch.zeros directly it crashes (don't know why)\n",
        "      embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "      null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "      for key in self.vocab.keys():\n",
        "          tensor_embedding = torch.from_numpy(embedding[key].copy())\n",
        "          if torch.equal(tensor_embedding, null_embedding):\n",
        "              embedding_matrix[self.vocab[key]] = torch.randn(embedding_dim)\n",
        "          else:\n",
        "              embedding_matrix[self.vocab[key]] = tensor_embedding\n",
        "              \n",
        "      return embedding_matrix\n",
        "  \n",
        "  def get_indexed_corpus(self):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      Dictionary\n",
        "          Containing correspondences word -> index\n",
        "      \n",
        "      list(list(torch.tensor))\n",
        "          The corpus represented as indexes corresponding to each word\n",
        "      \"\"\"\n",
        "      vocab = {}\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "          vocab[key] = idx\n",
        "      \n",
        "      indexed_corpus = [torch.tensor([vocab[word] for word in sent]) for sent in self.processed_corpus]\n",
        "      return indexed_corpus, self.labels\n",
        "\n",
        "  def embed_vocab(self, vocab):\n",
        "    for word in vocab.keys():\n",
        "      try:\n",
        "          self.vocab[word]\n",
        "      except:\n",
        "          self.vocab[word] = self._vocab_index\n",
        "          self._vocab_index += 1\n",
        "\n",
        "  def _create_vocab(self):\n",
        "      vocab = dict()\n",
        "      for word in self.corpus_words:\n",
        "        try:\n",
        "          vocab[word]\n",
        "        except:\n",
        "          vocab[word] = self._vocab_index\n",
        "          self._vocab_index += 1\n",
        "      return vocab\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.corpus)\n"
      ],
      "metadata": {
        "id": "GHTqA_vaA2Vm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class SubjectivityDataset(Dataset):\n",
        "  def __init__(self, raw_dataset):\n",
        "    super(SubjectivityDataset, self).__init__()\n",
        "    self.corpus = raw_dataset[0]\n",
        "    self.targets = raw_dataset[1]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    item = self.corpus[index]\n",
        "    label = self.targets[index]\n",
        "    return (item, label)"
      ],
      "metadata": {
        "id": "3NXv420XEUR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_sub = pipeline = Pipeline(UnderscoreRemoverFlat(),\n",
        "                                    CharacterRepetitionRemoverFlat(),\n",
        "                                    ApostrophesMergerFlat(),\n",
        "                                    ContractionCleanerFlat(),\n",
        "                                    SpecialCharsCleanerFlat(),\n",
        "                                    )\n",
        "corpus_subj = SubjectivityCorpus(pipeline)\n",
        "corpus_subj.embed_vocab(corpus.vocab)"
      ],
      "metadata": {
        "id": "B_18YMFwpe9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_subj = SubjectivityDataset(corpus_subj.get_indexed_corpus())\n",
        "train_loader_subj, test_loader_subj = get_data(128, dataset_subj, collate)"
      ],
      "metadata": {
        "id": "kTOap0UOpxhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_subj = corpus_subj.get_embedding_matrix(global_vectors, 300)"
      ],
      "metadata": {
        "id": "T7l4AVLtqsaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, net_obj= main(train_loader_subj, test_loader_subj, embedding_matrix_subj, device = \"cuda\", epochs = 10)\n",
        "\n",
        "# print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ],
      "metadata": {
        "id": "X1yZv0OksfuA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "dc20f90b-f535-4030-b549-03604f353de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0:\n",
            "Training loss: 0.0013972479817457497 \n",
            " Training accuracy: 81.75\n",
            "Test loss: 0.0011940113492310048 \n",
            " Test accuracy: 80.7\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.00021139853063505143 \n",
            " Training accuracy: 79.725\n",
            "Test loss: 0.0002531803525052965 \n",
            " Test accuracy: 81.2\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-279e90f32327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_obj\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix_subj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-65e4408686ce>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_loader, test_loader, embedding_matrix, device, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch {e}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-7956307cf77e>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(net, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try to use trained subjectivity in order to improve polarity"
      ],
      "metadata": {
        "id": "mUKX632yBajt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_testing(X, device = \"cuda\"):\n",
        "  if type(X) == list:\n",
        "    X = torch.tensor(X)\n",
        "  if len(X.size()) == 1:\n",
        "    X = torch.unsqueeze(X, 0)\n",
        "  \n",
        "  X_tensor = X.to(device)\n",
        "  X_final = pack_padded_sequence(X_tensor, torch.tensor([1]), batch_first=True)\n",
        "  return X_final\n"
      ],
      "metadata": {
        "id": "6bEcAGPHBy0o"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.sentiment.util import mark_negation\n",
        "\n",
        "class ObjectiveSentsRemover(PipelineElement):\n",
        "  def __init__(self, net = None, vocab = None):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    net\n",
        "      Already trained network in order to classify objective sentences\n",
        "    \"\"\"\n",
        "    super(ObjectiveSentsRemover, self).__init__()\n",
        "    if net == None:\n",
        "      raise TypeError(\"Network cannot be None-type\")\n",
        "    self.net = net\n",
        "    self.vocab = vocab\n",
        "  \n",
        "  def _indexed_corpus(self, corpus):\n",
        "    indexed_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for sent in doc:\n",
        "        new_sent = []\n",
        "        for word in sent:\n",
        "          new_sent.append(self.vocab[word])\n",
        "        new_doc.append(new_sent)\n",
        "      indexed_corpus.append(new_doc)\n",
        "    return indexed_corpus\n",
        "\n",
        "\n",
        "  def remove_objective_sents(self, corpus):\n",
        "    indexed_corpus = self._indexed_corpus(corpus)\n",
        "    print(len(corpus[0]))\n",
        "    with torch.no_grad():\n",
        "      self.net.eval()\n",
        "      # I want to keep the sentence if it is subjective i.e. when result of classification = 1\n",
        "      res = [[corpus[doc_idx][sent_idx] for sent_idx, sent in enumerate(doc) if self.net(collate_testing(sent))[0].max(dim = 1)[1].item() == 1]\n",
        "             for doc_idx, doc in enumerate(indexed_corpus)]\n",
        "      print(len(res[0]))\n",
        "    return res\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_objective_sents(corpus)\n",
        "\n",
        "class ShallowObjectiveSentsRemover(PipelineElement):\n",
        "  def __init__(self, threshold = .5, clf = MultinomialNB, trained = False):\n",
        "    self.vectorizer = CountVectorizer()\n",
        "    self.classifier = clf()\n",
        "    if not trained:\n",
        "      self.best_estimator = self._train()\n",
        "    else:\n",
        "      self.best_estimator = self.classifier\n",
        "  \n",
        "  def _train(self):\n",
        "    subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "    obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
        "\n",
        "    corpus = [self.neg_marking_list2str(d) for d in subj] + [self.neg_marking_list2str(d) for d in obj]\n",
        "    vectors = self.vectorizer.fit_transform(corpus)\n",
        "    labels = np.array([1] * len(subj) + [0] * len(obj))\n",
        "    scores = cross_validate(self.classifier, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['accuracy'], return_estimator=True)\n",
        "    estimator = scores[\"estimator\"][scores[\"test_accuracy\"].argmax()]\n",
        "    return estimator\n",
        "\n",
        "  def neg_marking_list2str(self, sent):\n",
        "    # takes the doc and produces a single list\n",
        "    # negates the whole document\n",
        "    negated_doc = mark_negation(sent, double_neg_flip=True)\n",
        "    return \" \".join([w for w in negated_doc])\n",
        "    \n",
        "\n",
        "  def remove_objective_sents(self, corpus):\n",
        "    transformed_corpus = [[self.vectorizer.transform([self.neg_marking_list2str(sent)]) for sent in doc] for doc in corpus]\n",
        "    print(len(corpus[0]))\n",
        "    res = [[corpus[doc_idx][sent_idx] for sent_idx, sent in enumerate(doc) if self.best_estimator.predict(sent).item()]\n",
        "           for doc_idx, doc in enumerate(transformed_corpus)]\n",
        "    print(len(res[0]))\n",
        "    return res\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_objective_sents(corpus)\n"
      ],
      "metadata": {
        "id": "d-wAhSa0ihJg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(UnderscoreRemover(),\n",
        "                    CharacterRepetitionRemover(),\n",
        "                    ApostrophesMerger(),\n",
        "                    ContractionCleaner(),\n",
        "                    SpecialCharsCleaner(),\n",
        "                    ShallowObjectiveSentsRemover(),\n",
        "                    )\n",
        "corpus = MovieReviewsCorpusLBSA(pipeline)\n",
        "# 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42sw8MWVfKpN",
        "outputId": "f54c9ed9-4a6e-4bd4-844f-45c5c6968bd6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = corpus.get_embedding_matrix(global_vectors, 300)\n",
        "dataset = MovieReviewsDatasetLBSA(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A6Hxlu_fL05",
        "outputId": "80b54b08-7802-4e31-c80c-5922294eafad"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = main_cross_validation(main_LBSA, dataset, embedding_matrix, collateLBSA, epochs = 12)\n",
        "\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU07k0-tjbx5",
        "outputId": "fcfb539e-e354-4dff-b1b0-3a457d0c8235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold: 0\n",
            "epoch 0:\n",
            "Training loss: 0.007720752822028266 \n",
            " Training accuracy: 58.166666666666664\n",
            "Test loss: 0.009283317625522614 \n",
            " Test accuracy: 51.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.005954421096377903 \n",
            " Training accuracy: 76.83333333333333\n",
            "Test loss: 0.008689112067222594 \n",
            " Test accuracy: 69.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.004417901304033067 \n",
            " Training accuracy: 84.88888888888889\n",
            "Test loss: 0.007869709432125092 \n",
            " Test accuracy: 80.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.0034277454846435122 \n",
            " Training accuracy: 91.77777777777779\n",
            "Test loss: 0.007329189479351044 \n",
            " Test accuracy: 67.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ff92W57YlpuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('nlu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e0262c2e7a08424d65c968f8ecfc5afb6b5a99089f86fd0fa27478ea619b0ef2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}