{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinni98/Sentiment-analysis-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NBg5KtW5AMy"
      },
      "source": [
        "## Polarity Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oGXvFNN46OT"
      },
      "source": [
        "### Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvHgFZJe4_Xq",
        "outputId": "d85b072b-eef3-4934-a146-e47b29c513e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/subjectivity.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import torch\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"movie_reviews\")\n",
        "nltk.download(\"subjectivity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApFi70um_WPB"
      },
      "source": [
        "## TODO: Use spacy negation marking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "C3PyvlOV60V7",
        "outputId": "2c1f8168-15f4-48d8-d399-ad6367b11b61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'class MovieReviewsCorpusNegMarked(MovieReviewsCorpus):\\n  def __init__(self):\\n    super().__init__()\\n    self._neg_marking()\\n    self.corpus_words = self.get_corpus_words()\\n    self.vocab = self._create_vocab()\\n  \\n  def _neg_marking(self):\\n    negated_corpus = [self._mark(doc) for doc in self.corpus]\\n    self.corpus = negated_corpus\\n  \\n  def _mark(self, doc):\\n    # negates the whole document\\n    negated_doc = mark_negation(doc, double_neg_flip=True)\\n    return negated_doc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "from nltk.sentiment.util import mark_negation\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MovieReviewsCorpus():\n",
        "  def __init__(self):\n",
        "    # list of documents, each document is a list containing words of that document\n",
        "    self.mr = movie_reviews\n",
        "    self.corpus, self.labels = self._flatten()\n",
        "    self.unprocessed_corpus = self._get_corpus()\n",
        "    self.corpus_words = self.get_corpus_words()\n",
        "    self.vocab = self._create_vocab()\n",
        "\n",
        "  \n",
        "  def _list_to_str(self, doc) -> str:\n",
        "    \"\"\"\n",
        "    Put all elements of the list into a single string, separating each element with a space.\n",
        "    \"\"\"\n",
        "    return \" \".join([w for sent in doc for w in sent])\n",
        "  \n",
        "  \n",
        "  def _flatten(self):\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    list[list[str]]\n",
        "      Each inner list represents a document. Each document is a list of tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # 3 nested list: each list contain a document, each inner list contains a phrase (until fullstop), each phrase contains words.\n",
        "    neg = self.mr.paras(categories = \"neg\")\n",
        "    pos = self.mr.paras(categories = \"pos\")\n",
        "\n",
        "    corpus = [[w for w in self._list_to_str(d).split(\" \")] for d in pos] + [[w for w in self._list_to_str(d).split(\" \")] for d in neg]\n",
        "    labels = [0] * len(pos) + [1] * len(neg)\n",
        "    return corpus, labels\n",
        "  \n",
        "  def _get_corpus(self):\n",
        "    neg = self.mr.paras(categories = \"neg\")\n",
        "    pos = self.mr.paras(categories = \"pos\")\n",
        "\n",
        "    return neg + pos\n",
        "\n",
        "  def movie_reviews_dataset_raw(self):\n",
        "    \"\"\"\n",
        "    Returns the dataset containing:\n",
        "    \n",
        "    - A list of all the documents\n",
        "    - The corresponding label for each document\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple(list, list)\n",
        "      The dataset: first element is the list of the document, the second element of the tuple is the associated label (positive or negative) for each document\n",
        "    \"\"\"\n",
        "    \n",
        "    return self.corpus, self.labels\n",
        "  \n",
        "  def get_sentence_ds(self):\n",
        "    neg = self.mr.paras(categories = \"neg\")\n",
        "    pos = self.mr.paras(categories = \"pos\")\n",
        "\n",
        "    pos = [phrase for doc in pos for phrase in doc]\n",
        "    neg = [phrase for doc in neg for phrase in doc]\n",
        "\n",
        "    labels = np.array([0] * len(pos) + [1] * len(neg))\n",
        "    corpus = neg+pos\n",
        "    return corpus, labels\n",
        "\n",
        "  \n",
        "  def get_corpus_words(self) -> list:\n",
        "    return [w for doc in self.corpus for w in doc]\n",
        "\n",
        "  def _create_vocab(self):\n",
        "    vocab = dict()\n",
        "    for word in self.corpus_words:\n",
        "      try:\n",
        "        vocab[word] += 1\n",
        "      except:\n",
        "        vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "\n",
        "\n",
        "\"\"\"class MovieReviewsCorpusNegMarked(MovieReviewsCorpus):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self._neg_marking()\n",
        "    self.corpus_words = self.get_corpus_words()\n",
        "    self.vocab = self._create_vocab()\n",
        "  \n",
        "  def _neg_marking(self):\n",
        "    negated_corpus = [self._mark(doc) for doc in self.corpus]\n",
        "    self.corpus = negated_corpus\n",
        "  \n",
        "  def _mark(self, doc):\n",
        "    # negates the whole document\n",
        "    negated_doc = mark_negation(doc, double_neg_flip=True)\n",
        "    return negated_doc\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZaF5yV9rlo-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class MovieReviewsDataset(Dataset):\n",
        "  def __init__(self, raw_dataset):\n",
        "    self.corpus = np.array(raw_dataset[0], dtype = object)\n",
        "    self.labels = np.array(raw_dataset[1], dtype = object)\n",
        "    self.max_element = len(max(self.corpus, key=lambda x: len(x)))\n",
        "    self.elements_to_tensor()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "  \n",
        "  def elements_to_tensor(self):\n",
        "    global_vectors = GloVe(name='840B', dim=300)\n",
        "    for idx, item in enumerate(self.corpus):\n",
        "      item_tensor = torch.empty(len(item), 300)\n",
        "      for i in range(len(item)):\n",
        "        token = item[i]\n",
        "        item_tensor[i] = global_vectors.get_vecs_by_tokens(token)\n",
        "      self.corpus[idx] = item_tensor\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    item = self.corpus[index]\n",
        "    label = self.labels[index]\n",
        "    return (item, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr7PPG1k4gFq"
      },
      "source": [
        "### Create the model class\n",
        "Let's first try with a simple BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCvqaJ8-hKmT"
      },
      "outputs": [],
      "source": [
        "from unicodedata import bidirectional\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, device = \"cuda\", input_size = 300, hidden_size = 128, output_size = 2):\n",
        "    super(BiLSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True, bidirectional=True, num_layers = 2)\n",
        "    self.fc = nn.Sequential(nn.ReLU(),\n",
        "                            nn.BatchNorm1d(hidden_size*2, eps = 1e-08),\n",
        "                            nn.Dropout(0.3),\n",
        "                            nn.Linear(hidden_size*2, output_size)\n",
        "                            )\n",
        "    \n",
        "    \n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "      if self.cuda:\n",
        "        return (torch.zeros(4, batch_size, self.hidden_size).to(self.device),\n",
        "                torch.zeros(4, batch_size, self.hidden_size).to(self.device),)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size = x.batch_sizes[0].item()\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "\n",
        "    # output: batch_size, sequence_length, hidden_size * 2 (since is bilstm)\n",
        "    out, _ = self.lstm(x, hidden)\n",
        "    out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "    # Interested only in the last layer\n",
        "    out = out[list(range(batch_size)), input_sizes - 1, :]\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out\n",
        "    \n",
        "\n",
        "# Inspired by https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-decoder\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, max_length, dropout_p=0.1, device = \"cuda\"):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.dropout(input)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZFg9BvQdZHB"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    in_size = targets.size(dim=0)\n",
        "\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJef4h1cfWym"
      },
      "outputs": [],
      "source": [
        "def test_step(net, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      in_size = targets.size(dim=0)\n",
        "\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = outputs.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import torch.optim as optim\n",
        "\n",
        "class AnnealingOptimizer(torch.optim.Optimizer, ABC):\n",
        "  \"\"\"\n",
        "  Defines and abstract class in order to implement an sgd optimizer using an annealing strategy\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0) -> None:\n",
        "    if not 0.0 <= lr:\n",
        "      raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "    if not 0 <= epoch:\n",
        "      raise ValueError(f\"Invalid epoch value: {epoch}\")\n",
        "    \n",
        "    self.nr_epochs = nr_epochs\n",
        "    self.epoch = epoch\n",
        "    self._alpha = 10\n",
        "    self._beta = 0.75\n",
        "    self._base_lr = lr\n",
        "\n",
        "  def update_lr(self):\n",
        "    \"\"\"\n",
        "    Updates the learning rate using the annealing strategy.\n",
        "    In order to let the annealing strategy to work correctly, this method should be called at every epoch during the network training\n",
        "\n",
        "    The learning rate for the classifier is 10 times bigger as proposed in the [Symnet paper](https://arxiv.org/pdf/1904.04663.pdf)\n",
        "    \"\"\"\n",
        "    self.epoch += 1\n",
        "    new_lr = self._compute_lr()\n",
        "    for g in self.optimizer.param_groups:\n",
        "      if g[\"name\"] == \"fe\":\n",
        "        g[\"lr\"] = new_lr\n",
        "      else:\n",
        "        g[\"lr\"] = new_lr*10\n",
        "\n",
        "    \n",
        "  def _compute_lr(self):\n",
        "    \"\"\"\n",
        "    Computes the learning rate using the proposed annealing strategy\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      updated learning rate\n",
        "    \"\"\"\n",
        "    etap = 1 / ((1 + self._alpha * self.epoch / self.nr_epochs ) ** self._beta)\n",
        "    return self._base_lr * etap\n",
        "\n",
        "  def step(self):\n",
        "    self.optimizer.step()\n",
        "  \n",
        "  def zero_grad(self):\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "class BiLSTMOptimizer(AnnealingOptimizer):\n",
        "  \"\"\"\n",
        "  Implements an annealing optimizer for Resnet\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0, momentum: float = 0.9) -> None:\n",
        "    super(BiLSTMOptimizer ,self).__init__(model, nr_epochs, lr, epoch)\n",
        "    \n",
        "    # Note that names for parameters group are important in order to update each group differently\n",
        "    self.optimizer = optim.SGD([\n",
        "                {'params': model.lstm.parameters(), \"name\": \"fe\"},\n",
        "                {'params': model.fc.parameters(), \"lr\": self._compute_lr()*10, \"name\": \"classifier\"}\n",
        "            ], lr=lr, momentum=momentum)\n",
        "    "
      ],
      "metadata": {
        "id": "Gn3jBnKtUb_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOeUmEHZNBvw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "\n",
        "def main(train_loader, test_loader, max_element, device = \"cuda\", epochs = 10):\n",
        "\n",
        "  net = BiLSTM(device = device).to(device)\n",
        "  decoder = \n",
        "\n",
        "  optimizer = Adam(net.parameters(), 0.001, betas = (0.9, 0.999), amsgrad=True)\n",
        "\n",
        "  cost_function = nn.CrossEntropyLoss()\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    test_loss, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "    print(f\"Test loss: {test_loss} \\n Test accuracy: {test_accuracy}\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "  \n",
        "  _, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "\n",
        "\n",
        "  return test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfNtrS8jmATx"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def pad(batch, max_size):\n",
        "  pad = torch.zeros(batch[0].size(dim=1))\n",
        "  for idx in range(len(batch)):\n",
        "    remaining = max_size - batch[idx].size(dim = 0)\n",
        "    batch[idx] = torch.cat((batch[idx], pad.repeat((remaining, 1))), dim = 0)\n",
        "  return batch\n",
        "\n",
        "def batch_to_tensor(X: List[torch.tensor], max_size):\n",
        "  X_tensor = torch.zeros(len(X), max_size, X[0].size(dim = 1))\n",
        "  for i, embed in enumerate(X):\n",
        "    X_tensor[i] = embed\n",
        "  return X_tensor\n",
        "\n",
        "def sort_ds(X, Y):\n",
        "  \"\"\"\n",
        "  Sort inputs by document lengths\n",
        "  \"\"\"\n",
        "  document_lengths = np.array([tens.size(dim = 0) for tens in X])\n",
        "  indexes = np.argsort(document_lengths)\n",
        "\n",
        "  X_sorted = X[indexes][::-1]\n",
        "  Y_sorted = Y[indexes][::-1]\n",
        "  document_lengths = torch.from_numpy(document_lengths[indexes][::-1].copy())\n",
        "\n",
        "  return X_sorted, Y_sorted, document_lengths\n",
        "\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "  X, Y = list(zip(*batch))\n",
        "  Y = np.array(list(Y))\n",
        "  X = np.array(list(X))\n",
        "\n",
        "  # Sort dataset\n",
        "  X, Y, document_lengths = sort_ds(X, Y)\n",
        "\n",
        "  # Get tensor sizes\n",
        "  max_size = torch.max(document_lengths).item()\n",
        "\n",
        "  # Pad tensor each element\n",
        "  X = pad(X, max_size)\n",
        "\n",
        "  # Transform the batch to a tensor\n",
        "  X_tensor = batch_to_tensor(X, max_size)\n",
        "  \n",
        "  # Return the padded sequence object\n",
        "  X_final = pack_padded_sequence(X_tensor, document_lengths, batch_first=True)\n",
        "  return X_final, torch.from_numpy(Y.copy())\n",
        "\n",
        "def get_data(batch_size: int, collate_fn):\n",
        "  batch = MovieReviewsCorpus()\n",
        "\n",
        "  dataset = MovieReviewsDataset(batch.movie_reviews_dataset_raw())\n",
        "\n",
        "  max_element = dataset.max_element\n",
        "\n",
        "  # Random Split\n",
        "\n",
        "  train_indexes, test_indexes = train_test_split(list(range(len(dataset.labels))), test_size = 0.2,\n",
        "                                                 stratify = dataset.labels, random_state = 42)\n",
        "  \n",
        "  train_ds = Subset(dataset, train_indexes)\n",
        "  test_ds = Subset(dataset, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "  test_loader = DataLoader(test_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "\n",
        "  return train_loader, test_loader, max_element"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader, max_element = get_data(128, collate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ85bpeYi8xN",
        "outputId": "59b67f93-c8b3-4d52-d23b-6268ba4e61da"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eDrwHvOIEsl2",
        "outputId": "cdde993a-7c93-4a7d-ebd9-70b1686166c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
            "        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
            "        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "tensor([[-3.6912e-03,  7.0637e-01],\n",
            "        [-4.3693e-01, -3.9087e-01],\n",
            "        [ 1.0633e+00,  7.6288e-02],\n",
            "        [ 5.4099e-01,  5.5560e-02],\n",
            "        [-3.3789e-01,  3.3105e-01],\n",
            "        [-4.6595e-01, -4.0238e-01],\n",
            "        [ 1.8964e-02,  4.3120e-01],\n",
            "        [-5.6574e-01,  5.9941e-01],\n",
            "        [-1.7378e-01,  1.3300e-01],\n",
            "        [-3.3249e-01,  6.6812e-02],\n",
            "        [-4.1974e-01, -6.7901e-01],\n",
            "        [-4.2615e-01, -9.3330e-02],\n",
            "        [ 6.2844e-01,  3.4237e-01],\n",
            "        [-5.5788e-01,  7.8848e-01],\n",
            "        [ 8.5738e-01, -7.6176e-02],\n",
            "        [ 2.9980e-01,  2.5140e-01],\n",
            "        [-2.5355e-01, -1.1924e-04],\n",
            "        [ 7.2396e-01, -1.0698e+00],\n",
            "        [-4.5904e-03,  6.1410e-03],\n",
            "        [ 7.6647e-01,  9.0205e-02],\n",
            "        [-1.0424e-01,  3.4817e-01],\n",
            "        [-2.6666e-01,  4.4229e-01],\n",
            "        [-2.3957e-01,  2.7361e-01],\n",
            "        [ 4.3275e-01, -1.5695e-01],\n",
            "        [-9.3005e-02,  1.8309e-01],\n",
            "        [-6.0595e-03, -1.1038e+00],\n",
            "        [ 1.1551e+00, -3.9035e-01],\n",
            "        [-7.7588e-01,  3.0609e-01],\n",
            "        [-1.5087e-01, -1.5178e-01],\n",
            "        [-4.7677e-01,  6.3745e-01],\n",
            "        [-2.4384e-01, -1.0309e+00],\n",
            "        [ 2.4653e-01, -6.1709e-01],\n",
            "        [ 5.5729e-01,  3.2064e+00],\n",
            "        [ 4.2578e-01, -4.5291e-02],\n",
            "        [-5.5637e-02,  5.0955e-01],\n",
            "        [-2.1825e-01, -1.6211e-01],\n",
            "        [ 2.3021e-01,  6.4421e-01],\n",
            "        [ 7.4928e-02,  7.6053e-01],\n",
            "        [ 1.1057e+00, -4.9116e-01],\n",
            "        [ 6.4208e-02,  2.5683e-01],\n",
            "        [-4.2634e-01, -1.8715e-01],\n",
            "        [ 9.3262e-01,  4.5432e-01],\n",
            "        [ 2.9333e-02,  3.0566e-01],\n",
            "        [-3.0914e-01,  3.5407e-01],\n",
            "        [-1.4217e+00,  2.5583e-01],\n",
            "        [ 9.9146e-02,  3.1808e-01],\n",
            "        [ 3.5012e-03, -3.6813e-01],\n",
            "        [ 7.3220e-02, -1.5043e-01],\n",
            "        [-2.6205e-01,  4.0294e-01],\n",
            "        [ 1.8651e+00, -9.5398e-01],\n",
            "        [-8.6494e-01, -5.7082e-03],\n",
            "        [ 3.4221e-01, -6.2578e-02],\n",
            "        [ 6.7347e-01, -1.6278e-01],\n",
            "        [-2.0813e-01, -4.2154e-02],\n",
            "        [ 3.4194e-01,  6.8695e-02],\n",
            "        [-5.2402e-01,  5.6908e-01],\n",
            "        [-6.6616e-02,  3.7479e-01],\n",
            "        [ 2.3114e-01, -7.1186e-01],\n",
            "        [ 1.6328e-01,  9.0367e-02],\n",
            "        [-7.3581e-01, -7.8816e-02],\n",
            "        [-7.2432e-01, -5.2206e-01],\n",
            "        [ 3.9608e-01, -5.0382e-02],\n",
            "        [-1.8586e-01, -4.3863e-01],\n",
            "        [ 6.3142e-01, -1.0483e-01],\n",
            "        [-2.4044e-02, -8.4304e-01],\n",
            "        [-6.9628e-01, -4.9033e-02],\n",
            "        [-4.4592e-02,  3.1516e-01],\n",
            "        [ 1.9619e-01, -4.0558e-01],\n",
            "        [-4.2154e-01,  2.1895e-01],\n",
            "        [ 8.1024e-01,  6.5530e-01],\n",
            "        [ 1.0271e+00, -7.3191e-01],\n",
            "        [ 8.6151e-01, -1.8200e-01],\n",
            "        [ 2.1248e-01,  5.1582e-01],\n",
            "        [ 2.2522e-01, -1.5927e+00],\n",
            "        [ 1.6557e+00, -4.8928e-01],\n",
            "        [-5.8950e-01, -4.5172e-03],\n",
            "        [-6.9864e-01,  1.0986e-01],\n",
            "        [-7.5491e-02,  4.8186e-02],\n",
            "        [-1.8959e-02, -4.4636e-01],\n",
            "        [-5.4641e-01,  3.8151e-01],\n",
            "        [ 3.9357e-01,  5.4153e-02],\n",
            "        [ 1.7185e+00,  6.1610e-01],\n",
            "        [-9.9464e-03,  5.8818e-01],\n",
            "        [-8.1080e-03,  4.6144e-01],\n",
            "        [-2.4868e-01, -1.9406e-01],\n",
            "        [ 1.0834e-02,  4.3151e-01],\n",
            "        [-7.2545e-02,  4.0238e-01],\n",
            "        [-2.6861e-01,  1.9193e-01],\n",
            "        [-9.3609e-02, -2.7940e-01],\n",
            "        [ 1.0819e+00,  6.0933e-01],\n",
            "        [ 2.3764e+00, -2.1918e+00],\n",
            "        [-5.8716e-01, -2.4501e-01],\n",
            "        [-8.6713e-02,  2.3989e-01],\n",
            "        [-7.1707e-02, -2.9489e-01],\n",
            "        [-1.7432e-01, -2.5988e-02],\n",
            "        [ 6.2835e-01,  7.3063e-01],\n",
            "        [-8.3116e-01, -4.4289e-02],\n",
            "        [ 1.5018e+00,  1.4189e-02],\n",
            "        [ 5.4472e-01,  2.2247e-01],\n",
            "        [ 1.3359e+00, -1.0043e+00],\n",
            "        [-1.7093e-01,  8.5706e-01],\n",
            "        [-6.7157e-01, -4.9736e-01],\n",
            "        [-6.3444e-02,  7.0262e-01],\n",
            "        [ 2.5484e-01,  2.8150e-01],\n",
            "        [ 4.2530e-01, -7.3925e-01],\n",
            "        [ 1.0083e+00, -2.5200e-02],\n",
            "        [-7.6547e-02,  3.5252e-01],\n",
            "        [-8.7839e-01,  1.2303e+00],\n",
            "        [ 6.8816e-02, -1.0891e-02],\n",
            "        [-3.5294e-01, -2.1158e-01],\n",
            "        [-2.5672e-01,  3.6556e-01],\n",
            "        [-3.6647e-01, -5.4714e-01],\n",
            "        [ 1.4910e-03,  1.6003e-01],\n",
            "        [-2.2029e-01,  9.0362e-02],\n",
            "        [ 1.2100e+00,  8.8576e-02],\n",
            "        [-6.2460e-02,  2.0554e-01],\n",
            "        [-6.8501e-02,  3.9076e-01],\n",
            "        [ 6.1619e-01, -8.7906e-01],\n",
            "        [-1.7218e-02,  1.6402e-01],\n",
            "        [-3.1762e-01, -9.3467e-01],\n",
            "        [-4.3063e-01,  2.6168e-01],\n",
            "        [-7.1018e-01,  7.1941e-01],\n",
            "        [ 2.8861e-03, -3.4012e-01],\n",
            "        [ 6.8576e-01,  3.5299e-01],\n",
            "        [-5.3581e-01, -4.1126e-01],\n",
            "        [-2.0742e-01,  4.7834e-01],\n",
            "        [ 1.9866e-01,  4.0830e-01],\n",
            "        [ 5.2363e-01, -4.0011e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
            "        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
            "        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
            "        1, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "tensor([[ 2.1524e+00, -6.8467e-01],\n",
            "        [-1.4475e-01, -8.8017e-01],\n",
            "        [ 1.3306e+00, -6.1956e-01],\n",
            "        [-2.7914e-01,  1.5585e-03],\n",
            "        [ 1.3911e-02, -1.4675e+00],\n",
            "        [ 2.3446e-01, -8.6974e-02],\n",
            "        [-2.2723e-01,  6.2092e-02],\n",
            "        [ 3.7255e+00, -4.1211e+00],\n",
            "        [ 6.5588e-01,  1.0961e-01],\n",
            "        [-6.6308e-01, -4.4073e-01],\n",
            "        [ 8.6944e-02, -3.6005e-02],\n",
            "        [-6.7634e-01,  8.0950e-01],\n",
            "        [ 1.1066e-01,  2.1518e-01],\n",
            "        [ 4.1048e-01, -5.5735e-01],\n",
            "        [ 1.1536e-01,  1.7198e-01],\n",
            "        [-6.4233e-01,  2.9180e-01],\n",
            "        [-3.7282e-02,  7.9264e-02],\n",
            "        [-3.3926e-01, -1.1263e-01],\n",
            "        [ 2.7287e-01, -1.2850e-01],\n",
            "        [ 2.4919e-01, -2.8703e-01],\n",
            "        [-2.5616e-01, -2.5543e-02],\n",
            "        [ 2.0810e+00, -8.1835e-01],\n",
            "        [-1.7561e+00,  1.6360e+00],\n",
            "        [ 2.3466e-01, -6.4542e-01],\n",
            "        [-1.7914e-02, -2.8879e-01],\n",
            "        [ 2.1360e-01,  3.0025e-01],\n",
            "        [ 6.5377e-01, -8.2901e-02],\n",
            "        [-4.9998e-02,  1.0084e-01],\n",
            "        [-1.3909e+00, -8.0247e-02],\n",
            "        [-7.6537e-02,  2.5362e-01],\n",
            "        [ 2.2273e+00, -4.0154e-01],\n",
            "        [ 2.2636e-01, -3.6512e-02],\n",
            "        [ 1.2342e+00, -2.7823e-01],\n",
            "        [ 5.2632e-01, -5.3460e-01],\n",
            "        [-9.8011e-01,  7.2503e-01],\n",
            "        [-2.0672e+00, -2.6794e-01],\n",
            "        [ 1.3621e+00, -1.0485e+00],\n",
            "        [-1.1773e+00, -3.8445e-01],\n",
            "        [ 4.9046e-01, -6.5857e-01],\n",
            "        [ 9.9771e-01, -1.7486e-01],\n",
            "        [ 1.3053e-02,  4.0452e-02],\n",
            "        [-1.1273e+00,  4.9154e-01],\n",
            "        [ 8.3896e-01, -1.0936e-01],\n",
            "        [-7.5636e-01, -2.3930e-01],\n",
            "        [ 9.2447e-02,  9.1782e-03],\n",
            "        [-1.1683e+00,  8.5645e-03],\n",
            "        [ 6.3174e-01, -8.3718e-01],\n",
            "        [-2.1746e+00, -4.4097e-01],\n",
            "        [-1.0676e+00,  4.4249e-01],\n",
            "        [ 5.0740e-01,  4.2859e-01],\n",
            "        [ 6.3250e-01,  1.5606e-01],\n",
            "        [-3.9798e-02, -2.9771e-01],\n",
            "        [ 8.1388e-01, -9.4900e-02],\n",
            "        [-9.2668e-01,  1.0787e-02],\n",
            "        [-1.5998e+00,  2.7915e-01],\n",
            "        [ 7.5463e-01,  3.4312e-01],\n",
            "        [ 3.0351e-01,  1.6744e-01],\n",
            "        [ 7.5421e-01,  9.3429e-01],\n",
            "        [ 5.8720e-01, -6.0754e-02],\n",
            "        [-3.0870e-01, -1.6993e-01],\n",
            "        [ 6.3647e-01,  1.1142e-01],\n",
            "        [-1.9617e-01,  1.2322e+00],\n",
            "        [ 2.3365e-01, -1.0519e-01],\n",
            "        [-8.7223e-01,  1.3860e-01],\n",
            "        [-4.5360e-01,  1.1453e-01],\n",
            "        [-1.3386e+00,  2.2216e-01],\n",
            "        [ 3.4812e-02, -7.0019e-01],\n",
            "        [-2.0156e-01,  1.1560e-01],\n",
            "        [ 4.1532e-02,  1.9052e-01],\n",
            "        [ 6.7205e-01,  3.0653e-02],\n",
            "        [-9.0849e-02,  3.0663e-01],\n",
            "        [-1.2009e-01,  8.0255e-01],\n",
            "        [ 7.3095e-01, -6.4764e-01],\n",
            "        [-9.6567e-02, -7.4275e-01],\n",
            "        [ 3.6277e-01, -4.6100e-01],\n",
            "        [-1.1446e+00,  7.2820e-01],\n",
            "        [-6.3114e-01,  6.5725e-01],\n",
            "        [ 5.9647e-02,  6.9485e-02],\n",
            "        [ 1.2185e-01,  2.9456e-01],\n",
            "        [ 2.0429e-01, -2.2054e-01],\n",
            "        [ 2.9456e-02, -6.6472e-01],\n",
            "        [ 1.0262e+00, -1.0899e+00],\n",
            "        [ 1.9728e-01,  3.6566e-02],\n",
            "        [-4.2852e-01, -2.1237e-01],\n",
            "        [-4.3252e-01,  6.3792e-01],\n",
            "        [-1.6182e-01, -5.1180e-01],\n",
            "        [-3.0045e-01,  1.4141e-03],\n",
            "        [ 4.8775e-01,  6.8398e-01],\n",
            "        [ 5.7946e-01,  1.5958e-01],\n",
            "        [-1.5345e-01,  3.9338e-02],\n",
            "        [-1.0717e+00, -8.6719e-01],\n",
            "        [-4.1973e-01, -6.3602e-01],\n",
            "        [-1.4620e-01,  5.0874e-01],\n",
            "        [-1.3050e-01, -2.5684e-01],\n",
            "        [ 2.6515e-01,  4.3000e-01],\n",
            "        [ 8.8677e-01, -3.8364e-01],\n",
            "        [-3.1174e-01,  1.7543e-01],\n",
            "        [ 3.3544e-01, -4.7635e-03],\n",
            "        [-3.4482e-01,  1.6044e-01],\n",
            "        [ 3.0624e-01, -1.0075e+00],\n",
            "        [ 5.6797e-01,  2.6789e-01],\n",
            "        [-3.9860e-01,  6.9047e-01],\n",
            "        [ 1.9941e+00,  1.1460e+00],\n",
            "        [ 3.9630e-02, -4.8716e-01],\n",
            "        [-3.5433e-01,  7.5112e-01],\n",
            "        [-7.2069e-01,  8.2989e-01],\n",
            "        [-6.9698e-01,  5.4610e-01],\n",
            "        [ 1.3053e-01, -4.7774e-03],\n",
            "        [ 3.9195e-01,  7.2827e-02],\n",
            "        [ 3.5134e-01, -2.9232e-01],\n",
            "        [-3.0949e-01,  4.9739e-01],\n",
            "        [ 1.0357e-01,  6.1292e-02],\n",
            "        [ 9.6397e-02,  5.4645e-01],\n",
            "        [ 1.2970e+00,  1.2981e+00],\n",
            "        [-6.8074e-01,  4.8273e-01],\n",
            "        [ 5.1363e-01, -7.6973e-01],\n",
            "        [ 9.3586e-02,  7.6832e-02],\n",
            "        [ 3.5271e-01,  1.4823e-01],\n",
            "        [ 3.6951e-01,  3.9345e-01],\n",
            "        [-1.1870e-01,  3.3044e-02],\n",
            "        [-1.2494e-01,  7.0178e-02],\n",
            "        [-1.2957e-01, -4.2890e-01],\n",
            "        [-4.0881e-01,  4.9593e-01],\n",
            "        [ 8.3568e-01, -1.7592e-01],\n",
            "        [-1.1399e-01,  8.0535e-01],\n",
            "        [ 1.3875e-01, -3.8636e-01],\n",
            "        [-4.3851e-01, -2.2554e-01],\n",
            "        [ 8.0612e-02,  4.5985e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
            "        1, 1, 1, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "tensor([[ 0.6083,  0.5056],\n",
            "        [-0.0225,  0.1473],\n",
            "        [ 0.2145,  0.3126],\n",
            "        [-0.1634, -0.4158],\n",
            "        [ 0.1001,  0.0356],\n",
            "        [ 0.1382,  0.1860],\n",
            "        [-0.5892,  0.2875],\n",
            "        [-0.7602,  0.2388],\n",
            "        [ 0.4075, -0.4064],\n",
            "        [ 0.8491, -0.2303],\n",
            "        [-0.7398, -0.6313],\n",
            "        [ 0.0752,  0.4024],\n",
            "        [-0.2024, -0.7623],\n",
            "        [-0.2456, -0.0722],\n",
            "        [-0.3594,  0.0639],\n",
            "        [-0.7045, -0.5888],\n",
            "        [-0.2270, -0.2468],\n",
            "        [ 0.2869,  0.2733],\n",
            "        [-0.1870, -0.2954],\n",
            "        [ 0.2149,  0.2742],\n",
            "        [ 0.5323,  0.2119],\n",
            "        [-0.5053, -1.0442],\n",
            "        [ 0.1739,  0.3150],\n",
            "        [ 0.3450,  0.0486],\n",
            "        [ 0.9030, -0.5443],\n",
            "        [ 0.1663, -0.1464],\n",
            "        [ 0.4791,  0.4135],\n",
            "        [-0.0256,  0.0456],\n",
            "        [ 1.1137,  0.1003],\n",
            "        [ 0.1835,  1.0563],\n",
            "        [ 0.0141, -0.0061],\n",
            "        [-0.8589, -0.7318],\n",
            "        [-0.3030,  0.9007],\n",
            "        [ 0.1658, -0.3851],\n",
            "        [ 0.5942, -1.4034],\n",
            "        [ 1.7718, -0.7520],\n",
            "        [ 0.1322, -0.9986],\n",
            "        [-0.3567, -0.4713],\n",
            "        [ 1.7796, -2.2360],\n",
            "        [ 1.0407, -2.3839],\n",
            "        [-0.3818,  0.1158],\n",
            "        [-0.2301, -0.4300],\n",
            "        [-0.2527,  0.2438],\n",
            "        [-0.1813, -0.2708],\n",
            "        [ 0.2946,  1.1205],\n",
            "        [-0.4363, -0.5327],\n",
            "        [ 0.2669,  0.1994],\n",
            "        [-0.2178, -0.1736],\n",
            "        [ 0.2225, -0.4808],\n",
            "        [-2.2737,  3.0299],\n",
            "        [ 0.6854,  0.1713],\n",
            "        [-0.5664,  0.2587],\n",
            "        [-0.1784, -0.3094],\n",
            "        [ 0.5865, -0.0163],\n",
            "        [-0.2692,  0.5824],\n",
            "        [-0.3767, -0.0681],\n",
            "        [-0.0325,  0.8520],\n",
            "        [-0.1943,  0.0727],\n",
            "        [-1.5635,  0.6666],\n",
            "        [ 1.9878, -1.3791],\n",
            "        [-0.6924, -0.2640],\n",
            "        [ 0.4971, -0.4363],\n",
            "        [-0.2397, -0.7872],\n",
            "        [ 0.8061, -0.6068],\n",
            "        [-0.2866,  0.0708],\n",
            "        [-0.2971,  0.2989],\n",
            "        [-0.3062, -0.6798],\n",
            "        [ 0.0552, -0.3522],\n",
            "        [-0.9454,  0.2162],\n",
            "        [-0.3105,  0.5557],\n",
            "        [-0.1706,  0.0884],\n",
            "        [-0.0402,  0.3038],\n",
            "        [-0.3683, -0.3877],\n",
            "        [ 0.3178, -0.2832],\n",
            "        [ 0.6495,  0.5434],\n",
            "        [-0.3444,  0.9201],\n",
            "        [-0.2104, -0.3074],\n",
            "        [-0.0581,  0.2772],\n",
            "        [ 0.1910, -0.0552],\n",
            "        [-0.2147,  0.6504],\n",
            "        [-0.0659,  0.3912],\n",
            "        [-0.1135, -0.1005],\n",
            "        [ 0.1615,  1.1013],\n",
            "        [-0.1164, -0.1260],\n",
            "        [ 0.8246, -0.0420],\n",
            "        [-0.0866, -0.0544],\n",
            "        [ 2.5670, -0.1958],\n",
            "        [ 0.1330,  0.1732],\n",
            "        [ 0.0567,  0.4502],\n",
            "        [-0.1922, -0.0941],\n",
            "        [-0.0521, -0.2696],\n",
            "        [ 0.1580, -0.2259],\n",
            "        [-0.1286, -0.1456],\n",
            "        [ 0.0398,  0.2745],\n",
            "        [ 0.4882, -0.0398],\n",
            "        [-0.5209,  0.0564],\n",
            "        [ 0.1223,  0.0361],\n",
            "        [-0.2833,  0.6721],\n",
            "        [ 0.0712,  0.1240],\n",
            "        [ 2.2455,  0.9770],\n",
            "        [ 0.0656,  0.4534],\n",
            "        [ 0.0837,  1.3505],\n",
            "        [-0.3773, -0.0948],\n",
            "        [ 0.1744,  0.0852],\n",
            "        [ 0.1741,  0.1733],\n",
            "        [-0.1018,  0.4140],\n",
            "        [-0.2963,  0.9662],\n",
            "        [ 1.2773, -0.0761],\n",
            "        [ 0.4092,  0.4250],\n",
            "        [-0.6673, -0.0427],\n",
            "        [-0.4225, -0.1924],\n",
            "        [-0.2514,  0.1351],\n",
            "        [ 0.5905,  0.2189],\n",
            "        [ 0.3633, -0.2943],\n",
            "        [ 1.0951, -1.2563],\n",
            "        [ 0.4162, -0.2516],\n",
            "        [-0.0430,  0.3867],\n",
            "        [-0.8896,  0.0615],\n",
            "        [ 0.0338,  0.8862],\n",
            "        [-0.2208,  0.5930],\n",
            "        [ 0.3642, -0.2270],\n",
            "        [ 0.4758, -0.5487],\n",
            "        [ 0.2276,  0.0067],\n",
            "        [ 0.4917,  0.0614],\n",
            "        [-0.1386, -0.7495],\n",
            "        [-0.2267, -0.2317],\n",
            "        [ 0.1709, -0.1417],\n",
            "        [-0.4553, -0.3125]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
            "        0, 0, 1, 1, 0, 0, 1, 0], device='cuda:0')\n",
            "tensor([[ 5.8994e-03,  4.4060e-01],\n",
            "        [-2.2423e-01,  3.5278e-01],\n",
            "        [ 9.4407e-02, -2.5357e-01],\n",
            "        [-9.8077e-01, -5.4743e-02],\n",
            "        [-2.2359e-01,  3.2831e-01],\n",
            "        [-2.4747e-03,  2.0161e-01],\n",
            "        [ 1.1748e-01,  1.4861e-01],\n",
            "        [ 9.5704e-02,  1.3822e-01],\n",
            "        [-3.5432e-01, -2.9209e-03],\n",
            "        [ 3.9921e-01, -6.2203e-01],\n",
            "        [-1.6828e-01,  3.4772e-01],\n",
            "        [-3.9464e-01,  9.7729e-02],\n",
            "        [ 3.0139e-01, -6.4242e-02],\n",
            "        [ 6.7649e-01, -6.7983e-01],\n",
            "        [ 9.5492e-01, -6.2690e-01],\n",
            "        [-8.7103e-01, -1.0290e-01],\n",
            "        [-4.3798e-01, -1.4414e+00],\n",
            "        [-1.2239e-01,  2.6680e-01],\n",
            "        [-1.0392e+00,  3.7904e-01],\n",
            "        [-2.5115e-01,  4.0323e-01],\n",
            "        [-4.1389e-02, -1.3028e-01],\n",
            "        [-3.3794e-01, -2.7687e-01],\n",
            "        [ 2.5543e-02,  5.0218e-01],\n",
            "        [-1.5173e-01,  3.2873e-01],\n",
            "        [ 2.7195e-01,  9.0592e-01],\n",
            "        [-6.2265e-02,  3.5987e-01],\n",
            "        [ 1.2424e-01, -1.1148e-01],\n",
            "        [-2.3483e-01, -2.8483e-02],\n",
            "        [-7.2831e-02,  2.8192e-03],\n",
            "        [-1.7864e-01,  1.2820e-02],\n",
            "        [ 5.2867e-01,  3.7354e-01],\n",
            "        [-1.0051e-01, -4.3867e-01],\n",
            "        [-6.3887e-01, -1.7712e-01],\n",
            "        [ 2.2517e-01,  5.6493e-01],\n",
            "        [-2.1887e-01, -1.4273e-01],\n",
            "        [-3.0854e-02, -9.0522e-02],\n",
            "        [-5.1037e-01,  2.2410e-01],\n",
            "        [ 1.0072e+00, -5.3317e-01],\n",
            "        [-6.3752e-01, -5.9672e-01],\n",
            "        [ 1.8549e-01,  3.0068e-01],\n",
            "        [ 4.4538e-01, -1.0639e+00],\n",
            "        [-7.0659e-01, -1.8145e-01],\n",
            "        [ 9.2111e-01,  6.0927e-01],\n",
            "        [-7.4557e-01, -3.6409e-01],\n",
            "        [ 1.5885e-01,  4.9646e-01],\n",
            "        [-3.5810e-01,  1.5781e-01],\n",
            "        [-4.2957e-01, -6.7803e-01],\n",
            "        [ 5.8293e-02, -2.0036e-01],\n",
            "        [-4.1726e-01, -9.4058e-02],\n",
            "        [ 6.7740e-01,  8.7325e-01],\n",
            "        [ 3.1305e-02,  2.6997e-01],\n",
            "        [-6.3693e-02,  6.4695e-01],\n",
            "        [-1.0635e-02, -2.2389e-01],\n",
            "        [ 7.1145e-02, -5.4936e-01],\n",
            "        [ 2.5075e+00, -1.8988e+00],\n",
            "        [ 5.5566e-01, -7.2031e-02],\n",
            "        [ 5.0376e-01,  1.1653e-01],\n",
            "        [-1.5019e-01, -2.9443e-01],\n",
            "        [-5.6912e-01,  7.4012e-01],\n",
            "        [ 9.5302e-02, -2.0549e-01],\n",
            "        [ 3.7089e-01, -1.2085e-01],\n",
            "        [-1.4945e-01,  4.4305e-01],\n",
            "        [-8.4117e-01, -5.3835e-01],\n",
            "        [-7.6749e-01,  3.2238e-01],\n",
            "        [ 2.7908e-02, -2.6775e-01],\n",
            "        [-4.0527e-02,  9.0429e-02],\n",
            "        [ 6.7421e-01,  1.9513e-01],\n",
            "        [ 7.0946e-02, -5.5858e-01],\n",
            "        [-8.3831e-01, -1.5869e-01],\n",
            "        [-1.3238e-01, -3.4854e-01],\n",
            "        [ 4.9000e+00, -5.5077e-01],\n",
            "        [ 2.4439e-01, -7.7987e-01],\n",
            "        [ 1.2458e-01,  4.0316e-02],\n",
            "        [-2.5128e-01,  1.2414e-01],\n",
            "        [-3.1997e-02,  5.2709e-01],\n",
            "        [ 8.2830e-01,  4.1348e-01],\n",
            "        [-1.5625e-01,  9.5868e-01],\n",
            "        [-1.9630e-01, -3.3837e-01],\n",
            "        [ 6.9363e-01, -7.8527e-03],\n",
            "        [-2.4549e-01,  2.2514e-01],\n",
            "        [-2.6749e-01, -5.9652e-01],\n",
            "        [-5.0712e-01, -4.0353e-01],\n",
            "        [ 3.9170e-01,  6.5725e-01],\n",
            "        [ 1.0819e-01,  1.1719e-01],\n",
            "        [-6.8372e-01, -3.8394e-01],\n",
            "        [-1.7341e-02,  9.8445e-01],\n",
            "        [ 6.4173e-01,  3.7666e-01],\n",
            "        [-3.2463e-01, -3.6851e-03],\n",
            "        [-1.4179e-01,  5.1305e-01],\n",
            "        [ 2.6202e-01,  1.2292e-01],\n",
            "        [ 4.8530e-01, -6.9473e-01],\n",
            "        [-3.2834e-01,  4.7303e-01],\n",
            "        [-1.7038e-01,  3.8345e-02],\n",
            "        [-5.6396e-01,  2.5388e-01],\n",
            "        [-1.4072e-01, -1.1410e+00],\n",
            "        [ 2.6337e-01,  1.1986e-01],\n",
            "        [ 1.4357e-01,  3.6672e-01],\n",
            "        [ 1.0983e+00,  2.4384e+00],\n",
            "        [-2.1336e-01, -2.9617e-01],\n",
            "        [-5.1415e-02,  2.1137e-01],\n",
            "        [ 1.4559e-02, -8.0972e-01],\n",
            "        [-6.3982e-02, -2.1059e-01],\n",
            "        [ 1.9690e+00,  7.8066e-01],\n",
            "        [ 3.2738e-01, -6.0587e-01],\n",
            "        [ 5.4847e-01, -4.3004e-02],\n",
            "        [-1.0930e+00, -2.8505e-01],\n",
            "        [ 2.7607e-01,  1.4617e-01],\n",
            "        [ 3.1758e-01,  2.3331e-01],\n",
            "        [ 1.3815e-01,  1.0235e-01],\n",
            "        [-2.8879e-01, -2.0420e-01],\n",
            "        [-2.9102e-01,  7.1894e-01],\n",
            "        [ 3.6835e-01, -8.0442e-01],\n",
            "        [-2.0168e-01,  7.0397e-01],\n",
            "        [-5.4991e-01,  2.8138e-01],\n",
            "        [-9.1807e-01,  1.7518e-01],\n",
            "        [ 3.5007e-01, -8.5850e-02],\n",
            "        [-5.0179e-01,  1.9953e-01],\n",
            "        [-2.1821e-01,  5.2461e-01],\n",
            "        [ 8.5372e-01,  1.4351e-02],\n",
            "        [ 3.5093e-01,  1.2432e-01],\n",
            "        [-3.7707e-01,  2.8803e-01],\n",
            "        [ 6.3009e-01, -4.8169e-01],\n",
            "        [-1.6831e-01,  7.6905e-02],\n",
            "        [ 4.9220e-01,  9.3430e-01],\n",
            "        [-2.9233e-01, -1.8717e-01],\n",
            "        [-2.5290e-01, -4.1330e-01],\n",
            "        [ 3.7972e-01,  3.4608e-01],\n",
            "        [ 1.1043e+00, -1.5268e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
            "        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
            "        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
            "        0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "tensor([[-5.3213e-01,  7.1233e-01],\n",
            "        [-1.1979e-01,  1.6363e-01],\n",
            "        [-6.9673e-01,  7.8800e-01],\n",
            "        [ 1.3718e+00,  3.1652e-01],\n",
            "        [ 9.0163e-02, -1.2066e+00],\n",
            "        [ 4.2809e-02, -3.0718e-01],\n",
            "        [-8.6175e-01,  3.0803e-02],\n",
            "        [-4.8170e-01, -7.0023e-01],\n",
            "        [-5.6704e-02, -1.2022e-01],\n",
            "        [ 5.7274e-01,  2.4520e-02],\n",
            "        [ 6.6020e-02, -4.0701e-02],\n",
            "        [-5.1588e-01,  4.0204e-01],\n",
            "        [ 3.1610e-01,  7.0461e-02],\n",
            "        [-3.9900e-02,  5.3657e-02],\n",
            "        [ 2.3565e-01, -2.7706e-01],\n",
            "        [ 9.8121e-01, -8.6549e-02],\n",
            "        [-2.2968e-01, -2.1596e-01],\n",
            "        [-5.0801e-01,  6.8541e-01],\n",
            "        [-1.2094e+00, -9.5377e-01],\n",
            "        [-1.8983e-01,  8.3611e-01],\n",
            "        [-1.9900e-01,  2.6190e-01],\n",
            "        [-7.5879e-02, -3.0731e-01],\n",
            "        [ 5.5418e-01, -3.9939e-01],\n",
            "        [ 1.5990e-01, -7.0167e-01],\n",
            "        [ 2.4154e-01, -1.5558e-01],\n",
            "        [ 1.3734e-01,  5.0743e-01],\n",
            "        [ 1.4662e-01,  2.0733e-03],\n",
            "        [-2.7225e-01,  3.1921e-01],\n",
            "        [-2.3946e-01, -2.1300e-01],\n",
            "        [-1.2883e-01, -8.1003e-02],\n",
            "        [ 7.5989e-03, -5.9492e-01],\n",
            "        [ 3.4642e-02,  4.0338e-01],\n",
            "        [ 1.4325e+00,  1.0038e-01],\n",
            "        [-3.9305e-01, -2.0412e-02],\n",
            "        [-1.1138e+00,  2.0750e-01],\n",
            "        [-2.7538e-01, -1.4908e-01],\n",
            "        [-1.2220e-01, -7.7616e-02],\n",
            "        [-1.7431e-01,  3.0649e-01],\n",
            "        [-2.5826e-01,  3.1682e-01],\n",
            "        [-2.1520e-01, -2.6876e-02],\n",
            "        [-3.1302e-01, -1.1172e-01],\n",
            "        [-6.6493e-01,  6.2201e-01],\n",
            "        [ 1.9774e-01,  3.1831e-01],\n",
            "        [ 3.1728e-01, -2.2725e-01],\n",
            "        [ 1.2690e-01, -5.0409e-01],\n",
            "        [ 1.7567e+00, -2.2689e+00],\n",
            "        [-4.3803e-01,  4.3264e-02],\n",
            "        [-5.7490e-01,  3.1583e-03],\n",
            "        [-6.5391e-02,  1.3912e-01],\n",
            "        [ 1.8963e-01, -2.0122e+00],\n",
            "        [ 2.5301e-01,  2.5637e-01],\n",
            "        [-5.1462e-01,  5.2446e-01],\n",
            "        [ 4.6203e-01,  1.4674e-01],\n",
            "        [ 8.4500e-01,  2.8249e-01],\n",
            "        [ 3.9814e-01,  2.0912e-01],\n",
            "        [ 8.2606e-01, -3.4206e-01],\n",
            "        [-8.4178e-02, -8.5603e-01],\n",
            "        [-6.4106e-01, -1.6395e+00],\n",
            "        [ 1.4682e+00, -1.2581e-01],\n",
            "        [-2.4636e-01,  8.2758e-01],\n",
            "        [-3.8729e-01,  1.4564e-01],\n",
            "        [ 1.1903e-01,  1.1856e-01],\n",
            "        [ 6.3724e-01, -1.0478e-01],\n",
            "        [-4.6363e-03, -1.7355e-01],\n",
            "        [ 4.4814e-01, -1.2882e-01],\n",
            "        [ 6.5025e-01,  6.1616e-01],\n",
            "        [ 2.8073e-01,  2.7006e-01],\n",
            "        [ 2.4700e-01,  1.3990e+00],\n",
            "        [-4.4667e-01,  2.7597e-02],\n",
            "        [-2.1779e-01, -1.3955e-01],\n",
            "        [ 5.2555e-01, -9.3094e-02],\n",
            "        [-6.1326e-01, -8.9160e-01],\n",
            "        [-1.2799e-01, -1.2880e-01],\n",
            "        [-9.0533e-01, -1.0633e+00],\n",
            "        [ 5.4649e-01, -1.0318e-01],\n",
            "        [ 8.1734e-01,  1.1104e+00],\n",
            "        [-4.6194e-02,  2.0044e-01],\n",
            "        [ 2.9247e-01,  9.2697e-01],\n",
            "        [-2.9522e-01, -4.2899e-01],\n",
            "        [ 3.1301e-02,  7.8785e-01],\n",
            "        [ 6.0585e-01, -7.1453e-01],\n",
            "        [-6.6278e-01,  2.8601e-01],\n",
            "        [ 2.0414e-01,  6.0469e-01],\n",
            "        [ 4.0047e-01, -5.7773e-01],\n",
            "        [-4.3280e-01,  4.5624e-01],\n",
            "        [-1.0087e+00,  6.3284e-01],\n",
            "        [-5.5763e-01,  1.0813e-01],\n",
            "        [ 5.0729e-01, -1.1862e+00],\n",
            "        [-5.9553e-01, -3.1695e-01],\n",
            "        [ 7.2445e-01, -2.1880e-01],\n",
            "        [-6.9407e-02, -9.5405e-01],\n",
            "        [ 6.7335e-02,  2.2631e-01],\n",
            "        [-6.0974e-01,  2.6644e-01],\n",
            "        [-1.5491e+00,  7.3158e-01],\n",
            "        [-3.0803e-01,  4.6995e-02],\n",
            "        [ 3.0429e-01,  4.9530e-01],\n",
            "        [ 2.9481e-01, -5.6512e-02],\n",
            "        [ 1.0084e-01, -3.2617e-01],\n",
            "        [-2.9460e-01, -2.8560e-01],\n",
            "        [-1.9458e-01,  8.0513e-01],\n",
            "        [ 1.1014e+00, -1.3754e-01],\n",
            "        [-3.7738e-01,  6.7302e-01],\n",
            "        [-2.4702e-01,  2.2870e+00],\n",
            "        [-1.9263e-01,  6.3722e-02],\n",
            "        [-3.2797e-01,  5.8222e-02],\n",
            "        [ 6.0843e-01, -1.6291e+00],\n",
            "        [-4.6126e-01, -3.0548e-01],\n",
            "        [-3.3616e-01,  1.6301e-02],\n",
            "        [-1.3887e-02,  1.5997e-01],\n",
            "        [-1.5917e-01, -1.8963e-01],\n",
            "        [ 7.6408e-02,  6.4954e-01],\n",
            "        [ 1.3582e+00, -1.8921e+00],\n",
            "        [ 2.2485e-01, -1.3131e+00],\n",
            "        [-3.7365e-01, -1.1322e-01],\n",
            "        [ 4.0278e+00,  3.4665e+00],\n",
            "        [-2.2819e-01, -1.1537e-01],\n",
            "        [-4.0663e-01,  8.7141e-02],\n",
            "        [ 1.3067e-01, -2.3655e-01],\n",
            "        [-3.9074e-01,  5.1249e-01],\n",
            "        [ 5.6977e-01, -2.8218e-01],\n",
            "        [ 1.7926e-03,  7.9493e-01],\n",
            "        [ 4.9857e-01, -3.9015e-01],\n",
            "        [-3.6997e-01, -1.2858e+00],\n",
            "        [ 1.0434e-01,  1.0877e+00],\n",
            "        [ 2.7328e-02, -6.8702e-01],\n",
            "        [-3.1153e-01,  6.7814e-01],\n",
            "        [-5.9283e-01,  2.6350e-01],\n",
            "        [ 5.2751e-01, -7.1194e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
            "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "tensor([[ 6.2582e-02, -1.8544e+00],\n",
            "        [-1.0372e+00, -3.8985e-01],\n",
            "        [ 2.2119e-01,  6.9096e-01],\n",
            "        [ 1.8914e-01, -1.0111e-01],\n",
            "        [-2.9198e-01, -3.0677e-01],\n",
            "        [-3.0673e-01,  3.0668e-01],\n",
            "        [-2.5326e-01, -8.1873e-02],\n",
            "        [ 5.0830e-01,  1.4369e-01],\n",
            "        [ 2.6602e-01, -4.6768e-01],\n",
            "        [ 5.9395e-02,  4.0353e-01],\n",
            "        [ 3.3483e-02,  9.6888e-02],\n",
            "        [-2.4993e-01, -1.4812e-01],\n",
            "        [ 1.0027e-01,  2.3766e-01],\n",
            "        [ 1.4342e+00,  7.8987e-01],\n",
            "        [-1.0534e-01, -7.8613e-01],\n",
            "        [-7.6358e-01, -5.6630e-01],\n",
            "        [ 1.2958e+00, -1.6163e+00],\n",
            "        [ 4.8941e-01, -9.9754e-02],\n",
            "        [-2.1484e-01,  2.7022e-01],\n",
            "        [ 1.0649e-01, -6.9112e-02],\n",
            "        [ 1.9889e-01,  5.3656e-01],\n",
            "        [-1.2585e-01,  2.2070e-04],\n",
            "        [ 3.0542e-01,  1.3012e-01],\n",
            "        [ 8.1003e-01,  7.3424e-01],\n",
            "        [ 5.0467e-01, -2.2327e-01],\n",
            "        [ 7.6174e-02, -3.8417e-01],\n",
            "        [-8.9042e-02,  5.7282e-01],\n",
            "        [ 7.6730e-02,  3.0999e-01],\n",
            "        [ 2.5792e-01, -2.6571e-02],\n",
            "        [-3.4747e-02, -9.7907e-01],\n",
            "        [-3.4090e-01,  5.1546e-01],\n",
            "        [ 2.6957e-01,  8.5843e-01],\n",
            "        [ 9.2287e-01,  4.6161e-01],\n",
            "        [-6.4987e-03,  1.3581e-01],\n",
            "        [ 6.7860e-01,  6.3299e-01],\n",
            "        [ 4.8626e-01,  1.2492e-01],\n",
            "        [ 4.2105e-01,  6.5294e-02],\n",
            "        [ 1.7460e-01,  1.2160e-01],\n",
            "        [ 7.5021e-03,  4.4931e-01],\n",
            "        [-3.8648e-01,  3.3260e-01],\n",
            "        [ 3.7406e-01, -1.3363e+00],\n",
            "        [ 1.2866e-01, -4.0754e-01],\n",
            "        [ 2.5916e-01, -1.7456e+00],\n",
            "        [ 8.0111e-01,  2.8985e-01],\n",
            "        [ 3.1487e-01,  4.3926e-01],\n",
            "        [ 1.0136e+00, -1.5004e+00],\n",
            "        [-2.9705e-02, -4.6669e-01],\n",
            "        [ 8.8855e-03,  1.1084e+00],\n",
            "        [ 6.2472e-03, -2.1116e-01],\n",
            "        [ 6.6366e-01, -1.0271e-01],\n",
            "        [ 2.4635e-01, -2.6735e-02],\n",
            "        [-3.4057e-02,  7.0966e-01],\n",
            "        [-1.1925e+00,  1.4662e-02],\n",
            "        [ 1.9067e-02, -3.1365e-01],\n",
            "        [ 6.2046e-01, -1.6966e+00],\n",
            "        [-1.1720e-01,  1.9565e-02],\n",
            "        [ 1.1262e-01,  9.9019e-02],\n",
            "        [-3.3150e-01,  1.0551e-01],\n",
            "        [-6.9074e-01,  1.1923e+00],\n",
            "        [ 7.7061e-01,  1.3466e-01],\n",
            "        [ 5.9954e-01, -2.0290e+00],\n",
            "        [-6.0561e-02, -3.3724e-01],\n",
            "        [-8.7790e-01,  2.4143e-01],\n",
            "        [ 6.6291e-01,  5.7736e-01],\n",
            "        [ 3.6481e-01,  1.8542e-01],\n",
            "        [-3.9212e-01, -1.5271e-01],\n",
            "        [ 8.7300e-01, -1.3714e-01],\n",
            "        [-1.0397e-01,  2.8927e-01],\n",
            "        [ 6.1757e-01, -1.0355e-01],\n",
            "        [-3.4124e-01, -1.1078e-01],\n",
            "        [ 1.6035e-01,  4.9748e-01],\n",
            "        [ 4.4764e-02,  2.8863e-01],\n",
            "        [ 2.5229e-01, -6.0945e-01],\n",
            "        [ 5.8926e-01, -8.3716e-01],\n",
            "        [-3.4506e-01,  2.9020e-01],\n",
            "        [-6.6771e-01, -1.0262e+00],\n",
            "        [-2.2835e-01, -2.1945e-01],\n",
            "        [-5.1286e-01,  2.0362e-01],\n",
            "        [-4.2145e-01, -1.5818e+00],\n",
            "        [-6.1483e-01,  4.8506e-01],\n",
            "        [-8.8885e-02, -1.4527e-01],\n",
            "        [ 1.8532e-01,  1.0663e-01],\n",
            "        [ 3.4319e-02,  3.7712e-01],\n",
            "        [-3.7992e-01,  1.1920e-01],\n",
            "        [-3.6884e-01,  3.5649e-01],\n",
            "        [ 3.9650e-01, -5.0215e-01],\n",
            "        [ 5.4927e-01,  3.6611e-03],\n",
            "        [ 1.0528e+00,  1.6314e+00],\n",
            "        [-3.4646e-01,  3.3860e-01],\n",
            "        [ 2.5675e-01,  7.4460e-02],\n",
            "        [-1.0544e-01,  2.3604e-01],\n",
            "        [-1.5513e-02, -8.8732e-01],\n",
            "        [ 9.4682e-01, -7.7588e-01],\n",
            "        [ 4.5049e-01,  1.5378e-01],\n",
            "        [ 6.5233e-02, -2.2456e-01],\n",
            "        [ 3.8348e-01,  1.3166e-01],\n",
            "        [-2.8303e-01,  1.1633e+00],\n",
            "        [-1.2059e-01, -2.1332e-01],\n",
            "        [-5.5849e-01,  4.2437e-01],\n",
            "        [-3.9397e-01,  8.9633e-02],\n",
            "        [ 2.7527e-01,  8.4352e-01],\n",
            "        [ 1.4179e-01,  6.4872e-01],\n",
            "        [-3.9875e-01,  4.2793e-02],\n",
            "        [ 6.8663e-01,  1.1391e-01],\n",
            "        [-3.9961e-01,  3.0486e-01],\n",
            "        [ 1.0949e-02, -6.4825e-02],\n",
            "        [-1.8322e-01, -4.6254e-01],\n",
            "        [ 3.8054e-01, -6.8179e-01],\n",
            "        [-8.7092e-01,  5.9989e-01],\n",
            "        [-4.3582e-01,  1.1232e-01],\n",
            "        [ 2.1216e+00,  3.2024e-01],\n",
            "        [-2.0094e-01,  1.0646e-01],\n",
            "        [ 7.2624e-01, -1.6918e+00],\n",
            "        [-8.9687e-02,  3.2067e-01],\n",
            "        [ 1.6532e-01, -7.3857e-01],\n",
            "        [ 6.1706e-01, -1.1193e-01],\n",
            "        [-4.0282e-01, -8.3610e-02],\n",
            "        [-1.0108e+00,  7.9143e-01],\n",
            "        [ 1.8730e-01,  6.0487e-02],\n",
            "        [ 2.4051e+00,  2.8911e+00],\n",
            "        [ 1.1480e+00, -3.5422e-02],\n",
            "        [-1.6374e-01, -3.6327e-01],\n",
            "        [-2.6771e-01,  3.7274e-01],\n",
            "        [ 2.5813e-01, -1.9195e-01],\n",
            "        [-2.5363e-01,  2.2617e-01],\n",
            "        [-6.1130e-01,  3.8728e-01],\n",
            "        [-9.2915e-01, -1.2019e+00],\n",
            "        [ 2.2736e-01, -1.6514e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-9c3c365c8f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Overall accuracy: {accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-80a2040bb040>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_loader, test_loader, device, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch {e}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-083524729aaa>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(net, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0min_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-9151d6b7923e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# output: batch_size, sequence_length, hidden_size * 2 (since is bilstm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Interested only in the last layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 773\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "accuracy = main(train_loader, test_loader, max_element, device = \"cuda\", epochs = 50)\n",
        "\n",
        "print(f\"Overall accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pBBa_vvYSnL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "tensor([1315, 1222, 1011, 1010,  936,  862,  814,  807,  807,  764,  718,  515,\n",
        "         495,  388,  344,  323])\n",
        "tensor([1617, 1361, 1311, 1178, 1081, 1068,  958,  941,  925,  768,  688,  619,\n",
        "         604,  573,  484,  405])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0Af9ORg9pFKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P67fhfYpSZA"
      },
      "source": [
        "# First try to parse phrases documet-wise, then try to parse each phrase of a document separately, and then aggregate the result (if there are more positive phrases then positive, otherwise negative). (Try also to give a weight depending on the number of sentiment lexemes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8-04GhL4p89"
      },
      "source": [
        "### Training procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBD3vt1v4r65"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM_dyKuc4sww"
      },
      "source": [
        "### Main function containing also cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mK4Rmfr4ziT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA1Jgp7d_MgC"
      },
      "source": [
        "### (Possible improvement, apply UDA to GLOVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8toXOrs_T2C"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "bMm0vFo00oLS",
        "outputId": "0c6e196d-48e3-4c2b-b5b1-3e1337ae226d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-253956998aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandford\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandfordTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandfordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk.tokenize.standford'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize.stanford import StanfordTokenizer\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class VectorizerPipeline():\n",
        "\n",
        "  def __init__(self, corpus, pipe= {\"tokenizer\": \"stanford\", \"embedding\": \"glove\"}, embedding_size: int = 300):\n",
        "    self.corpus = corpus\n",
        "    if embedding_size:\n",
        "      self.embedding_size = embedding_size\n",
        "    else:\n",
        "      self.embedding_size = 300\n",
        "    \n",
        "    self._allowed = {\n",
        "        \"tokenizer\": [\"stanford\"],\n",
        "        \"embedding\": [\"glove\"],\n",
        "        \"lemmatizer\": [],\n",
        "        \"stop-word-removal\": [],\n",
        "    }\n",
        "    self.pipe = {\n",
        "        \"tokenizer\": None,\n",
        "        \"embedding\": None,\n",
        "        \"lemmatizer\": None,\n",
        "        \"stop-word-removal\": None,\n",
        "    }\n",
        "    if pipe:\n",
        "      for key, value in pipe.items():\n",
        "        try:\n",
        "          if pipe[key] in self._allowed[key]:\n",
        "            self.pipe[key] = value\n",
        "          else:\n",
        "            raise ValueError(f\"Invalid type of {key}. \\n Valid {key}s are {self._allowed[key]}\")\n",
        "        except KeyError:\n",
        "          raise KeyError(f\"Invalid step in the pipeline: {key}. \\n valid steps are {list(self._allowed.keys())}\")\n",
        "\n",
        "\n",
        "  def tokenization(self, batch):\n",
        "    tok = StanfordTokenizer()\n",
        "    X = [tok(x) for x in batch]\n",
        "    return X\n",
        "  \n",
        "  def embedding(self, batch):\n",
        "    max_length = max(batch, key=len)\n",
        "\n",
        "  def vectorize(self, batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqmjnrCt_U2K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "c176b223-e9f6-4af2-9a1c-541577ccb9ad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4cce880bcee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovieReviewsCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovieReviewsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovie_reviews_dataset_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c6998c471427>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# list of documents, each document is a list containing words of that document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munprocessed_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_corpus_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c6998c471427>\u001b[0m in \u001b[0;36m_flatten\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c6998c471427>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# Get everything we can from this piece.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tok\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# Update the offset table.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    308\u001b[0m                 \u001b[0;34m\"block reader %s() should return list or tuple.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36m_read_para_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 [\n\u001b[1;32m    136\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sent_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 ]\n\u001b[1;32m    139\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \"\"\"\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \"\"\"\n\u001b[1;32m   1420\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"next_tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \"\"\"\n\u001b[1;32m   1441\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "corpus = MovieReviewsCorpus()\n",
        "\n",
        "dataset = MovieReviewsDataset(corpus.movie_reviews_dataset_raw())\n",
        "\n",
        "pipeline = VectorizerPipeline(corpus)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size = 64, collate_fn = pipeline.vectorize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO3lxc0l3sxI"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from tqdm import tqdm\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "\n",
        "corpus = MovieReviewsCorpus()\n",
        "\n",
        "global_vectors = GloVe(name='840B', dim=300)\n",
        "\n",
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    null_embedding = torch.tensor([0.0]*300)\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "          if torch.equal(embeddings_index.get_vecs_by_tokens(word), null_embedding):\n",
        "            raise KeyError\n",
        "          a[word] = embeddings_index.get_vecs_by_tokens(word)\n",
        "          k += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print()\n",
        "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
        "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x\n",
        "\n",
        "\n",
        "oov = check_coverage(corpus.vocab, global_vectors)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONy7Qcn9xk5qUaFSGW+/IA",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}