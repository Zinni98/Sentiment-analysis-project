{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinni98/Sentiment-analysis-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NBg5KtW5AMy"
      },
      "source": [
        "## Polarity Classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8msH_nw3rf39",
        "outputId": "1fcd8791-aed8-4435-a076-cea0c625a34b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gdrive/My Drive/nlu-project\")"
      ],
      "metadata": {
        "id": "vdSs1gS1rgh_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvHgFZJe4_Xq",
        "outputId": "512f51a4-6bf5-4dac-8e39-7b788e372532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "import torch\n",
        "from nltk.corpus import movie_reviews\n",
        "import numpy as np\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"movie_reviews\")\n",
        "nltk.download(\"subjectivity\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"sentiwordnet\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fglEQLVLtc9C"
      },
      "source": [
        "## Exploratory analysis\n",
        "\n",
        "Firstly let's explore the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dKetqTKrZFE"
      },
      "outputs": [],
      "source": [
        "mr = movie_reviews\n",
        "neg = mr.paras(categories = \"neg\")\n",
        "pos = mr.paras(categories = \"pos\")\n",
        "print(f\"length of each part of the dataset:\\n - pos: {len(pos)} \\n - neg: {len(neg)}\")\n",
        "print(pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Y4wuDNt7XW"
      },
      "source": [
        "It's easy to see that data comes in the following format:\n",
        "\n",
        "- pos = [doc1, doc2, ..., doc1000] (the same applies for negative sentiment examples)\n",
        "\n",
        "Where each doc has the following structure:\n",
        "\n",
        "- doc1 = [sentence_1, sentence_2, ..., sentence_k]\n",
        "\n",
        "Each sentence is a list of tokens, so the dataset is already tokenized.\n",
        "\n",
        "### Word embedding\n",
        "Since I'm going to use deep learning models, I'm going to choose a word embedding to transform the text into vectors.\n",
        "I'm going to start with a pretrained version of GloVe word embedding.\n",
        "Since is a pre-trained word embedding (hence basically a lookup table), I'm going to check how many words of the vocabulary are covered by the pretrained word embedding model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_SEn4EFxKTd"
      },
      "outputs": [],
      "source": [
        "def create_vocab(corpus_words):\n",
        "    vocab = dict()\n",
        "    for word in corpus_words:\n",
        "      try:\n",
        "        vocab[word] += 1\n",
        "      except:\n",
        "        vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "def get_corpus_words(corpus) -> list:\n",
        "    return [w for doc in corpus for sent in doc for w in sent]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdofBhKWxQHx"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from tqdm import tqdm\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "\n",
        "global_vectors = GloVe(name='840B', dim=300)\n",
        "\n",
        "# function inspired by https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings/notebook\n",
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    null_embedding = torch.tensor([0.0]*300)\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "          if torch.equal(embeddings_index.get_vecs_by_tokens(word), null_embedding):\n",
        "            raise KeyError\n",
        "          a[word] = embeddings_index.get_vecs_by_tokens(word)\n",
        "          k += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print()\n",
        "    print(f'Found embeddings for {len(a) / len(vocab):.2%} of vocab')\n",
        "    print(f'Found embeddings for  {k / (k + i):.2%} of all text')\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x\n",
        "\n",
        "vocab = create_vocab(get_corpus_words(pos + neg))\n",
        "oov = check_coverage(vocab, global_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K0HUOB2-E1V"
      },
      "outputs": [],
      "source": [
        "oov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEjIRmuG-Quz"
      },
      "source": [
        "I'm going to see which are the words that are not covered by the embedding (Out Of Vocabulary words), so I can try to see if there are some tenchniques that can be applied in order to improve coverage.\n",
        "The majority of OOV words aren't related with a praticular sentiment (they are basically nouns or some type punctuation), so they can be safely removed. That happens because unknown words are encoded as $[0] * embedding.length$, so no useful information is added.\n",
        "Others OOV words are regular words surrounded by underscores, so they are not recognized by the fixed word embedding. To avoid this problem I implemented a procedure in order to clean these words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgvlBpnMB8EO"
      },
      "outputs": [],
      "source": [
        "def remove_underscores(corpus):\n",
        "  for doc in corpus:\n",
        "    for sent in doc:\n",
        "      for idx, word in enumerate(sent):\n",
        "        if \"_\" in word:\n",
        "          cleaned_word = _clean_word(word)\n",
        "          sent[idx] = cleaned_word\n",
        "  return corpus\n",
        "\n",
        "\n",
        "def _clean_word(word: str):\n",
        "  word = word.replace(\"_\", \" \")\n",
        "  word = word.split()\n",
        "  word = \" \".join(word)\n",
        "  return word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUphYnDOa785",
        "outputId": "e0b314d1-9afc-4056-bcd1-d731c2fb907a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 39519/39519 [00:01<00:00, 28083.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Found embeddings for 92.48% of vocab\n",
            "Found embeddings for  99.61% of all text\n"
          ]
        }
      ],
      "source": [
        "corpus = pos + neg\n",
        "clean_corpus = remove_underscores(corpus), oov\n",
        "vocab = create_vocab(get_corpus_words(clean_corpus))\n",
        "oov = check_coverage(vocab, global_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cmath import phase\n",
        "from dis import findlabels\n",
        "from unicodedata import name\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "\n",
        "CONTRACTION_MAP =  {\"ain't\": \"is not\",\n",
        "                        \"aren't\": \"are not\",\n",
        "                        \"can't\": \"cannot\",\n",
        "                        \"can't've\": \"cannot have\",\n",
        "                        \"'cause\": \"because\",\n",
        "                        \"could've\": \"could have\",\n",
        "                        \"couldn't\": \"could not\",\n",
        "                        \"couldn't've\": \"could not have\",\n",
        "                        \"didn't\": \"did not\",\n",
        "                        \"doesn't\": \"does not\",\n",
        "                        \"don't\": \"do not\",\n",
        "                        \"hadn't\": \"had not\",\n",
        "                        \"hadn't've\": \"had not have\",\n",
        "                        \"hasn't\": \"has not\",\n",
        "                        \"haven't\": \"have not\",\n",
        "                        \"he'd\": \"he would\",\n",
        "                        \"he'd've\": \"he would have\",\n",
        "                        \"he'll\": \"he will\",\n",
        "                        \"he'll've\": \"he he will have\",\n",
        "                        \"he's\": \"he is\",\n",
        "                        \"how'd\": \"how did\",\n",
        "                        \"how'd'y\": \"how do you\",\n",
        "                        \"how'll\": \"how will\",\n",
        "                        \"how's\": \"how is\",\n",
        "                        \"i'd\": \"i would\",\n",
        "                        \"i'd've\": \"i would have\",\n",
        "                        \"i'll\": \"i will\",\n",
        "                        \"i'll've\": \"i will have\",\n",
        "                        \"i'm\": \"i am\",\n",
        "                        \"i've\": \"i have\",\n",
        "                        \"isn't\": \"is not\",\n",
        "                        \"it'd\": \"it would\",\n",
        "                        \"it'd've\": \"it would have\",\n",
        "                        \"it'll\": \"it will\",\n",
        "                        \"it'll've\": \"it will have\",\n",
        "                        \"it's\": \"it is\",\n",
        "                        \"let's\": \"let us\",\n",
        "                        \"ma'am\": \"madam\",\n",
        "                        \"mayn't\": \"may not\",\n",
        "                        \"might've\": \"might have\",\n",
        "                        \"mightn't\": \"might not\",\n",
        "                        \"mightn't've\": \"might not have\",\n",
        "                        \"must've\": \"must have\",\n",
        "                        \"mustn't\": \"must not\",\n",
        "                        \"mustn't've\": \"must not have\",\n",
        "                        \"needn't\": \"need not\",\n",
        "                        \"needn't've\": \"need not have\",\n",
        "                        \"o'clock\": \"of the clock\",\n",
        "                        \"oughtn't\": \"ought not\",\n",
        "                        \"oughtn't've\": \"ought not have\",\n",
        "                        \"shan't\": \"shall not\",\n",
        "                        \"sha'n't\": \"shall not\",\n",
        "                        \"shan't've\": \"shall not have\",\n",
        "                        \"she'd\": \"she would\",\n",
        "                        \"she'd've\": \"she would have\",\n",
        "                        \"she'll\": \"she will\",\n",
        "                        \"she'll've\": \"she will have\",\n",
        "                        \"she's\": \"she is\",\n",
        "                        \"should've\": \"should have\",\n",
        "                        \"shouldn't\": \"should not\",\n",
        "                        \"shouldn't've\": \"should not have\",\n",
        "                        \"so've\": \"so have\",\n",
        "                        \"so's\": \"so as\",\n",
        "                        \"that'd\": \"that would\",\n",
        "                        \"that'd've\": \"that would have\",\n",
        "                        \"that's\": \"that is\",\n",
        "                        \"there'd\": \"there would\",\n",
        "                        \"there'd've\": \"there would have\",\n",
        "                        \"there's\": \"there is\",\n",
        "                        \"they'd\": \"they would\",\n",
        "                        \"they'd've\": \"they would have\",\n",
        "                        \"they'll\": \"they will\",\n",
        "                        \"they'll've\": \"they will have\",\n",
        "                        \"they're\": \"they are\",\n",
        "                        \"they've\": \"they have\",\n",
        "                        \"to've\": \"to have\",\n",
        "                        \"wasn't\": \"was not\",\n",
        "                        \"we'd\": \"we would\",\n",
        "                        \"we'd've\": \"we would have\",\n",
        "                        \"we'll\": \"we will\",\n",
        "                        \"we'll've\": \"we will have\",\n",
        "                        \"we're\": \"we are\",\n",
        "                        \"we've\": \"we have\",\n",
        "                        \"weren't\": \"were not\",\n",
        "                        \"what'll\": \"what will\",\n",
        "                        \"what'll've\": \"what will have\",\n",
        "                        \"what're\": \"what are\",\n",
        "                        \"what's\": \"what is\",\n",
        "                        \"what've\": \"what have\",\n",
        "                        \"when's\": \"when is\",\n",
        "                        \"when've\": \"when have\",\n",
        "                        \"where'd\": \"where did\",\n",
        "                        \"where's\": \"where is\",\n",
        "                        \"where've\": \"where have\",\n",
        "                        \"who'll\": \"who will\",\n",
        "                        \"who'll've\": \"who will have\",\n",
        "                        \"who's\": \"who is\",\n",
        "                        \"who've\": \"who have\",\n",
        "                        \"why's\": \"why is\",\n",
        "                        \"why've\": \"why have\",\n",
        "                        \"will've\": \"will have\",\n",
        "                        \"won't\": \"will not\",\n",
        "                        \"won't've\": \"will not have\",\n",
        "                        \"would've\": \"would have\",\n",
        "                        \"wouldn't\": \"would not\",\n",
        "                        \"wouldn't've\": \"would not have\",\n",
        "                        \"y'all\": \"you all\",\n",
        "                        \"y'all'd\": \"you all would\",\n",
        "                        \"y'all'd've\": \"you all would have\",\n",
        "                        \"y'all're\": \"you all are\",\n",
        "                        \"y'all've\": \"you all have\",\n",
        "                        \"you'd\": \"you would\",\n",
        "                        \"you'd've\": \"you would have\",\n",
        "                        \"you'll\": \"you will\",\n",
        "                        \"you'll've\": \"you will have\",\n",
        "                        \"you're\": \"you are\",\n",
        "                        \"you've\": \"you have\",\n",
        "                    }\n",
        "\n",
        "class MRAbstractPipeline():\n",
        "    def __init__(self):\n",
        "        self.pipeline = []\n",
        "    \n",
        "    def pipe(self, corpus):\n",
        "        for el in self.pipeline:\n",
        "            corpus = el(corpus)\n",
        "        return corpus\n",
        "    \n",
        "    def __call__(self, *args, **kwds):\n",
        "        if args[0] == None:\n",
        "            raise ValueError(\"Need a corpus as argument\")\n",
        "        corpus = args[0]\n",
        "        return self.pipe(corpus)\n",
        "        \n",
        "\n",
        "class MRPipelineTokens(MRAbstractPipeline):\n",
        "    \"\"\"\n",
        "    Pipeline for documents represented as list of tokens\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(MRPipelineTokens, self).__init__()\n",
        "        self.pipeline = [self.remove_underscores, \n",
        "                         self.reducing_character_repetitions,\n",
        "                         #self.merge_apostrophes,\n",
        "                         self.clean_contractions,\n",
        "                         self.clean_special_chars,\n",
        "                         self.remove_stop_words]\n",
        "\n",
        "    def remove_underscores(self, corpus):\n",
        "        \"\"\"\n",
        "        Solves the problem where some of the words are surrounded by underscores\n",
        "        (e.g. \"_hello_\")\n",
        "        \"\"\"\n",
        "        for doc in corpus:\n",
        "            for idx, word in enumerate(doc):\n",
        "                if \"_\" in word:\n",
        "                    cleaned_word = self._clean_word(word)\n",
        "                    doc[idx] = cleaned_word\n",
        "        return corpus\n",
        "\n",
        "\n",
        "    def _clean_word(self, word: str):\n",
        "        word = word.replace(\"_\", \" \")\n",
        "        # remove spaces before and after the word\n",
        "        word = word.split()\n",
        "        word = \" \".join(word)\n",
        "        return word\n",
        "    \n",
        "    def reducing_character_repetitions(self, corpus):\n",
        "        \n",
        "        new_corpus = []\n",
        "        for doc in corpus:\n",
        "            new_doc = [self._clean_repetitions(w) for w in doc]\n",
        "            new_corpus.append(new_doc)\n",
        "        return new_corpus\n",
        "\n",
        "    # inspired by https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a\n",
        "    def _clean_repetitions(self, word):\n",
        "        \"\"\"\n",
        "        This Function will reduce repetition to two characters \n",
        "        for alphabets and to one character for punctuations.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            word: str                \n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            Finally formatted text with alphabets repeating to \n",
        "            one characters & punctuations limited to one repetition \n",
        "            \n",
        "        Example:\n",
        "        Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
        "        Output : Really, Great !?.;:)\n",
        "\n",
        "        \"\"\"\n",
        "        # Pattern matching for all case alphabets\n",
        "        pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "\n",
        "        # Limiting all the repetitions to two characters.\n",
        "        # MODIFIED: keep only one repetition of the character\n",
        "        formatted_text = pattern_alpha.sub(r\"\\1\\1\", word) \n",
        "\n",
        "        # Pattern matching for all the punctuations that can occur\n",
        "        pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "\n",
        "        # Limiting punctuations in previously formatted string to only one.\n",
        "        combined_formatted = pattern_punct.sub(r'\\1', formatted_text)\n",
        "\n",
        "        # The below statement is replacing repetitions of spaces that occur more than two times with that of one occurrence.\n",
        "        final_formatted = re.sub(' {2,}',' ', combined_formatted)\n",
        "        return final_formatted\n",
        "    \n",
        "    def merge_apostrophes(self, corpus):\n",
        "      new_corpus = []\n",
        "      for doc in corpus:\n",
        "        indexes = self._get_neg_indexes(doc)\n",
        "        for el in indexes:\n",
        "          doc[el[0]:el[1]] = [\"\".join(doc[el[0]:el[1]])]\n",
        "        new_corpus.append(doc)\n",
        "      return new_corpus\n",
        "\n",
        "    def _get_neg_indexes(self, sent):\n",
        "      contr = [\"t\", \"ve\", \"re\", \"ll\", \"d\", \"all\", \"y\", \"cause\", \"m\", \"clock\", \"am\", \"s\"]\n",
        "      indexes = []\n",
        "      for idx, word in enumerate(sent):\n",
        "        # Try-except to avoid out of range indexes (there can be some \"'\" a the beginning or end of the phrase)\n",
        "        try:\n",
        "          if word==\"'\" and sent[idx+1] in contr:\n",
        "            indexes.append((idx-1,idx+2))\n",
        "        except:\n",
        "          pass\n",
        "      return indexes\n",
        "    \n",
        "    def clean_contractions(self, corpus):\n",
        "        new_corpus = []\n",
        "        for doc in corpus:\n",
        "            new_doc = []\n",
        "            for word in doc:\n",
        "                try:\n",
        "                    correct = CONTRACTION_MAP[word]\n",
        "                    correct = correct.split()\n",
        "                    new_doc += correct\n",
        "                except:\n",
        "                    new_doc.append(word)\n",
        "            new_corpus.append(new_doc)\n",
        "        return new_corpus\n",
        "\n",
        "    def clean_special_chars(self, corpus):\n",
        "        new_corpus = [[self._clean_special_word(w) for w in doc] for doc in corpus]\n",
        "        new_corpus = [[w for w in doc] for doc in corpus]\n",
        "        return new_corpus\n",
        "    \n",
        "    def _clean_special_word(self, word):\n",
        "        # The formatted text after removing not necessary punctuations.\n",
        "        formatted_text = re.sub(r\"[^a-zA-Z0-9:€$-,%.?!]+\", '', word) \n",
        "        # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
        "        return formatted_text\n",
        "    \n",
        "    def remove_stop_words(self, corpus):\n",
        "        stops = stopwords.words(\"english\")\n",
        "        stops = [word for word in stops if \"'t\" not in word or \"not\" not in word]\n",
        "        return [[word for word in doc if word not in stops] for doc in corpus]\n",
        "    \n",
        "\n",
        "class MRPipelinePhrases(MRAbstractPipeline):\n",
        "    \"\"\"\n",
        "    Pipeline for documents represented as list of phrases\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(MRPipelinePhrases, self).__init__()\n",
        "        self.pipeline = [self.remove_underscores, \n",
        "                         self.clean_special_chars,\n",
        "                         self.reducing_character_repetitions,\n",
        "                         self.lemmatize]\n",
        "\n",
        "    def remove_underscores(self, corpus):\n",
        "        \"\"\"\n",
        "        Solves the problem where some of the words are surrounded by underscores\n",
        "        (e.g. \"_hello_\")\n",
        "        \"\"\"\n",
        "        new_corpus = [self._clean_word(doc) for doc in corpus]\n",
        "        return new_corpus\n",
        "\n",
        "\n",
        "    def _clean_word(self, doc: str):\n",
        "        doc = doc.replace(\"_\", \" \")\n",
        "        return doc\n",
        "    \n",
        "    def reducing_character_repetitions(self, corpus):\n",
        "        new_corpus = [self._clean_repetitions(doc) for doc in corpus]\n",
        "        return new_corpus\n",
        "    \n",
        "    # inspired by https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a\n",
        "    def _clean_repetitions(self, word):\n",
        "        \"\"\"\n",
        "        This Function will reduce repetition to two characters \n",
        "        for alphabets and to one character for punctuations.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            word: str                \n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            Finally formatted text with alphabets repeating to \n",
        "            one characters & punctuations limited to one repetition \n",
        "            \n",
        "        Example:\n",
        "        Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
        "        Output : Realy, Great !?.;:)\n",
        "\n",
        "        \"\"\"\n",
        "        # Pattern matching for all case alphabets\n",
        "        pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "\n",
        "        # Limiting all the repetitions to two characters.\n",
        "        # MODIFIED: keep only one repetition of the character\n",
        "        formatted_text = pattern_alpha.sub(r\"\\1\\1\", word) \n",
        "\n",
        "        # Pattern matching for all the punctuations that can occur\n",
        "        pattern_punct = re.compile(r'([., /#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "\n",
        "        # Limiting punctuations in previously formatted string to only one.\n",
        "        combined_formatted = pattern_punct.sub(r'\\1', formatted_text)\n",
        "\n",
        "        # The below statement is replacing repetitions of spaces that occur more than two times with that of one occurrence.\n",
        "        final_formatted = re.sub(' {2,}',' ', combined_formatted)\n",
        "        return final_formatted\n",
        "\n",
        "    def clean_special_chars(self, corpus):\n",
        "        new_corpus = [self._clean_special_word(doc)  for doc in corpus]\n",
        "        return new_corpus\n",
        "    \n",
        "    def _clean_special_word(self, word):\n",
        "        # The formatted text after removing not necessary punctuations.\n",
        "        formatted_text = re.sub(r\"[^a-zA-Z0-9:€$-,%.?!]+\", ' ', word) \n",
        "        # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
        "        return formatted_text\n",
        "    \n",
        "\n",
        "    def lemmatize(self, corpus):\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        return [[token.lemma_ for token in nlp(doc)] for doc in corpus]\n"
      ],
      "metadata": {
        "id": "spYv4QqyqJ0H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skv2-rEQwMtR"
      },
      "source": [
        "### Corpus class\n",
        "I'm going to create a class for the representation of the corpus in order to have a self contained way to have all the functions that may be useful for the processing of the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C3PyvlOV60V7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "import numpy as np\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "\n",
        "class MovieReviewsCorpusPhrases():\n",
        "    def __init__(self, preprocess_pipeline = None):\n",
        "        \"\"\"\n",
        "        If non preprocess_pipeline is given, the text gets tokenized by default\n",
        "        using spacy tokenizer\n",
        "        \"\"\"\n",
        "        self.mr = movie_reviews\n",
        "        if preprocess_pipeline != None and not isinstance(preprocess_pipeline, MRPipelinePhrases):\n",
        "            raise ValueError(f\"preprocess_pipeline is not valid, you should pass \\\n",
        "                                a MRPipelinePhrases object or None\")\n",
        "        self.pipeline = preprocess_pipeline\n",
        "        self.raw_corpus, self.labels = self._get_raw_corpus()\n",
        "        if self.pipeline == None:\n",
        "            self.processed_corpus = self.raw_corpus\n",
        "        else:\n",
        "            # Flattened and preprocessed corpus\n",
        "            self.processed_corpus = self._preprocess()\n",
        "        \n",
        "        self.vocab = self._create_vocab()\n",
        "        \n",
        "\n",
        "    def _get_raw_corpus(self):\n",
        "        neg = [self.mr.raw(doc) for doc in self.mr.fileids()[:1000]]\n",
        "        pos = [self.mr.raw(doc) for doc in self.mr.fileids()[1000:]]\n",
        "        labels = [0]*len(neg) + [1]*len(pos)\n",
        "        return neg + pos, labels\n",
        "    \n",
        "    def _preprocess(self):\n",
        "        if self.pipeline != None:\n",
        "            return self.pipeline(self.raw_corpus)\n",
        "        else:\n",
        "            return self.raw_corpus\n",
        "        \n",
        "    def _create_vocab(self):\n",
        "        vocab = dict()\n",
        "        corpus_words = [w for doc in self.processed_corpus for w in doc]\n",
        "        for word in corpus_words:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except:\n",
        "                vocab[word] = 1\n",
        "        return vocab\n",
        "\n",
        "    def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            A 2D which each row has the corresponding embedding from the vocabulary\n",
        "        \"\"\"\n",
        "        matrix_length = len(self.vocab)\n",
        "        embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "        # If I use torch.zeros directly it crashes (don't know why)\n",
        "        embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "        null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "        for idx, key in enumerate(self.vocab.keys()):\n",
        "            if torch.equal(embedding[key], null_embedding):\n",
        "                embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "            else:\n",
        "                embedding_matrix[idx] = embedding[key]\n",
        "                \n",
        "        return embedding_matrix\n",
        "    \n",
        "    def get_indexed_corpus(self):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        Dictionary\n",
        "            Containing correspondences word -> index\n",
        "        \n",
        "        list(list(torch.tensor))\n",
        "            The corpus represented as indexes corresponding to each word\n",
        "        \"\"\"\n",
        "        vocab = {}\n",
        "        for idx, key in enumerate(self.vocab.keys()):\n",
        "            vocab[key] = idx\n",
        "        \n",
        "        indexed_corpus = [torch.tensor([torch.tensor(vocab[w], dtype=torch.int32) for w in doc]) for doc in self.processed_corpus]\n",
        "        return indexed_corpus, self.labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.processed_corpus)\n",
        "\n",
        "\n",
        "class MovieReviewsCorpus():\n",
        "    def __init__(self, preprocess_pipeline = None):\n",
        "        # list of documents, each document is a list containing words of that document\n",
        "        self.mr = movie_reviews\n",
        "        self.pipeline = preprocess_pipeline\n",
        "        # Corpus as list of documents. Documents as list of sentences. Sentences as list of tokens\n",
        "        self.unprocessed_corpus, self.labels = self._get_corpus()\n",
        "        # Corpus as list of documents. Documents as list of tokens\n",
        "        self.flattened_corpus = self._flatten()\n",
        "        if preprocess_pipeline == None:\n",
        "            self.processed_corpus = self.flattened_corpus\n",
        "        else:\n",
        "            # Flattened and preprocessed corpus\n",
        "            self.processed_corpus = self._preprocess()\n",
        "\n",
        "        self.corpus_words = self.get_corpus_words()\n",
        "        self.vocab = self._create_vocab()\n",
        "\n",
        "\n",
        "\n",
        "    def _list_to_str(self, doc) -> str:\n",
        "        \"\"\"\n",
        "        Put all elements of the list into a single string, separating each element with a space.\n",
        "        \"\"\"\n",
        "        return \" \".join([w for sent in doc for w in sent])\n",
        "\n",
        "    def _preprocess(self):\n",
        "        return self.pipeline(self.flattened_corpus)\n",
        "\n",
        "    def _flatten(self):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        list[list[str]]\n",
        "            Each inner list represents a document. Each document is a list of tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        # 3 nested list: each list contain a document, each inner list contains a phrase (until fullstop), each phrase contains words.\n",
        "\n",
        "        corpus = [[w for w in self._list_to_str(d).split(\" \")] for d in self.unprocessed_corpus]\n",
        "        return corpus\n",
        "\n",
        "    def _get_corpus(self):\n",
        "        neg = self.mr.paras(categories = \"neg\")\n",
        "        pos = self.mr.paras(categories = \"pos\")\n",
        "        labels = [0] * len(pos) + [1] * len(neg)\n",
        "        return neg + pos, labels\n",
        "\n",
        "    def movie_reviews_dataset_raw(self):\n",
        "        \"\"\"\n",
        "        Returns the dataset containing:\n",
        "\n",
        "        - A list of all the documents\n",
        "        - The corresponding label for each document\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple(list, list)\n",
        "            The dataset: first element is the list of the document, the second element of the tuple is the associated label (positive or negative) for each document\n",
        "        \"\"\"\n",
        "\n",
        "        return self.flattened_corpus, self.labels\n",
        "\n",
        "    def get_sentence_ds(self):\n",
        "        neg = self.mr.paras(categories = \"neg\")\n",
        "        pos = self.mr.paras(categories = \"pos\")\n",
        "\n",
        "        pos = [phrase for doc in pos for phrase in doc]\n",
        "        neg = [phrase for doc in neg for phrase in doc]\n",
        "\n",
        "        labels = np.array([0] * len(pos) + [1] * len(neg))\n",
        "        corpus = neg+pos\n",
        "        return corpus, labels\n",
        "\n",
        "\n",
        "    def get_corpus_words(self) -> list:\n",
        "        return [w for doc in self.processed_corpus for w in doc]\n",
        "    \n",
        "    def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            A 2D which each row has the corresponding embedding from the vocabulary\n",
        "        \"\"\"\n",
        "        matrix_length = len(self.vocab)\n",
        "        embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "        # If I use torch.zeros directly it crashes (don't know why)\n",
        "        embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "        null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "        for idx, key in enumerate(self.vocab.keys()):\n",
        "            if torch.equal(embedding[key], null_embedding):\n",
        "                embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "            else:\n",
        "                embedding_matrix[idx] = embedding[key]\n",
        "                \n",
        "        return embedding_matrix\n",
        "    \n",
        "    def get_fasttext_embedding_matrix(self, embedding, embedding_dim):\n",
        "        matrix_length = len(self.vocab)\n",
        "        embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "        # If I use torch.zeros directly it crashes (don't know why)\n",
        "        embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "        null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "        for idx, key in enumerate(self.vocab.keys()):\n",
        "            tensor_embedding = torch.from_numpy(embedding[key].copy())\n",
        "            if torch.equal(tensor_embedding, null_embedding):\n",
        "                embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "            else:\n",
        "                embedding_matrix[idx] = tensor_embedding\n",
        "                \n",
        "        return embedding_matrix\n",
        "    \n",
        "    def get_indexed_corpus(self):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        Dictionary\n",
        "            Containing correspondences word -> index\n",
        "        \n",
        "        list(list(torch.tensor))\n",
        "            The corpus represented as indexes corresponding to each word\n",
        "        \"\"\"\n",
        "        vocab = {}\n",
        "        for idx, key in enumerate(self.vocab.keys()):\n",
        "            vocab[key] = idx\n",
        "        \n",
        "        indexed_corpus = [torch.tensor([torch.tensor(vocab[w], dtype=torch.int32) for w in doc]) for doc in self.processed_corpus]\n",
        "        return indexed_corpus, self.labels\n",
        "\n",
        "\n",
        "    def _create_vocab(self):\n",
        "        vocab = dict()\n",
        "        for word in self.corpus_words:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except:\n",
        "                vocab[word] = 1\n",
        "        return vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.flattened_corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zZaF5yV9rlo-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class MovieReviewsDataset(Dataset):\n",
        "  def __init__(self, raw_dataset):\n",
        "    super(MovieReviewsDataset, self).__init__()\n",
        "    self.corpus = np.array(raw_dataset[0], dtype = object)\n",
        "    self.targets = np.array(raw_dataset[1], dtype = np.int64)\n",
        "    self.max_element = len(max(self.corpus, key=lambda x: len(x)))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    item = self.corpus[index]\n",
        "    label = self.targets[index]\n",
        "    return (item, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr7PPG1k4gFq"
      },
      "source": [
        "### Create the model class\n",
        "Let's first try with a simple BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bCvqaJ8-hKmT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix = None, device = \"cuda\", input_size = 300, hidden_size = 128, output_size = 2):\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        if embedding_matrix != None:\n",
        "          self.embedding = self.create_embedding_layer(embedding_matrix)\n",
        "        else:\n",
        "          self.embedding = None\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True, bidirectional=True)\n",
        "        self.fc = nn.Sequential(nn.ReLU(),\n",
        "                                nn.BatchNorm1d(hidden_size*2, eps = 1e-08),\n",
        "                                nn.Dropout(0.3),\n",
        "                                nn.Linear(hidden_size*2, output_size)\n",
        "                                )\n",
        "\n",
        "    def create_embedding_layer(self, embedding_matrix):\n",
        "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "        emb_layer = nn.Embedding(num_embeddings, embedding_dim, -1)\n",
        "        emb_layer.load_state_dict({\"weight\": embedding_matrix})\n",
        "        return emb_layer\n",
        "\n",
        "    # function taken from https://discuss.pytorch.org/t/how-to-use-pack-sequence-if-we-are-going-to-use-word-embedding-and-bilstm/28184/4\n",
        "    def simple_elementwise_apply(self, fn, packed_sequence):\n",
        "        \"\"\"applies a pointwise function fn to each element in packed_sequence\"\"\"\n",
        "        return torch.nn.utils.rnn.PackedSequence(fn(packed_sequence.data), packed_sequence.batch_sizes)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        if self.cuda:\n",
        "            return (torch.zeros(2, batch_size, self.hidden_size).to(self.device),\n",
        "                    torch.zeros(2, batch_size, self.hidden_size).to(self.device),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.batch_sizes[0].item()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        x = self.simple_elementwise_apply(self.embedding, x)\n",
        "\n",
        "        # output: batch_size, sequence_length, hidden_size * 2 (since is bilstm)\n",
        "        out, _ = self.lstm(x, hidden)\n",
        "        out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "        # Interested only in the last layer\n",
        "        out = out[list(range(batch_size)), input_sizes - 1, :]\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BiLSTMAttention(BiLSTM):\n",
        "    # BiLSTM with attention inspired by the following paper: https://aclanthology.org/S18-1040.pdf\n",
        "    def __init__(self, embedding_matrix = None, device=\"cuda\", input_size=300,\n",
        "                 hidden_size=128, context_size = None, output_size=2):\n",
        "        super(BiLSTMAttention, self).__init__(embedding_matrix, device, input_size, hidden_size, output_size)\n",
        "        # Not self attention :)\n",
        "        if context_size != None:\n",
        "          self.attention = nn.Linear(self.hidden_size * 2, context_size)\n",
        "          self.history = nn.Parameter(torch.randn(context_size))\n",
        "        else:\n",
        "          self.attention = nn.Linear(self.hidden_size * 2, 1)\n",
        "          self.history = None\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.batch_sizes[0].item()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        if self.embedding != None:\n",
        "          x = self.simple_elementwise_apply(self.embedding, x)\n",
        "\n",
        "        # output: batch_size, sequence_length, hidden_size * 2 (since is bilstm)\n",
        "        out, _ = self.lstm(x, hidden)\n",
        "        out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "\n",
        "        if self.history == None:\n",
        "          attention_values = torch.tanh(self.attention(out)).squeeze()\n",
        "          attention_weights = torch.softmax(attention_values, dim = 1).unsqueeze(1)\n",
        "          # n_docs, sequence_length\n",
        "        else:\n",
        "          attention_values = torch.tanh(self.attention(out))\n",
        "          attention_weights = torch.softmax(attention_values.matmul(self.history), dim = 1).unsqueeze(1)\n",
        "          # n_docs, sequence_length\n",
        "\n",
        "        out = torch.sum(attention_weights.matmul(out), dim = 1)\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        attention_weights = attention_weights.squeeze()\n",
        "        att = [doc[:input_sizes[idx]] for idx, doc in enumerate(attention_weights)]\n",
        "\n",
        "        return out, att\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yZFg9BvQdZHB"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for batch_idx, (inputs, targets, _) in enumerate(data_loader):\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    in_size = targets.size(dim=0)\n",
        "\n",
        "    outputs, _ = net(inputs)\n",
        "\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LJef4h1cfWym"
      },
      "outputs": [],
      "source": [
        "def test_step(net, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets, _) in enumerate(data_loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      in_size = targets.size(dim=0)\n",
        "\n",
        "      outputs, _ = net(inputs)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = outputs.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OOeUmEHZNBvw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "\n",
        "def main(train_loader, test_loader, embedding_matrix, device = \"cuda\", epochs = 10):\n",
        "\n",
        "  net = BiLSTMAttention(embedding_matrix, device = device, input_size=300).to(device)\n",
        "\n",
        "  optimizer = Adam(net.parameters(), 0.001, betas = (0.9, 0.9), amsgrad=True)\n",
        "\n",
        "  cost_function = nn.CrossEntropyLoss()\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    test_loss, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "    print(f\"Test loss: {test_loss} \\n Test accuracy: {test_accuracy}\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "  \n",
        "  _, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "\n",
        "  return test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GfNtrS8jmATx"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def pad(batch, max_size):\n",
        "    # try:\n",
        "    pad = torch.tensor([-1]*batch[0].size(dim=1), dtype = torch.float).to(\"cuda\")\n",
        "    embedded = 1\n",
        "    # except:\n",
        "    #  pad = torch.tensor([-1])\n",
        "    #  embedded = 0\n",
        "    for idx in range(len(batch)):\n",
        "        remaining = max_size - batch[idx].size(dim = 0)\n",
        "        abc = pad.repeat(remaining)\n",
        "        if embedded:\n",
        "          batch[idx] = torch.cat((batch[idx], pad.repeat(remaining, 1)), dim = 0)\n",
        "        else:\n",
        "          batch[idx] = torch.cat((batch[idx], pad.repeat(remaining)), dim = 0)\n",
        "    return batch\n",
        "\n",
        "def batch_to_tensor(X: List[torch.tensor], max_size):\n",
        "    # try:\n",
        "    X_tensor = torch.zeros((len(X), max_size, X[0].size(dim=1)), dtype=torch.float).to(\"cuda\")\n",
        "    # except:\n",
        "    #  X_tensor = torch.zeros((len(X), max_size), dtype=torch.int32)\n",
        "\n",
        "    for i, embed in enumerate(X):\n",
        "        X_tensor[i] = embed\n",
        "    return X_tensor\n",
        "\n",
        "def sort_ds(X, Y):\n",
        "    \"\"\"\n",
        "    Sort inputs by document lengths\n",
        "    \"\"\"\n",
        "    document_lengths = np.array([tens.size(dim = 0) for tens in X])\n",
        "    indexes = np.argsort(document_lengths)\n",
        "    document_lengths = document_lengths.tolist()\n",
        "\n",
        "    X_sorted = [X[idx] for idx in indexes][::-1]\n",
        "    Y_sorted = [Y[idx] for idx in indexes][::-1]\n",
        "    document_lengths = torch.tensor([document_lengths[idx] for idx in indexes][::-1])\n",
        "\n",
        "    return X_sorted, Y_sorted, document_lengths, indexes\n",
        "\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    X, Y = list(zip(*batch))\n",
        "    # Sort dataset\n",
        "    X, Y, document_lengths, indexes = sort_ds(X, Y)\n",
        "\n",
        "    # Get tensor sizes\n",
        "    max_size = torch.max(document_lengths).item()\n",
        "\n",
        "    # Pad tensor each element\n",
        "    X = pad(X, max_size)\n",
        "\n",
        "    # Transform the batch to a tensor\n",
        "    X_tensor = batch_to_tensor(X, max_size)\n",
        "    Y_tensor = torch.tensor(Y)\n",
        "    # Return the padded sequence object\n",
        "    X_final = pack_padded_sequence(X_tensor, document_lengths, batch_first=True)\n",
        "    return X_final, Y_tensor, indexes\n",
        "\n",
        "\n",
        "def get_data(batch_size: int, dataset, collate_fn, random_state = 42):\n",
        "  # Random Split\n",
        "  train_indexes, test_indexes = train_test_split(list(range(len(dataset.targets))), test_size = 0.2,\n",
        "                                                  stratify = dataset.targets, random_state = random_state)\n",
        "\n",
        "  train_ds = Subset(dataset, train_indexes)\n",
        "  test_ds = Subset(dataset, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "  test_loader = DataLoader(test_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "\n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "def main_cross_validation(main, dataset, embedding_matrix, collate_fn,\n",
        "                          device = \"cuda\", epochs = 20, random_state = 42, batch_size = 128):\n",
        "\n",
        "  targets = np.asarray(dataset.targets, dtype=np.int64)\n",
        "\n",
        "  skf = StratifiedKFold(5, shuffle = True, random_state=random_state)\n",
        "\n",
        "  fold_accuracies = []\n",
        "  \n",
        "  for fold, (train_indexes, val_indexes) in enumerate(skf.split(np.zeros(len(dataset)),\n",
        "                                                      targets)):\n",
        "    print(f\"\\n Fold: {fold}\")\n",
        "    train_sampler = SubsetRandomSampler(train_indexes)\n",
        "    val_sampler = SubsetRandomSampler(val_indexes)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size = batch_size, sampler = train_sampler,\n",
        "                              collate_fn = collate_fn, pin_memory=True)\n",
        "    val_loader = DataLoader(dataset, batch_size = batch_size, sampler = val_sampler,\n",
        "                            collate_fn = collate_fn, pin_memory = True)\n",
        "\n",
        "\n",
        "    val_accuracy = main(train_loader, val_loader, embedding_matrix, device, epochs)\n",
        "    \n",
        "    fold_accuracies.append(val_accuracy)\n",
        "\n",
        "\n",
        "  fold_accuracies = np.array(fold_accuracies)\n",
        "\n",
        "  return fold_accuracies.mean(), fold_accuracies.std()\n",
        "\n"
      ],
      "metadata": {
        "id": "FkRBKES24_0v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lexicon Based Supervised Attention Model"
      ],
      "metadata": {
        "id": "MkeZyHH15r6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MRPipelineLBSA(MRAbstractPipeline):\n",
        "    \"\"\"\n",
        "    Pipeline for documents represented as list of tokens\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(MRPipelineLBSA, self).__init__()\n",
        "        self.pipeline = [self.remove_underscores, \n",
        "                         self.reducing_character_repetitions,\n",
        "                         self.merge_apostrophes,\n",
        "                         self.clean_contractions,\n",
        "                         self.clean_special_chars,\n",
        "                         # self.remove_stop_words\n",
        "                         ]\n",
        "\n",
        "    def remove_underscores(self, corpus):\n",
        "        \"\"\"\n",
        "        Solves the problem where some of the words are surrounded by underscores\n",
        "        (e.g. \"_hello_\")\n",
        "        \"\"\"\n",
        "\n",
        "        for doc in corpus:\n",
        "          for sent in doc:\n",
        "            for idx, word in enumerate(sent):\n",
        "                if \"_\" in word:\n",
        "                    cleaned_word = self._clean_word(word)\n",
        "                    sent[idx] = cleaned_word\n",
        "        return corpus\n",
        "\n",
        "\n",
        "    def _clean_word(self, word: str):\n",
        "        word = word.replace(\"_\", \" \")\n",
        "        # remove spaces before and after the word\n",
        "        word = word.split()\n",
        "        word = \" \".join(word)\n",
        "        return word\n",
        "    \n",
        "    def reducing_character_repetitions(self, corpus):\n",
        "        \n",
        "        new_corpus = [[[self._clean_repetitions(w) for w in sent] for sent in doc] for doc in corpus]\n",
        "        return new_corpus\n",
        "\n",
        "    # inspired by https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a\n",
        "    def _clean_repetitions(self, word):\n",
        "        \"\"\"\n",
        "        This Function will reduce repetition to two characters \n",
        "        for alphabets and to one character for punctuations.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            word: str                \n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            Finally formatted text with alphabets repeating to \n",
        "            one characters & punctuations limited to one repetition \n",
        "            \n",
        "        Example:\n",
        "        Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
        "        Output : Really, Great !?.;:)\n",
        "\n",
        "        \"\"\"\n",
        "        # Pattern matching for all case alphabets\n",
        "        pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "\n",
        "        # Limiting all the repetitions to two characters.\n",
        "        # MODIFIED: keep only one repetition of the character\n",
        "        formatted_text = pattern_alpha.sub(r\"\\1\\1\", word) \n",
        "\n",
        "        # Pattern matching for all the punctuations that can occur\n",
        "        pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "\n",
        "        # Limiting punctuations in previously formatted string to only one.\n",
        "        combined_formatted = pattern_punct.sub(r'\\1', formatted_text)\n",
        "\n",
        "        # The below statement is replacing repetitions of spaces that occur more than two times with that of one occurrence.\n",
        "        final_formatted = re.sub(' {2,}',' ', combined_formatted)\n",
        "        return final_formatted\n",
        "    \n",
        "    def merge_apostrophes(self, corpus):\n",
        "      new_corpus = []\n",
        "      for doc in corpus:\n",
        "        new_doc = []\n",
        "        for sent in doc:\n",
        "          indexes = self._get_neg_indexes(sent)\n",
        "          for el in indexes:\n",
        "            sent[el[0]:el[1]] = [\"\".join(sent[el[0]:el[1]])]\n",
        "          new_doc.append(sent)\n",
        "        new_corpus.append(new_doc)\n",
        "      return new_corpus\n",
        "\n",
        "    def _get_neg_indexes(self, sent):\n",
        "      contr = [\"t\", \"ve\", \"re\", \"ll\", \"d\", \"all\", \"y\", \"cause\", \"m\", \"clock\", \"am\", \"s\"]\n",
        "      indexes = []\n",
        "      for idx, word in enumerate(sent):\n",
        "        # Try-except to avoid out of range indexes (there can be some \"'\" a the beginning or end of the phrase)\n",
        "        try:\n",
        "          if word==\"'\" and sent[idx+1] in contr:\n",
        "            indexes.append((idx-1,idx+2))\n",
        "        except:\n",
        "          pass\n",
        "      return indexes\n",
        "    \n",
        "    def clean_contractions(self, corpus):\n",
        "        new_corpus = []\n",
        "        for doc in corpus:\n",
        "          new_doc = []\n",
        "          for sent in doc:\n",
        "            new_sent = []\n",
        "            for word in sent:\n",
        "                try:\n",
        "                    correct = CONTRACTION_MAP[word]\n",
        "                    correct = correct.split()\n",
        "                    new_sent += correct\n",
        "                except:\n",
        "                    new_sent.append(word)\n",
        "            new_doc.append(new_sent)\n",
        "          new_corpus.append(new_doc)\n",
        "        return new_corpus\n",
        "\n",
        "    def clean_special_chars(self, corpus):\n",
        "        new_corpus = [[[self._clean_special_word(w) for w in sent] for sent in doc] for doc in corpus] \n",
        "        return new_corpus\n",
        "    \n",
        "    def _clean_special_word(self, word):\n",
        "        # The formatted text after removing not necessary punctuations.\n",
        "        formatted_text = re.sub(r\"[^a-zA-Z0-9:€$-,%.?!]+\", '', word) \n",
        "        # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
        "        return formatted_text\n",
        "    \n",
        "    def remove_stop_words(self, corpus):\n",
        "      stops = stopwords.words(\"english\")\n",
        "      # We don't want to remove stop words associated with negations\n",
        "      stops = [word for word in stops if \"'t\" not in word or \"not\" not in word]\n",
        "      return [[[word for word in sent if word not in stops] for sent in doc] for doc in corpus]"
      ],
      "metadata": {
        "id": "sJQywSXpD33-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MovieReviewsCorpusLBSA():\n",
        "  def __init__(self, preprocess_pipeline = None):\n",
        "      \"\"\"\n",
        "      If non preprocess_pipeline is given, the text gets tokenized by default\n",
        "      using spacy tokenizer\n",
        "      \"\"\"\n",
        "      self.mr = movie_reviews\n",
        "      if preprocess_pipeline != None and not isinstance(preprocess_pipeline, MRPipelineLBSA):\n",
        "          raise ValueError(f\"preprocess_pipeline is not valid, you should pass \\\n",
        "                              a MRPipelineLBSA object or None\")\n",
        "      self.pipeline = preprocess_pipeline\n",
        "      self.raw_corpus, self.labels = self._get_raw_corpus()\n",
        "      if self.pipeline == None:\n",
        "          self.processed_corpus = self.raw_corpus\n",
        "      else:\n",
        "          # Flattened and preprocessed corpus\n",
        "          self.processed_corpus = self._preprocess()\n",
        "      \n",
        "      self.vocab = self._create_vocab()\n",
        "      \n",
        "\n",
        "  def _get_raw_corpus(self):\n",
        "      neg = self.mr.paras(categories = \"neg\")\n",
        "      pos = self.mr.paras(categories = \"pos\")\n",
        "      labels = [0]*len(neg) + [1]*len(pos)\n",
        "      return neg + pos, labels\n",
        "  \n",
        "  def _preprocess(self):\n",
        "      if self.pipeline != None:\n",
        "          return self.pipeline(self.raw_corpus)\n",
        "      else:\n",
        "          return self.raw_corpus\n",
        "      \n",
        "  def _create_vocab(self):\n",
        "      vocab = dict()\n",
        "      corpus_words = [w for doc in self.processed_corpus for sent in doc for w in sent]\n",
        "      for word in corpus_words:\n",
        "          try:\n",
        "              vocab[word] += 1\n",
        "          except:\n",
        "              vocab[word] = 1\n",
        "      return vocab\n",
        "\n",
        "  def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      np.ndarray\n",
        "          A 2D which each row has the corresponding embedding from the vocabulary\n",
        "      \"\"\"\n",
        "      matrix_length = len(self.vocab)\n",
        "      embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "      # If I use torch.zeros directly it crashes (don't know why)\n",
        "      embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "      null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "          if torch.equal(embedding[key], null_embedding):\n",
        "              embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "          else:\n",
        "              embedding_matrix[idx] = embedding[key]\n",
        "              \n",
        "      return embedding_matrix\n",
        "  \n",
        "  def get_indexed_corpus(self):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      Dictionary\n",
        "          Containing correspondences word -> index\n",
        "      \n",
        "      list(int)\n",
        "          labels associated with each document\n",
        "      \"\"\"\n",
        "      vocab = {}\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "          vocab[key] = idx\n",
        "      \n",
        "      # each doc is a list of tensor which represent sentences, each sentence is a tensor of indexed words\n",
        "      indexed_corpus = [[torch.tensor([vocab[w] for w in sent], dtype=torch.int32) \n",
        "                        for sent in doc]\n",
        "                        for doc in self.processed_corpus]\n",
        "      return indexed_corpus, self.labels\n",
        "  \n",
        "  def __len__(self):\n",
        "      return len(self.processed_corpus)\n",
        "\n",
        "c = MovieReviewsCorpusLBSA()"
      ],
      "metadata": {
        "id": "n9gmVtdg_Gf1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "\n",
        "class MovieReviewsDatasetLBSA(Dataset):\n",
        "  def __init__(self, corpus):\n",
        "    super(MovieReviewsDatasetLBSA, self).__init__()\n",
        "    self.corpus = corpus\n",
        "    indexed_corpus = self.corpus.get_indexed_corpus()\n",
        "    # Word level gold attention vector\n",
        "    self.word_lambda = 3\n",
        "    self.sentence_lambda = 3\n",
        "    self.sentiment_degree = self._compute_sentiment_degree()\n",
        "    self.wl_gold_av = self._compute_gold_words()\n",
        "    self.sl_gold_av = self._compute_gold_sents()\n",
        "    self.data = indexed_corpus[0]\n",
        "    self.targets = indexed_corpus[1]\n",
        "  \n",
        "  def _compute_sentiment_degree(self):\n",
        "    senti_vocab = self._build_senti_vocab(self.corpus.vocab)\n",
        "    path = '/content/gdrive/My Drive/nlu-project/lexicons/'\n",
        "    mpqa_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'mpqa/mpqa.json')\n",
        "    bingliu_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'bingliu/bingliu.json')\n",
        "    inquirer_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'inquirer/inquirer.json')\n",
        "    concreteness_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'concreteness/concreteness.json')\n",
        "    twitter_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'twitter/twitter.json')\n",
        "    qwn_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'qwn/qwn.json')\n",
        "    social_sent = self._build_social_sent_vocab(self.corpus.vocab, path + 'social_sent')\n",
        "    senticnet_vocab = self.build_sentic_net_vocab(self.corpus.vocab, path + \"sentic_net/senticnet.txt\")\n",
        "    social_sent = self.build_social_sent_vocab(self.corpus.vocab, path + \"social_sent\")\n",
        "    res = self._compute_average_sentiment_degree(senti_vocab,\n",
        "                                                 mpqa_vocab,\n",
        "                                                 bingliu_vocab,\n",
        "                                                 inquirer_vocab,\n",
        "                                                 concreteness_vocab,\n",
        "                                                 twitter_vocab,\n",
        "                                                 qwn_vocab,\n",
        "                                                 senticnet_vocab,\n",
        "                                                 social_sent\n",
        "                                                 )\n",
        "    \n",
        "    corpus = self.corpus.processed_corpus\n",
        "    scores = [[[res[word] for word in sent] for sent in doc] for doc in corpus]\n",
        "    return scores\n",
        "\n",
        "  def _compute_gold_sents(self):\n",
        "    sentence_sentiment_degree  = [[sum(sent)/len(sent) for sent in doc] for doc in self.sentiment_degree]\n",
        "    gold = [self._normalized_softmax(doc, self.sentence_lambda) for doc in sentence_sentiment_degree]\n",
        "    return gold\n",
        "\n",
        "\n",
        "  def _compute_gold_words(self):\n",
        "    gold = [[self._normalized_softmax(sent_scores, self.word_lambda) for sent_scores in doc] for doc in self.sentiment_degree]\n",
        "    return gold\n",
        "\n",
        "  def _normalized_softmax(self, sequence, lam):\n",
        "    multiplied_sequence = [lam * el for el in sequence]\n",
        "    total = sum([math.exp(el) for el in sequence])\n",
        "    res = torch.tensor([math.exp(lam * el)/total for el in sequence])\n",
        "    return res\n",
        "\n",
        "  def _build_0_1_vocab(self, vocab, path):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/williamleif/socialsent/blob/master/socialsent/data/lexicons/mpqa.json\n",
        "\n",
        "    Values:\n",
        "    - 1 = positive\n",
        "    - 0 = neutral\n",
        "    - -1 = negative\n",
        "    \n",
        "    The absolute value will be taken\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "      lexicon = json.load(f)\n",
        "    \n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "    \n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = lexicon[key]\n",
        "        res_vocab[key] = abs(value)\n",
        "      except KeyError:\n",
        "        pass\n",
        "    \n",
        "    return res_vocab\n",
        "\n",
        "\n",
        "  def _build_senti_vocab(self, vocab):\n",
        "    \"\"\"\n",
        "    builds a vocab using senti-wordnet\n",
        "    \"\"\"\n",
        "    senti_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      senti_vocab[key] = 0\n",
        "\n",
        "    max_value = 0\n",
        "    for key in senti_vocab.keys():\n",
        "      senses = list(swn.senti_synsets(key))\n",
        "      pos = 0\n",
        "      neg = 0\n",
        "      for sense in senses:\n",
        "        if sense.synset.name().split(\".\")[0] == key:\n",
        "          pos += sense.pos_score()\n",
        "          neg += sense.neg_score()\n",
        "      if (pos != 0) or (neg != 0):\n",
        "        senti_vocab[key] = max(pos, neg)\n",
        "      if senti_vocab[key] > max_value:\n",
        "        max_value = senti_vocab[key]\n",
        "\n",
        "    for key in senti_vocab.keys():\n",
        "      senti_vocab[key] = self.maprange((0, max_value), (0, 1), senti_vocab[key])\n",
        "\n",
        "    return senti_vocab\n",
        "  \n",
        "  def build_sentic_net_vocab(self, vocab, path):\n",
        "\n",
        "    df = pd.read_csv(path, sep=\"\\t+\")\n",
        "\n",
        "    df.replace([\"negative\", \"positive\"], 1, inplace = True)\n",
        "    # df.set_index([\"CONCEPT\"], inplace = True)\n",
        "\n",
        "    df = dict(zip(df.CONCEPT, df.POLARITY))\n",
        "\n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "\n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = df[key]\n",
        "        res_vocab[key] = value\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    return res_vocab\n",
        "  \n",
        "  def _social_sent_vocab(self, vocab, path):\n",
        "    word_path = f\"{path}/frequent_words/\"\n",
        "    adj_path = f\"{path}/adjectives\"\n",
        "    word_files = [os.path.join(word_path, filename) for filename in os.listdir(word_path) if \".tsv\" in filename]\n",
        "    adj_files = [os.path.join(adj_path, filename) for filename in os.listdir(adj_path) if \".tsv\" in filename]\n",
        "\n",
        "    word_dfs = [pd.read_csv(f, sep = \"\\t\", names = [\"word\", \"mean\", \"std\"]) for f in word_files]\n",
        "    adj_dfs = [pd.read_csv(f, sep = \"\\t\", names = [\"word\", \"mean\", \"std\"]) for f in adj_files]\n",
        "\n",
        "    words = pd.concat(word_dfs)\n",
        "    adjs = pd.concat(adj_dfs)\n",
        "\n",
        "    tot = pd.concat([words, adjs])\n",
        "\n",
        "    tot = tot.drop(\"std\", axis = 1)\n",
        "    tot[\"mean\"] = tot[\"mean\"].abs()\n",
        "    tot.sort_values(by=[\"mean\"], inplace = True)\n",
        "    tot.drop_duplicates(subset = \"word\", keep=\"last\", inplace = True)\n",
        "    display(tot)\n",
        "\n",
        "    tot = dict(zip(tot[\"word\"], tot[\"mean\"]))\n",
        "\n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "\n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = tot[key]\n",
        "        res_vocab[key] = value\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    return res_vocab\n",
        "  \n",
        "  def maprange(self, a, b, s):\n",
        "    \"\"\"\n",
        "    Maps the number s from range a = [a1, a2] to range b = [b1, b2]\n",
        "    \"\"\"\n",
        "    # Source: https://rosettacode.org/wiki/Map_range#Python\n",
        "    (a1, a2), (b1, b2) = a, b\n",
        "    return  b1 + ((s - a1) * (b2 - b1) / (a2 - a1))\n",
        "  \n",
        "  def _compute_average_sentiment_degree(self, *args):\n",
        "    \"\"\"\n",
        "    Assumption: all arguments in args are dictionaries containing the same keys\n",
        "    and a numbers as value.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict\n",
        "      average of the sentiment degree across dictionaries for each word\n",
        "    \n",
        "    Example\n",
        "    -------\n",
        "    we have two dictionaries that give a sentiment degree to words:\n",
        "    a = {\"good\": 0.9, \"bad\": 0.7}\n",
        "    b = {\"good\": 0.5, \"bad\": 0.1}\n",
        "\n",
        "    result = {\"good\": 0.7, \"bad\": 0.4}\n",
        "    \"\"\"\n",
        "    n_args = len(args)\n",
        "    res = {}\n",
        "    for arg in args:\n",
        "      for key in arg.keys():\n",
        "        try:\n",
        "          res[key] += arg[key] # / n_args\n",
        "        except KeyError:\n",
        "          res[key] = arg[key] # / n_args\n",
        "    \n",
        "    return res\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = self.data[index]\n",
        "    label = self.targets[index]\n",
        "    gold_word = self.wl_gold_av[index]\n",
        "    gold_sent = self.sl_gold_av[index]\n",
        "    return (item, label, gold_word, gold_sent)\n",
        "\n",
        "corpus = MovieReviewsCorpusLBSA()\n",
        "ds = MovieReviewsDatasetLBSA(corpus)\n",
        "print(type(ds.sl_gold_av[1]))"
      ],
      "metadata": {
        "id": "8lGBLNBxo4Jv",
        "outputId": "1cdfe57b-dee7-49fa-8820-fa27ac3c099b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand:\n",
        "- If it is better to introduce intermediate supervision\n",
        "\n",
        "- If it is better to use one hot encoding for the output\n",
        "\n",
        "- If I intepreted well the word-loss"
      ],
      "metadata": {
        "id": "IT_UVsevkmNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLBSA(BiLSTMAttention):\n",
        "    # Lexicon Based Supervised Attention model (LBSA) inspired by the following paper: https://aclanthology.org/C18-1074.pdf\n",
        "    def __init__(self, embedding_matrix, device=\"cuda\", input_size=300,\n",
        "                 hidden_size=128, context_size = 10, output_size=2):\n",
        "\n",
        "        super(EncoderLBSA, self).__init__(embedding_matrix, device, input_size, hidden_size, context_size, output_size)\n",
        "        self.fc = nn.Sequential(nn.ReLU(),\n",
        "                                nn.Dropout(0.2),\n",
        "                                nn.Linear(hidden_size*2, hidden_size),\n",
        "                                nn.ReLU()\n",
        "                                )\n",
        "\n",
        "    # TODO: Pass the part inside for to super.forward()  \n",
        "    def forward(self, x):\n",
        "      att = []\n",
        "      res = []\n",
        "      for i, doc in enumerate(x):\n",
        "        doc = doc.to(self.device)\n",
        "        batch_size = doc.batch_sizes[0].item()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        doc = self.simple_elementwise_apply(self.embedding, doc)\n",
        "\n",
        "        out, _ = self.lstm(doc, hidden)\n",
        "        out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "        # n_sents, n_words_per_sent, hidden_size * 2 (since is bilstm)\n",
        "\n",
        "\n",
        "        attention_values = torch.tanh(self.attention(out))\n",
        "        # n_sents, n_words_per_sent, context_size\n",
        "\n",
        "        attention_weights = torch.softmax(attention_values.matmul(self.history), dim = 1).unsqueeze(1)\n",
        "        # n_sents, n_words_per_sent\n",
        "\n",
        "        out = torch.sum(attention_weights.matmul(out), dim = 1)\n",
        "        # n_sents, hidden*2\n",
        "\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        attention_weights = attention_weights.squeeze(dim=1)\n",
        "\n",
        "        att.append([sent[:input_sizes[idx]] for idx, sent in enumerate(attention_weights)])\n",
        "\n",
        "        res.append(out)\n",
        "      # n_doc, seq_lengths, hidden * 2\n",
        "      return res, att"
      ],
      "metadata": {
        "id": "k_dniRPY5q2A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sortLBSA(X, w_gold, s_gold):\n",
        "\n",
        "  sentence_lengths = [np.array([sent.size(dim=0) for sent in doc]) for doc in X]\n",
        "  indexes = [np.argsort(doc) for doc in sentence_lengths]\n",
        "  indexes = [el.tolist() for el in indexes]\n",
        "\n",
        "  X_sorted = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(X)]\n",
        "  # w_gold = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(w_gold)]\n",
        "  # s_gold = [torch.tensor([doc[idx2] for idx2 in indexes[idx]][::-1]) for idx, doc in enumerate(s_gold)]\n",
        "  sentence_lengths = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(sentence_lengths)]\n",
        "\n",
        "  return X_sorted, w_gold, s_gold, sentence_lengths, indexes\n",
        "\n",
        "def padLBSA(batch, max_sizes):\n",
        "    pad = torch.tensor([-1])\n",
        "    for idx1, doc in enumerate(batch):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        remaining = max_sizes[idx1] - sent.size(dim = 0)\n",
        "        batch[idx1][idx2] = torch.cat((sent, pad.repeat(remaining)), dim = 0)\n",
        "    return batch\n",
        "\n",
        "def to_tensorLBSA(batch, max_sizes):\n",
        "  res = []\n",
        "  for idx, doc in enumerate(batch):\n",
        "    buff = torch.zeros(len(doc), max_sizes[idx], dtype=torch.int32)\n",
        "    for idx2, sent in enumerate(doc):\n",
        "      buff[idx2] = sent\n",
        "\n",
        "    res.append(buff)\n",
        "  return res\n",
        "\n",
        "def collateLBSA(batch):\n",
        "  X, Y, w_gold, s_gold = list(zip(*batch))\n",
        "\n",
        "  X, w_gold, s_gold, sentence_lengths, indexes = sortLBSA(X, w_gold, s_gold)\n",
        "  # can take doc[0] since senetence_lengths is sorted\n",
        "  max_sizes = [doc[0] for doc in sentence_lengths]\n",
        "\n",
        "  # Pad tensor each element\n",
        "  X = padLBSA(X, max_sizes)\n",
        "  # Transform the batch to a tensor\n",
        "  X = to_tensorLBSA(X, max_sizes)\n",
        "\n",
        "  # Return the padded sequence object\n",
        "  X = [pack_padded_sequence(doc, sentence_lengths[idx], batch_first=True) for idx, doc in enumerate(X)]\n",
        "  return X, Y, w_gold, s_gold, indexes"
      ],
      "metadata": {
        "id": "JyjRmkGAIu94"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def element_wise_log_loss(out, labels):\n",
        "  res = - out.log().mul(labels).sum(dim=0)\n",
        "  return res\n",
        "\n",
        "def loss_LBSA(outputs, targets, mu_w = 0.001, mu_s = 0.05):\n",
        "  dec_output, w_att, s_att = outputs\n",
        "  target, w_gold, s_gold = targets\n",
        "\n",
        "  total_loss = 0\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "\n",
        "  total_loss += ce(dec_output, target)\n",
        "\n",
        "  w_loss = torch.mean(torch.tensor([\n",
        "    torch.sum(torch.tensor([\n",
        "        element_wise_log_loss(w_att[idx1][idx2], sent) for idx2, sent in enumerate(doc)\n",
        "    ])) * mu_w for idx1, doc in enumerate(w_gold)\n",
        "  ]))\n",
        "  total_loss += w_loss\n",
        "\n",
        "  s_loss = torch.mean(torch.tensor([\n",
        "      element_wise_log_loss(s_att[idx], doc) * mu_s for idx, doc in enumerate(s_gold)\n",
        "  ]))\n",
        "  total_loss += s_loss\n",
        "\n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "VXw9lvmaWM3B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_LBSA(encoder, decoder, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "\n",
        "  for batch_idx, (inputs, target, w_gold, s_gold, _) in enumerate(data_loader):\n",
        "    \n",
        "    in_size = len(target)\n",
        "\n",
        "    enc_output, w_att = encoder(inputs)\n",
        "    \n",
        "    batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "    dec_input, target, indexes = collate(batch)\n",
        "    \n",
        "    s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "\n",
        "    target = target.to(device)\n",
        "    for idx1, doc in enumerate(w_gold):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "    for idx, doc in enumerate(s_gold):\n",
        "       s_gold[idx] = doc.to(device)\n",
        "    \n",
        "\n",
        "    dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "    outputs = (dec_output, w_att, s_att)\n",
        "    targets = (target, w_gold, s_gold)\n",
        "\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "yZF4uo99VKsh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_LBSA(encoder, decoder, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, target, w_gold, s_gold, _) in enumerate(data_loader):\n",
        "      in_size = len(target)\n",
        "\n",
        "      enc_output, w_att = encoder(inputs)\n",
        "      \n",
        "      batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "      dec_input, target, indexes = collate(batch)\n",
        "\n",
        "      s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "      # Not sorting also w_gold becuse in the encoder, documents don't get shuffled\n",
        "\n",
        "      target = target.to(device)\n",
        "      for idx1, doc in enumerate(w_gold):\n",
        "        for idx2, sent in enumerate(doc):\n",
        "          w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "      for idx, doc in enumerate(s_gold):\n",
        "        s_gold[idx] = doc.to(device)\n",
        "\n",
        "      dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "      outputs = (dec_output, w_att, s_att)\n",
        "      targets = (target, w_gold, s_gold)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "      \n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "RpyI0qRvC0cF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting must be a problem, otherwise word_level attention doesn't get the correct supervision.\n",
        "Same goes for sentences"
      ],
      "metadata": {
        "id": "ntMCKJDtuEnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_LBSA_new(encoder, decoder, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "\n",
        "  for batch_idx, (inputs, target, w_gold, s_gold, sent_indexes) in enumerate(data_loader):\n",
        "    \n",
        "    in_size = len(target)\n",
        "\n",
        "    enc_output, w_att = encoder(inputs)\n",
        "    # Sorting the sentences of the encoder to their original position\n",
        "    # Inverting the output because indexes are in ascending order, output is in descending\n",
        "    enc_output = [torch.flip(enc_output[doc_idx], dims = [0]) for doc_idx, _ in enumerate(enc_output)]\n",
        "    w_att = [w_att[doc_idx][::-1] for doc_idx, _ in enumerate(w_att)]\n",
        "    # Using argsort on the indexes reverses the previous argsort\n",
        "    inverted_indexes = [np.argsort(np.array(doc)) for doc in sent_indexes]\n",
        "    inverted_indexes = [el.tolist() for el in inverted_indexes]\n",
        "    # Sort the sentences with original sorting:\n",
        "    # n_doc, n_sents, hidden*2\n",
        "\n",
        "    enc_output = [enc_output[doc_idx][sent_idx] for doc_idx, sent_idx in enumerate(inverted_indexes)]\n",
        "    w_att = [[doc[idx2] for idx2 in inverted_indexes[idx]] for idx, doc in enumerate(w_att)]\n",
        "\n",
        "\n",
        "    # for i, doc in enumerate(enc_output):\n",
        "        # for j, sent in enumerate(doc):\n",
        "          # enc_output[i][j] = sent.cpu()\n",
        "    \n",
        "    batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "    dec_input, target, indexes = collate(batch)\n",
        "    \n",
        "    # s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "    target = target.to(device)\n",
        "    for idx1, doc in enumerate(w_gold):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "    for idx, doc in enumerate(s_gold):\n",
        "       s_gold[idx] = doc.to(device)\n",
        "    \n",
        "\n",
        "    dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "    dec_output = torch.flip(dec_output, dims = [0])\n",
        "    s_att = s_att[::-1]\n",
        "    target = torch.flip(target, dims = [0])\n",
        "\n",
        "\n",
        "    inverted_indexes = np.argsort(np.array(indexes))\n",
        "    inverted_indexes = inverted_indexes.tolist()\n",
        "    # Sort the sentences with original sorting:\n",
        "    # n_doc, n_sents, hidden*2\n",
        "    dec_output = dec_output[inverted_indexes]\n",
        "    s_att = [s_att[idx] for idx in inverted_indexes]\n",
        "    target = target[inverted_indexes]\n",
        "\n",
        "\n",
        "    outputs = (dec_output, w_att, s_att)\n",
        "    targets = (target, w_gold, s_gold)\n",
        "    \n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "RN2AM7tLgIDm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_LBSA_new(encoder, decoder, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, target, w_gold, s_gold, sent_indexes) in enumerate(data_loader):\n",
        "      in_size = len(target)\n",
        "\n",
        "      enc_output, w_att = encoder(inputs)\n",
        "\n",
        "      # First flip (go to ascending order)\n",
        "      enc_output = [torch.flip(enc_output[doc_idx], dims = [0]) for doc_idx, _ in enumerate(enc_output)]\n",
        "      w_att = [w_att[doc_idx][::-1] for doc_idx, _ in enumerate(w_att)]\n",
        "\n",
        "      # Second take indexes for getting the original positions\n",
        "      # Using argsort on the indexes reverses the previous argsort\n",
        "      inverted_indexes = [np.argsort(np.array(doc)) for doc in sent_indexes]\n",
        "      inverted_indexes = [el.tolist() for el in inverted_indexes]\n",
        "\n",
        "      # Third Sort the sentences with original sorting:\n",
        "      # n_doc, n_sents, hidden*2\n",
        "      enc_output = [enc_output[doc_idx][sent_idx] for doc_idx, sent_idx in enumerate(inverted_indexes)]\n",
        "      w_att = [[doc[idx2] for idx2 in inverted_indexes[idx]] for idx, doc in enumerate(w_att)]\n",
        "      \n",
        "      batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "      dec_input, target, indexes = collate(batch)\n",
        "\n",
        "      # s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "      # Not sorting also w_gold becuse in the encoder, documents don't get shuffled inside\n",
        "\n",
        "      target = target.to(device)\n",
        "      for idx1, doc in enumerate(w_gold):\n",
        "        for idx2, sent in enumerate(doc):\n",
        "          w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "      for idx, doc in enumerate(s_gold):\n",
        "        s_gold[idx] = doc.to(device)\n",
        "\n",
        "      dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "\n",
        "      dec_output = torch.flip(dec_output, dims = [0])\n",
        "      s_att = s_att[::-1]\n",
        "      target = torch.flip(target, dims = [0])\n",
        "\n",
        "      inverted_indexes = np.argsort(np.array(indexes))\n",
        "      inverted_indexes = inverted_indexes.tolist()\n",
        "      # Sort the sentences with original sorting:\n",
        "      # n_doc, n_sents, hidden*2\n",
        "      dec_output = dec_output[inverted_indexes]\n",
        "      s_att = [s_att[idx] for idx in inverted_indexes]\n",
        "      target = target[inverted_indexes]\n",
        "\n",
        "      outputs = (dec_output, w_att, s_att)\n",
        "      targets = (target, w_gold, s_gold)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "      \n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ],
      "metadata": {
        "id": "kEf6kIOghmBm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def main_LBSA(train_loader, test_loader, embedding_matrix, device = \"cuda\", epochs = 10):\n",
        "\n",
        "  encoder = EncoderLBSA(embedding_matrix = embedding_matrix, device = device, input_size=300, hidden_size=100).to(device)\n",
        "  decoder = BiLSTMAttention(device = device, input_size=100, context_size = 10).to(device)\n",
        "\n",
        "  optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), 0.001, betas = (0.9, 0.999), amsgrad=True)\n",
        "  scheduler = ExponentialLR(optimizer, 0.8)\n",
        "\n",
        "  cost_function = loss_LBSA\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step_LBSA_new(encoder, decoder, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    test_loss, test_accuracy = test_step_LBSA_new(encoder, decoder, test_loader, cost_function, device)\n",
        "    print(f\"Test loss: {test_loss} \\n Test accuracy: {test_accuracy}\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    scheduler.step()\n",
        "\n",
        "  return test_accuracy\n"
      ],
      "metadata": {
        "id": "3Xzpaa7IDN9Z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_vectors = GloVe(name='840B', dim=300, cache = \"/content/gdrive/My Drive/nlu-project/.vector_cache\")"
      ],
      "metadata": {
        "id": "hgNE3lxXrW4M"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mr_pipeline = MRPipelineLBSA()\n",
        "corpus = MovieReviewsCorpusLBSA(mr_pipeline)"
      ],
      "metadata": {
        "id": "Xu7rZw-mrqke"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = corpus.get_embedding_matrix(global_vectors, 300)\n",
        "# ds = corpus.get_indexed_corpus()"
      ],
      "metadata": {
        "id": "0TtoFzYDsGHk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MovieReviewsDatasetLBSA(corpus)\n",
        "# train_loader, test_loader = get_data(128, dataset, collate_fn=collateLBSA)"
      ],
      "metadata": {
        "id": "QQf9eKAqsIfr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "eDrwHvOIEsl2",
        "outputId": "35378dc8-9068-4477-c13c-ffb778266af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold: 0\n",
            "epoch 0:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-470fd284d9f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Oppure ragionare un attimo.... servono davvero i gradient per l'input? O posso fare il detach?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_LBSA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollateLBSA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-54ca1cd8c96c>\u001b[0m in \u001b[0;36mmain_cross_validation\u001b[0;34m(main, dataset, embedding_matrix, collate_fn, device, epochs, random_state, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mfold_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-1973a1a41408>\u001b[0m in \u001b[0;36mmain_LBSA\u001b[0;34m(train_loader, test_loader, embedding_matrix, device, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch {e}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step_LBSA_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step_LBSA_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-4b3029ead2d9>\u001b[0m in \u001b[0;36mtraining_step_LBSA_new\u001b[0;34m(encoder, decoder, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 87.3, 86.75 e qualcosa - 88.25\n",
        "# TODO: linear layer nel decoder,\n",
        "#       Sentiment degree come sommatoria invece che somma\n",
        "# Da fare una collate nuova direttamente che non vada a toccare i tensori esistenti.\n",
        "# Oppure ragionare un attimo.... servono davvero i gradient per l'input? O posso fare il detach?\n",
        "\n",
        "mean, std = main_cross_validation(main_LBSA, dataset, embedding_matrix, collateLBSA, epochs = 12)\n",
        "\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pBBa_vvYSnL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "tensor([1315, 1222, 1011, 1010,  936,  862,  814,  807,  807,  764,  718,  515,\n",
        "         495,  388,  344,  323])\n",
        "tensor([1617, 1361, 1311, 1178, 1081, 1068,  958,  941,  925,  768,  688,  619,\n",
        "         604,  573,  484,  405])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Af9ORg9pFKe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P67fhfYpSZA"
      },
      "source": [
        "# First try to parse phrases documet-wise, then try to parse each phrase of a document separately, and then aggregate the result (if there are more positive phrases then positive, otherwise negative). (Try also to give a weight depending on the number of sentiment lexemes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mK4Rmfr4ziT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('nlu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e0262c2e7a08424d65c968f8ecfc5afb6b5a99089f86fd0fa27478ea619b0ef2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}