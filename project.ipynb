{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinni98/Sentiment-analysis-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NBg5KtW5AMy"
      },
      "source": [
        "## Polarity Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8msH_nw3rf39",
        "outputId": "a918f9e1-33d7-4e32-a066-d454140e347e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vdSs1gS1rgh_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gdrive/My Drive/nlu-project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HvHgFZJe4_Xq"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import torch\n",
        "from nltk.corpus import movie_reviews\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vidUhnQmvEUr",
        "outputId": "bad9083b-7dae-4185-87a8-5548579161f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"movie_reviews\")\n",
        "nltk.download(\"subjectivity\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"sentiwordnet\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "spYv4QqyqJ0H"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import spacy\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.sentiment.util import mark_negation\n",
        "\n",
        "from nltk.corpus import subjectivity\n",
        "\n",
        "\n",
        "CONTRACTION_MAP =  {\"ain't\": \"is not\",\n",
        "                        \"aren't\": \"are not\",\n",
        "                        \"can't\": \"cannot\",\n",
        "                        \"can't've\": \"cannot have\",\n",
        "                        \"'cause\": \"because\",\n",
        "                        \"could've\": \"could have\",\n",
        "                        \"couldn't\": \"could not\",\n",
        "                        \"couldn't've\": \"could not have\",\n",
        "                        \"didn't\": \"did not\",\n",
        "                        \"doesn't\": \"does not\",\n",
        "                        \"don't\": \"do not\",\n",
        "                        \"hadn't\": \"had not\",\n",
        "                        \"hadn't've\": \"had not have\",\n",
        "                        \"hasn't\": \"has not\",\n",
        "                        \"haven't\": \"have not\",\n",
        "                        \"he'd\": \"he would\",\n",
        "                        \"he'd've\": \"he would have\",\n",
        "                        \"he'll\": \"he will\",\n",
        "                        \"he'll've\": \"he he will have\",\n",
        "                        \"he's\": \"he is\",\n",
        "                        \"how'd\": \"how did\",\n",
        "                        \"how'd'y\": \"how do you\",\n",
        "                        \"how'll\": \"how will\",\n",
        "                        \"how's\": \"how is\",\n",
        "                        \"i'd\": \"i would\",\n",
        "                        \"i'd've\": \"i would have\",\n",
        "                        \"i'll\": \"i will\",\n",
        "                        \"i'll've\": \"i will have\",\n",
        "                        \"i'm\": \"i am\",\n",
        "                        \"i've\": \"i have\",\n",
        "                        \"isn't\": \"is not\",\n",
        "                        \"it'd\": \"it would\",\n",
        "                        \"it'd've\": \"it would have\",\n",
        "                        \"it'll\": \"it will\",\n",
        "                        \"it'll've\": \"it will have\",\n",
        "                        \"it's\": \"it is\",\n",
        "                        \"let's\": \"let us\",\n",
        "                        \"ma'am\": \"madam\",\n",
        "                        \"mayn't\": \"may not\",\n",
        "                        \"might've\": \"might have\",\n",
        "                        \"mightn't\": \"might not\",\n",
        "                        \"mightn't've\": \"might not have\",\n",
        "                        \"must've\": \"must have\",\n",
        "                        \"mustn't\": \"must not\",\n",
        "                        \"mustn't've\": \"must not have\",\n",
        "                        \"needn't\": \"need not\",\n",
        "                        \"needn't've\": \"need not have\",\n",
        "                        \"o'clock\": \"of the clock\",\n",
        "                        \"oughtn't\": \"ought not\",\n",
        "                        \"oughtn't've\": \"ought not have\",\n",
        "                        \"shan't\": \"shall not\",\n",
        "                        \"sha'n't\": \"shall not\",\n",
        "                        \"shan't've\": \"shall not have\",\n",
        "                        \"she'd\": \"she would\",\n",
        "                        \"she'd've\": \"she would have\",\n",
        "                        \"she'll\": \"she will\",\n",
        "                        \"she'll've\": \"she will have\",\n",
        "                        \"she's\": \"she is\",\n",
        "                        \"should've\": \"should have\",\n",
        "                        \"shouldn't\": \"should not\",\n",
        "                        \"shouldn't've\": \"should not have\",\n",
        "                        \"so've\": \"so have\",\n",
        "                        \"so's\": \"so as\",\n",
        "                        \"that'd\": \"that would\",\n",
        "                        \"that'd've\": \"that would have\",\n",
        "                        \"that's\": \"that is\",\n",
        "                        \"there'd\": \"there would\",\n",
        "                        \"there'd've\": \"there would have\",\n",
        "                        \"there's\": \"there is\",\n",
        "                        \"they'd\": \"they would\",\n",
        "                        \"they'd've\": \"they would have\",\n",
        "                        \"they'll\": \"they will\",\n",
        "                        \"they'll've\": \"they will have\",\n",
        "                        \"they're\": \"they are\",\n",
        "                        \"they've\": \"they have\",\n",
        "                        \"to've\": \"to have\",\n",
        "                        \"wasn't\": \"was not\",\n",
        "                        \"we'd\": \"we would\",\n",
        "                        \"we'd've\": \"we would have\",\n",
        "                        \"we'll\": \"we will\",\n",
        "                        \"we'll've\": \"we will have\",\n",
        "                        \"we're\": \"we are\",\n",
        "                        \"we've\": \"we have\",\n",
        "                        \"weren't\": \"were not\",\n",
        "                        \"what'll\": \"what will\",\n",
        "                        \"what'll've\": \"what will have\",\n",
        "                        \"what're\": \"what are\",\n",
        "                        \"what's\": \"what is\",\n",
        "                        \"what've\": \"what have\",\n",
        "                        \"when's\": \"when is\",\n",
        "                        \"when've\": \"when have\",\n",
        "                        \"where'd\": \"where did\",\n",
        "                        \"where's\": \"where is\",\n",
        "                        \"where've\": \"where have\",\n",
        "                        \"who'll\": \"who will\",\n",
        "                        \"who'll've\": \"who will have\",\n",
        "                        \"who's\": \"who is\",\n",
        "                        \"who've\": \"who have\",\n",
        "                        \"why's\": \"why is\",\n",
        "                        \"why've\": \"why have\",\n",
        "                        \"will've\": \"will have\",\n",
        "                        \"won't\": \"will not\",\n",
        "                        \"won't've\": \"will not have\",\n",
        "                        \"would've\": \"would have\",\n",
        "                        \"wouldn't\": \"would not\",\n",
        "                        \"wouldn't've\": \"would not have\",\n",
        "                        \"y'all\": \"you all\",\n",
        "                        \"y'all'd\": \"you all would\",\n",
        "                        \"y'all'd've\": \"you all would have\",\n",
        "                        \"y'all're\": \"you all are\",\n",
        "                        \"y'all've\": \"you all have\",\n",
        "                        \"you'd\": \"you would\",\n",
        "                        \"you'd've\": \"you would have\",\n",
        "                        \"you'll\": \"you will\",\n",
        "                        \"you'll've\": \"you will have\",\n",
        "                        \"you're\": \"you are\",\n",
        "                        \"you've\": \"you have\",\n",
        "                    }\n",
        "\n",
        "class PipelineElement(ABC):\n",
        "  \"\"\"\n",
        "  Abstract class for the definition of each element\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def __call__(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "class Pipeline():\n",
        "  \"\"\"\n",
        "  Pipeline class which collects pipeline elements (in the order given).\n",
        "  This class implements __call__ method so it is a callable.\n",
        "  When called it applies all the PipelineElements in the order given.\n",
        "  \"\"\"\n",
        "  def __init__(self, *args):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    *args\n",
        "      PipelineElements\n",
        "    \"\"\"\n",
        "    self.pipeline = []\n",
        "    for arg in args:\n",
        "      self.add_pipeline_element(arg)\n",
        "\n",
        "  def add_pipeline_element(self, element: PipelineElement, position: int = None):\n",
        "    \"\"\"\n",
        "    Adds a new pipeline element to the pipeline\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    element : PipelineElement\n",
        "      the element to be added to the pipeline\n",
        "    \n",
        "    position : int\n",
        "      position in the pipeline where the element should be added\n",
        "      position ranges from 0 to (n_elements - 1) where n_elements\n",
        "      is the number of elements in the pipeline.\n",
        "    Raises\n",
        "    ------\n",
        "    TypeError\n",
        "      If the type of element is not PipelineElement\n",
        "    \"\"\"\n",
        "    if not issubclass(type(element), PipelineElement):\n",
        "      raise TypeError(\"Wrong element type, only Pipeline elements subclasses can be added\")\n",
        "    if position:\n",
        "      if position >= len(self.pipeline):\n",
        "        raise ValueError(\"position index exceeds the lenght of the pipeline\")\n",
        "      self.pipeline.insert(position, element)\n",
        "    else:\n",
        "      self.pipeline.append(element)\n",
        "  \n",
        "  def pipe(self, corpus):\n",
        "    \"\"\"\n",
        "    Applies each element in the pipeline\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : list\n",
        "      list containing each document in the corpus\n",
        "    \"\"\"\n",
        "    for el in self.pipeline:\n",
        "        corpus = el(corpus)\n",
        "    return corpus\n",
        "  \n",
        "  def get_elements(self):\n",
        "    \"\"\"\n",
        "    Gives elements of the pipeline with respective index indicateing the order\n",
        "    in which elements are called\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      Where the key indicates the position of each element in the pipeline\n",
        "      (i.e. execution order, where 0 is the first element of the pipeline\n",
        "      being called) and the value indicates the actual element.\n",
        "    \"\"\"\n",
        "    res = {}\n",
        "    for idx, el in pipeline:\n",
        "      res[idx] = el\n",
        "    return res\n",
        " \n",
        "  def __call__(self, *args):\n",
        "      if args[0] == None:\n",
        "          raise ValueError(\"Need a corpus as argument\")\n",
        "      corpus = args[0]\n",
        "      return self.pipe(corpus)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(pipeline)\n",
        "        \n",
        "# Flattened Elements\n",
        "\n",
        "class UnderscoreRemoverFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Assumes the corpus is flat (i.e. the corpus is a list of documents,\n",
        "  each document is a list of words, therefore the document is not\n",
        "  divided in sentences)\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(UnderscoreRemoverFlat, self).__init__()\n",
        "\n",
        "  def remove_underscores(self, corpus):\n",
        "    \"\"\"\n",
        "    Solves the problem where some of the words are surrounded by underscores\n",
        "    (e.g. \"_hello_\")\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : list of list of list\n",
        "      corpus to be processed\n",
        "    \"\"\"\n",
        "    for doc in corpus:\n",
        "        for idx, word in enumerate(doc):\n",
        "            if \"_\" in word:\n",
        "                cleaned_word = self._clean_word(word)\n",
        "                doc[idx] = cleaned_word\n",
        "    return corpus\n",
        "\n",
        "\n",
        "  def _clean_word(self, word: str):\n",
        "    word = word.replace(\"_\", \" \")\n",
        "    # in order to remove spaces before and after the word\n",
        "    word = word.split()\n",
        "    word = \" \".join(word)\n",
        "    return word\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_underscores(corpus)\n",
        "\n",
        "class CharacterRepetitionRemoverFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Reduces repetition to two characters \n",
        "  for alphabets and to one character for punctuations.\n",
        "\n",
        "  Examples\n",
        "  --------\n",
        "  >>> reducing_character_repetitions([[\"Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\"]])\n",
        "  Really, Great !?.;:)\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(CharacterRepetitionRemoverFlat, self).__init__()\n",
        "\n",
        "  def reducing_character_repetitions(self, corpus):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "      corpus : list of list of list\n",
        "    Returns\n",
        "    -------\n",
        "    list of list\n",
        "      Formatted text with alphabets repeating to \n",
        "      two characters & punctuations limited to one repetition \n",
        "\n",
        "    \"\"\"\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "        new_doc = [self._clean_repetitions(w) for w in doc]\n",
        "        new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  # inspired by https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a\n",
        "  def _clean_repetitions(self, word):\n",
        "    # Pattern matching for all case alphabets\n",
        "    pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "\n",
        "    # Limiting all the repetitions to two characters.\n",
        "    formatted_text = pattern_alpha.sub(r\"\\1\\1\", word) \n",
        "\n",
        "    # Pattern matching for all the punctuations that can occur\n",
        "    pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "\n",
        "    # Limiting punctuations in previously formatted string to only one.\n",
        "    combined_formatted = pattern_punct.sub(r'\\1', formatted_text)\n",
        "\n",
        "    # The below statement is replacing repetitions of spaces that occur more than two times with that of one occurrence.\n",
        "    final_formatted = re.sub(' {2,}',' ', combined_formatted)\n",
        "    return final_formatted\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.reducing_character_repetitions(corpus)\n",
        "\n",
        "class ApostrophesMergerFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Merges words like \"don't\" which in the original corpus are\n",
        "  separated like: [\"don\", \"'\", \"t\"]\n",
        "\n",
        "  Examples\n",
        "  --------\n",
        "  >>> am = ApostrophesMergerFlat()\n",
        "  >>> am([[\"I\", \"'\", \"ve\", \"a\", \"pair\", \"of\", \"shoes\"]])\n",
        "  [[\"I've\", \"a\", \"pair\", \"of\", \"shoes\"]]\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ApostrophesMergerFlat, self).__init__()\n",
        "\n",
        "  def merge_apostrophes(self, corpus):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : list of list of list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of list\n",
        "      Formatted text where contractions are merged into one single word\n",
        "    \n",
        "    \"\"\"\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      indexes = self._get_neg_indexes(doc)\n",
        "      for el in indexes:\n",
        "        doc[el[0]:el[1]] = [\"\".join(doc[el[0]:el[1]])]\n",
        "      new_corpus.append(doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def _get_neg_indexes(self, sent):\n",
        "\n",
        "    # s not considered because contraction can be either \"is\", genitive or \"has\"\n",
        "    contr = [\"t\", \"ve\", \"re\", \"ll\", \"d\", \"all\", \"y\", \"cause\", \"m\", \"clock\", \"am\"] #, \"s\"]\n",
        "    indexes = []\n",
        "    for idx, word in enumerate(sent):\n",
        "      # Try-except to avoid out of range indexes (there can be some \"'\" a the beginning or end of the phrase)\n",
        "      try:\n",
        "        if word==\"'\" and sent[idx+1] in contr:\n",
        "          indexes.append((idx-1,idx+2))\n",
        "      except:\n",
        "        pass\n",
        "    return indexes\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.merge_apostrophes(corpus)\n",
        "\n",
        "\n",
        "class ContractionCleanerFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Clean all contractions by using a predifined contraction map\n",
        "\n",
        "  Example\n",
        "  -------\n",
        "  >>> cc = ContractionCleanerFlat\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(ContractionCleanerFlat, self).__init__()\n",
        "\n",
        "  def clean_contractions(self, corpus):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : list of list of list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of list\n",
        "      Formatted text where contractions are merged into one single word\n",
        "\n",
        "    \"\"\"\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for word in doc:\n",
        "        try:\n",
        "            correct = CONTRACTION_MAP[word]\n",
        "            correct = correct.split()\n",
        "            new_doc += correct\n",
        "        except:\n",
        "            new_doc.append(word)\n",
        "      new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_contractions(corpus)\n",
        "\n",
        "class SpecialCharsCleanerFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Removes all special characters which are not part of\n",
        "  the folllowing regex pattern: \"[^a-zA-Z0-9:€$-,%?!]+\"\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(SpecialCharsCleanerFlat, self).__init__()\n",
        "\n",
        "  def clean_special_chars(self, corpus):\n",
        "    new_corpus = [[self._clean_special_word(w) for w in doc] for doc in corpus]\n",
        "    new_corpus = [[w for w in doc] for doc in corpus]\n",
        "    return new_corpus\n",
        "    \n",
        "  def _clean_special_word(self, word):\n",
        "    # The formatted text after removing not necessary punctuations.\n",
        "    formatted_text = re.sub(r\"[^a-zA-Z0-9:€$-,%?!]+\", '', word) \n",
        "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
        "    return formatted_text\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_special_chars(corpus)\n",
        "\n",
        "class StopWordsRemoverFlat(PipelineElement):\n",
        "  \"\"\"\n",
        "  Removes stopwords from the document.\n",
        "  It doesn't remove stopwords that contain negations\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(StopWordsRemoverFlat, self).__init__()\n",
        "\n",
        "  def remove_stop_words(self, corpus):\n",
        "    stops = stopwords.words(\"english\")\n",
        "    stops = [word for word in stops if \"'t\" not in word or \"not\" not in word]\n",
        "    return [[word for word in doc if word not in stops] for doc in corpus]\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_stop_words(corpus)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# Non-flattened elements\n",
        "\"\"\"\n",
        "These items are the same as before with the only difference that now\n",
        "the assumption is that the corpus is not flattened, so each document\n",
        "is composed by serveral separated sentences:\n",
        "\n",
        "Example\n",
        "-------\n",
        "\n",
        "\"\"\"\n",
        "class UnderscoreRemover(PipelineElement):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(UnderscoreRemover, self).__init__()\n",
        "\n",
        "  def remove_underscores(self, corpus):\n",
        "    \"\"\"\n",
        "    Solves the problem where some of the words are surrounded by underscores\n",
        "    (e.g. \"_hello_\")\n",
        "    \"\"\"\n",
        "    for doc in corpus:\n",
        "      for sent_idx, sent in enumerate(doc):\n",
        "        new_sent = []\n",
        "        for idx, word in enumerate(sent):\n",
        "          if \"_\" in word:\n",
        "            cleaned_word = self._clean_word(word)\n",
        "            new_sent += cleaned_word\n",
        "          else:\n",
        "            new_sent.append(word)\n",
        "        if len(new_sent) > 0:\n",
        "          doc[sent_idx] = new_sent\n",
        "    return corpus\n",
        "\n",
        "  def _clean_word(self, word: str):\n",
        "    word = word.replace(\"_\", \" \")\n",
        "    # remove spaces before and after the word\n",
        "    word = word.split()\n",
        "    return word\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_underscores(corpus)\n",
        "\n",
        "\n",
        "\n",
        "class CharacterRepetitionRemover(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(CharacterRepetitionRemover, self).__init__()\n",
        "\n",
        "  def reducing_character_repetitions(self, corpus):\n",
        "      new_corpus = [[[self._clean_repetitions(w) for w in sent] for sent in doc] for doc in corpus]\n",
        "      return new_corpus\n",
        "  \n",
        "  # inspired by https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a\n",
        "  def _clean_repetitions(self, word):\n",
        "    \"\"\"\n",
        "    This Function will reduce repetition to two characters \n",
        "    for alphabets and to one character for punctuations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        word: str                \n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Finally formatted text with alphabets repeating to \n",
        "        one characters & punctuations limited to one repetition \n",
        "        \n",
        "    Example:\n",
        "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
        "    Output : Really, Great !?.;:)\n",
        "\n",
        "    \"\"\"\n",
        "    # Pattern matching for all case alphabets\n",
        "    pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "\n",
        "    # Limiting all the repetitions to two characters.\n",
        "    # MODIFIED: keep only one repetition of the character\n",
        "    formatted_text = pattern_alpha.sub(r\"\\1\\1\", word) \n",
        "\n",
        "    # Pattern matching for all the punctuations that can occur\n",
        "    pattern_punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "\n",
        "    # Limiting punctuations in previously formatted string to only one.\n",
        "    combined_formatted = pattern_punct.sub(r'\\1', formatted_text)\n",
        "\n",
        "    # The below statement is replacing repetitions of spaces that occur more than two times with that of one occurrence.\n",
        "    final_formatted = re.sub(' {2,}',' ', combined_formatted)\n",
        "    return final_formatted\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.reducing_character_repetitions(corpus)\n",
        "\n",
        "\n",
        "class ApostrophesMerger(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(ApostrophesMerger, self).__init__()\n",
        "\n",
        "  def merge_apostrophes(self, corpus):\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for sent in doc:\n",
        "        indexes = self._get_neg_indexes(sent)\n",
        "        for el in indexes:\n",
        "          sent[el[0]:el[1]] = [\"\".join(sent[el[0]:el[1]])]\n",
        "        new_doc.append(sent)\n",
        "      new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def _get_neg_indexes(self, sent):\n",
        "    contr = [\"t\", \"ve\", \"re\", \"ll\", \"d\", \"all\", \"y\", \"cause\", \"m\", \"clock\", \"am\"]#, \"s\"]\n",
        "    indexes = []\n",
        "    for idx, word in enumerate(sent):\n",
        "      # Try-except to avoid out of range indexes (there can be some \"'\" a the beginning or end of the phrase)\n",
        "      try:\n",
        "        if word==\"'\" and sent[idx+1] in contr:\n",
        "          indexes.append((idx-1,idx+2))\n",
        "      except:\n",
        "        pass\n",
        "      return indexes\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.merge_apostrophes(corpus)\n",
        "\n",
        "\n",
        "class ContractionCleaner(PipelineElement):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ContractionCleaner, self).__init__()\n",
        "\n",
        "  def clean_contractions(self, corpus):\n",
        "    new_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for sent in doc:\n",
        "        new_sent = []\n",
        "        for word in sent:\n",
        "          try:\n",
        "            correct = CONTRACTION_MAP[word]\n",
        "            correct = correct.split()\n",
        "            new_sent += correct\n",
        "          except:\n",
        "            new_sent.append(word)\n",
        "        new_doc.append(new_sent)\n",
        "      new_corpus.append(new_doc)\n",
        "    return new_corpus\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_contractions(corpus)\n",
        "\n",
        "\n",
        "class SpecialCharsCleaner(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(SpecialCharsCleaner, self).__init__()\n",
        "  \n",
        "  def clean_special_chars(self, corpus):\n",
        "      for idx_doc, doc in enumerate(corpus):\n",
        "        for sent_idx, sent in enumerate(doc):\n",
        "          new_sent = []\n",
        "          for word_idx, word in enumerate(sent):\n",
        "            new_word = self._clean_special_word(word)\n",
        "            if new_word != \" \":\n",
        "              new_sent += new_word.split()\n",
        "          if len(new_sent) > 0:\n",
        "            doc[sent_idx] = new_sent\n",
        "      return corpus\n",
        "    \n",
        "  def _clean_special_word(self, word):\n",
        "    # The formatted text after removing not necessary punctuations.\n",
        "    formatted_text = re.sub(r\"[^a-zA-Z0-9:€$-,%?!]+\", ' ', word) \n",
        "    return formatted_text\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.clean_special_chars(corpus)\n",
        "\n",
        "class StopWordsRemover(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super(StopWordsRemover, self).__init__()\n",
        "\n",
        "  def remove_stop_words(self, corpus):\n",
        "    stops = stopwords.words(\"english\")\n",
        "    # Don't want to remove stop words associated with negations\n",
        "    stops = [word for word in stops if \"'t\" not in word or \"not\" not in word]\n",
        "    return [[[word for word in sent if word not in stops] for sent in doc] for doc in corpus]\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_stop_words(corpus)\n",
        "\n",
        "class ShallowObjectiveSentsRemover(PipelineElement):\n",
        "  def __init__(self, threshold = .5, clf = MultinomialNB, trained = False):\n",
        "    self.vectorizer = CountVectorizer()\n",
        "    self.classifier = clf()\n",
        "    if not trained:\n",
        "      self.best_estimator = self._train()\n",
        "    else:\n",
        "      self.best_estimator = self.classifier\n",
        "  \n",
        "  def _train(self):\n",
        "    subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "    obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
        "\n",
        "    corpus = [self.neg_marking_list2str(d) for d in subj] + [self.neg_marking_list2str(d) for d in obj]\n",
        "    vectors = self.vectorizer.fit_transform(corpus)\n",
        "    labels = np.array([1] * len(subj) + [0] * len(obj))\n",
        "    scores = cross_validate(self.classifier, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['accuracy'], return_estimator=True)\n",
        "    estimator = scores[\"estimator\"][scores[\"test_accuracy\"].argmax()]\n",
        "    return estimator\n",
        "\n",
        "  def neg_marking_list2str(self, sent):\n",
        "    # takes the doc and produces a single list\n",
        "    # negates the whole document\n",
        "    negated_doc = mark_negation(sent, double_neg_flip=True)\n",
        "    return \" \".join([w for w in negated_doc])\n",
        "    \n",
        "\n",
        "  def remove_objective_sents(self, corpus):\n",
        "    transformed_corpus = [[self.vectorizer.transform([self.neg_marking_list2str(sent)]) for sent in doc] for doc in corpus]\n",
        "    res = [[corpus[doc_idx][sent_idx] for sent_idx, sent in enumerate(doc) if self.best_estimator.predict(sent).item()]\n",
        "           for doc_idx, doc in enumerate(transformed_corpus)]\n",
        "    return res\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_objective_sents(corpus)\n",
        "\n",
        "class Flattener(PipelineElement):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  \n",
        "  def flatten(self, corpus):\n",
        "    corpus = [[w for sent in doc for w in sent] for doc in corpus]\n",
        "    return corpus\n",
        "\n",
        "  def __call__(self, corpus):\n",
        "    return self.flatten(corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skv2-rEQwMtR"
      },
      "source": [
        "### Corpus class\n",
        "I'm going to create a class for the representation of the corpus in order to have a self contained way to have all the information about corpus attributes (vocab, words ....)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dw31aob2EyxL"
      },
      "outputs": [],
      "source": [
        "class CorpusBase(ABC):\n",
        "  \"\"\"\n",
        "  Base class for representing corpus\n",
        "  \"\"\"\n",
        "  def __init__(self, preprocess_pipeline = None, corpus = None):\n",
        "    if corpus == None:\n",
        "      self.unprocessed_corpus, self.labels = self._get_corpus()\n",
        "    else:\n",
        "      self.unprocessed_corpus, self.labels = corpus[0], corpus[1]\n",
        "    self.pipeline = preprocess_pipeline\n",
        "    if preprocess_pipeline == None:\n",
        "      self.processed_corpus = self._preprocess()\n",
        "    else:\n",
        "      # Flattened and preprocessed corpus\n",
        "      self.processed_corpus = self._preprocess()\n",
        "\n",
        "    self.vocab = self._create_vocab()\n",
        "  \n",
        "  @abstractmethod\n",
        "  def get_indexed_corpus(self):\n",
        "    \"\"\"\n",
        "    Method to return the corpus where each word is replaced\n",
        "    with the index of the word itself in the vocabulary.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def _preprocess(self):\n",
        "    \"\"\"\n",
        "    Preprocess the unprocessed corpus using the pipeline object, if it is not None.\n",
        "    Otherwise, return the unprocessed corpus as it is.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      The processed corpus.\n",
        "    \"\"\"\n",
        "    if self.pipeline != None:\n",
        "      return self.pipeline(self.unprocessed_corpus)\n",
        "    else:\n",
        "      return self.unprocessed_corpus\n",
        "  \n",
        "\n",
        "  def _get_corpus(self):\n",
        "    \"\"\"\n",
        "    Get the movie reviews corpus and their labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "      A tuple containing the list of movie reviews and the list of labels.\n",
        "    \"\"\"\n",
        "    mr = movie_reviews\n",
        "    neg = mr.paras(categories = \"neg\")\n",
        "    pos = mr.paras(categories = \"pos\")\n",
        "    labels = [0] * len(pos) + [1] * len(neg)\n",
        "    return neg + pos, labels\n",
        "    \n",
        "  def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "    \"\"\"\n",
        "    Create an embedding matrix from a given embedding dictionary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embedding : dict\n",
        "        A dictionary mapping vocabulary items to their embeddings.\n",
        "    embedding_dim : int\n",
        "        The dimensionality of the embeddings.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      A matrix of size (len(self.vocab), embedding_dim) containing the embeddings for the vocabulary items.\n",
        "    \"\"\"\n",
        "    matrix_length = len(self.vocab)\n",
        "    embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "    # If I use torch.zeros directly it crashes (don't know why)\n",
        "    embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "    null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "    for idx, key in enumerate(self.vocab.keys()):\n",
        "      if torch.equal(embedding[key], null_embedding):\n",
        "        embedding_matrix[idx] = torch.randn(embedding_dim)\n",
        "      else:\n",
        "        embedding_matrix[idx] = embedding[key]\n",
        "            \n",
        "    return embedding_matrix\n",
        "\n",
        "  def _create_vocab(self):\n",
        "    \"\"\"\n",
        "    Create a vocabulary from the processed corpus.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      A dictionary mapping vocabulary items to their frequencies in the corpus.\n",
        "    \"\"\"\n",
        "    vocab = dict()\n",
        "    corpus_words = [w for doc in self.processed_corpus for sent in doc for w in sent]\n",
        "    for word in corpus_words:\n",
        "      try:\n",
        "        vocab[word] += 1\n",
        "      except:\n",
        "        vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.processed_corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C3PyvlOV60V7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "import numpy as np\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "\n",
        "class MovieReviewsCorpus(CorpusBase):\n",
        "  def __init__(self, preprocess_pipeline = None):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    flat : bool\n",
        "      if true flattens each document by removing the sentece division\n",
        "    \"\"\"\n",
        "    # Corpus as list of documents. Documents as list of sentences. Sentences as list of tokens\n",
        "    super().__init__(preprocess_pipeline)\n",
        "    # Flattening corpus\n",
        "    self.processed_corpus = [[w for sent in doc for w in sent] for doc in self.processed_corpus]\n",
        "  \n",
        "  def get_indexed_corpus(self):\n",
        "    \"\"\"\n",
        "\n",
        "    Returns the corpus as tensor of indexes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list(torch.tensor(torch.tensor))\n",
        "      The corpus represented as indexes corresponding to each word\n",
        "    \n",
        "    list\n",
        "      labels associated with each document\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    for idx, key in enumerate(self.vocab.keys()):\n",
        "      vocab[key] = idx\n",
        "\n",
        "    indexed_corpus = [torch.tensor([torch.tensor(vocab[w], dtype=torch.int32) for w in doc]) for doc in self.processed_corpus]\n",
        "    return indexed_corpus, self.labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zZaF5yV9rlo-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class MovieReviewsDataset(Dataset):\n",
        "  def __init__(self, raw_dataset):\n",
        "    super(MovieReviewsDataset, self).__init__()\n",
        "    self.corpus = raw_dataset[0]\n",
        "    self.targets = raw_dataset[1]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    item = self.corpus[index]\n",
        "    label = self.targets[index]\n",
        "    return (item, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr7PPG1k4gFq"
      },
      "source": [
        "### Create the model class\n",
        "Let's first try with a simple BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bCvqaJ8-hKmT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, embedding_matrix = None, device = \"cuda\", input_size = 300, hidden_size = 128, output_size = 2):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    if embedding_matrix != None:\n",
        "      self.embedding = self.create_embedding_layer(embedding_matrix)\n",
        "    else:\n",
        "      self.embedding = None\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True, bidirectional=True)\n",
        "    self.fc = nn.Sequential(nn.ReLU(),\n",
        "                            nn.BatchNorm1d(hidden_size*2, eps = 1e-08),\n",
        "                            nn.Dropout(0.2),\n",
        "                            nn.Linear(hidden_size*2, output_size)\n",
        "                            )\n",
        "\n",
        "  def create_embedding_layer(self, embedding_matrix):\n",
        "    \"\"\"\n",
        "    Create an embedding layer from a given embedding matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embedding_matrix : torch.Tensor\n",
        "      A matrix containing the embeddings for the vocabulary items.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.nn.Module\n",
        "      An embedding layer that maps vocabulary items to their embeddings.\n",
        "    \"\"\"\n",
        "    num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim, -1)\n",
        "    emb_layer.load_state_dict({\"weight\": embedding_matrix})\n",
        "    return emb_layer\n",
        "\n",
        "  # function taken from https://discuss.pytorch.org/t/how-to-use-pack-sequence-if-we-are-going-to-use-word-embedding-and-bilstm/28184/4\n",
        "  def simple_elementwise_apply(self, fn, packed_sequence):\n",
        "    \"\"\"applies a pointwise function fn to each element in packed_sequence\"\"\"\n",
        "    return torch.nn.utils.rnn.PackedSequence(fn(packed_sequence.data), packed_sequence.batch_sizes)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    \"\"\"\n",
        "    Initialize the hidden state of the LSTM.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch_size : int\n",
        "      The batch size of the input data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple of torch.Tensor\n",
        "      A tuple containing two zero tensors with shape (2, batch_size, self.hidden_size) representing the hidden state of the LSTM. If the model is using CUDA, the tensors will be moved to the device.\n",
        "    \"\"\"\n",
        "    if self.cuda:\n",
        "      return (torch.zeros(2, batch_size, self.hidden_size).to(self.device),\n",
        "              torch.zeros(2, batch_size, self.hidden_size).to(self.device),)\n",
        "\n",
        "  def common(self, x):\n",
        "    \"\"\"\n",
        "    Perform common processing on the input data for each subclass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.nn.utils.rnn.PackedSequence\n",
        "      input data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor, list of int\n",
        "      A tensor containing the processed data, and a list of the lengths of the sequences in the original packed sequence.\n",
        "    \"\"\"\n",
        "    batch_size = x.batch_sizes[0].item()\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "\n",
        "    if self.embedding != None:\n",
        "      x = self.simple_elementwise_apply(self.embedding, x)\n",
        "\n",
        "    # output: batch_size, sequence_length, hidden_size * 2 (since is bilstm)\n",
        "    out, _ = self.lstm(x, hidden)\n",
        "    out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "\n",
        "    return out, input_sizes\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.batch_sizes[0].item()\n",
        "    out, input_sizes = self.common(x)\n",
        "    # Interested only in the last layer\n",
        "    out = out[list(range(batch_size)), input_sizes - 1, :]\n",
        "    out = self.fc(out)\n",
        "    out = out.squeeze()\n",
        "    return out\n",
        "\n",
        "class Residual(nn.Module):\n",
        "  \"\"\"\n",
        "  Residual block module\n",
        "  \"\"\"\n",
        "  def __init__(self, in_size, out_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = nn.Sequential(nn.ReLU(),\n",
        "                              nn.Linear(in_size, in_size)\n",
        "                              )\n",
        "    self.fc2 = nn.Sequential(nn.ReLU(),\n",
        "                              nn.Linear(in_size, in_size),\n",
        "                              nn.ReLU()\n",
        "                              )\n",
        "\n",
        "    self.fc3 = nn.Linear(in_size, out_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    lay1 = self.fc1(x)\n",
        "    lay2 = self.fc2(lay1) + x\n",
        "    out = self.fc3(lay2)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class BiLSTMAttention(BiLSTM):\n",
        "  \"\"\"\n",
        "  BiLSTM with attention inspired by the following paper: https://aclanthology.org/S18-1040.pdf\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_matrix = None, device=\"cuda\", input_size=300,\n",
        "                hidden_size=128, context_size = None, output_size=2):\n",
        "    super().__init__(embedding_matrix, device, input_size, hidden_size, output_size)\n",
        "    if context_size:\n",
        "      # Not self attention :)\n",
        "      self.attention = nn.Linear(self.hidden_size * 2, context_size)\n",
        "      self.history = nn.Parameter(torch.randn(context_size))\n",
        "    else:\n",
        "      self.attention = nn.Linear(self.hidden_size * 2, 1)\n",
        "      self.history = None\n",
        "    \n",
        "    self.fc = Residual(hidden_size*2, output_size)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out, input_sizes = super().common(x)\n",
        "\n",
        "    if self.history != None:\n",
        "      attention_values = torch.tanh(self.attention(out))\n",
        "      attention_weights = torch.softmax(attention_values.matmul(self.history), dim = 1).unsqueeze(1)\n",
        "      # n_docs, sequence_length\n",
        "    else:\n",
        "      attention_values = torch.tanh(self.attention(out)).squeeze(dim = 2)\n",
        "      attention_weights = torch.softmax(attention_values, dim = 1).unsqueeze(1)\n",
        "      # n_docs, sequence_length\n",
        "\n",
        "    out = torch.sum(attention_weights.matmul(out), dim = 1)\n",
        "\n",
        "    out = self.fc(out)\n",
        "\n",
        "    attention_weights = attention_weights.squeeze(dim = 1)\n",
        "    att = [doc[:input_sizes[idx]] for idx, doc in enumerate(attention_weights)]\n",
        "\n",
        "    return out, att\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yZFg9BvQdZHB"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  \"\"\"\n",
        "  Perform a training step on a given model using a given data loader.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  net : torch.nn.Module\n",
        "    The model to be trained.\n",
        "  data_loader : torch.utils.data.DataLoader\n",
        "    A data loader providing the training data.\n",
        "  optimizer : torch.optim.Optimizer\n",
        "    The optimizer to be used for training.\n",
        "  cost_function : function\n",
        "    loss function.\n",
        "  device : str, optional\n",
        "    The device on which to perform the training, by default 'cuda'\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float, float\n",
        "    The average loss and accuracy over the training data.\n",
        "  \"\"\"\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for batch_idx, (inputs, targets, _) in enumerate(data_loader):\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    in_size = targets.size(dim=0)\n",
        "\n",
        "    outputs, _ = net(inputs)\n",
        "\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LJef4h1cfWym"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def print_confusion_matrix(y_true, y_pred):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  plt.imshow(cm, cmap=plt.cm.Blues, interpolation='None', aspect='auto')\n",
        "\n",
        "  plt.colorbar()\n",
        "\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.ylabel('True label')\n",
        "\n",
        "  plt.xticks([0, 1])\n",
        "  plt.yticks([0, 1])\n",
        "\n",
        "  plt.gca().set_xticklabels(['Positive', 'Negative'])\n",
        "  plt.gca().set_yticklabels(['Positive', 'Negative'])\n",
        "  for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "      text = plt.text(j, i, cm[i, j],\n",
        "                      ha=\"center\", va=\"center\", color=\"black\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device = 'cuda', conf = False):\n",
        "  \"\"\"\n",
        "  Perform a test step on a given model using a given data loader.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  net : torch.nn.Module\n",
        "    The model to be tested.\n",
        "  data_loader : torch.utils.data.DataLoader\n",
        "    A data loader providing the test data.\n",
        "  cost_function : function\n",
        "    loss function\n",
        "  device : str, optional\n",
        "    The device on which to perform the testing, by default 'cuda'\n",
        "  conf : bool\n",
        "    Prints the confusion matrix if true\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float, float, sklearn.metrics.confusion_matrix\n",
        "      The average loss and accuracy over the test data, plus the confusion matrix.\n",
        "  \"\"\"\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  if conf:\n",
        "    total_result = []\n",
        "    total_ground_truth = []\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, targets, _) in enumerate(data_loader):\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      in_size = targets.size(dim=0)\n",
        "\n",
        "      outputs, _ = net(inputs)\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = outputs.max(dim=1)\n",
        "\n",
        "      if conf:\n",
        "        total_result.extend(predicted.tolist())\n",
        "        total_ground_truth.extend(targets.tolist())\n",
        "\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "    \n",
        "    if conf:\n",
        "      print_confusion_matrix(total_ground_truth, total_result)\n",
        "\n",
        "\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OOeUmEHZNBvw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim import RAdam\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "def main(train_loader, test_loader, embedding_matrix, device = \"cuda\", epochs = 10):\n",
        "  \"\"\"\n",
        "  Train and evaluate a BiLSTMAttention model on a given dataset.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  train_loader : torch.utils.data.DataLoader\n",
        "    A data loader providing the training data.\n",
        "  test_loader : torch.utils.data.DataLoader\n",
        "    A data loader providing the test data.\n",
        "  embedding_matrix : torch.Tensor\n",
        "    An embedding matrix having the embedding of each word for the used vocabulary.\n",
        "  device : str, optional\n",
        "    The device, default \"cuda\"\n",
        "  epochs : int, optional\n",
        "    The number of epochs to train the model for, default 10.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    The test accuracy of the trained model.\n",
        "  \"\"\"\n",
        "  net = BiLSTMAttention(embedding_matrix, device = device, input_size=300).to(device)\n",
        "\n",
        "  optimizer = Adam(net.parameters(), 0.001, betas = (0.9, 0.999), amsgrad=True)\n",
        "\n",
        "  cost_function = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Empirical result in this scenario:\n",
        "  # Even if I am using an adaptive learning rate, the schduler has been shown to guarantee\n",
        "  # a more stable convergence (more stable results across folds in k-fold)\n",
        "  scheduler = ExponentialLR(optimizer, 0.8)\n",
        "\n",
        "  flag = False\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    if e == epochs - 1:\n",
        "      test_loss, test_accuracy = test_step(net, test_loader, cost_function, device, True)\n",
        "    else:\n",
        "      test_loss, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "    print(f\"Test loss: {test_loss} \\n Test accuracy: {test_accuracy}\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    if train_accuracy == 100:\n",
        "      if flag:\n",
        "        break\n",
        "      else:\n",
        "        flag = True\n",
        "    scheduler.step()\n",
        "  _, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "\n",
        "  return test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GfNtrS8jmATx"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def pad(batch: List[torch.tensor], max_size: int):\n",
        "  \"\"\"\n",
        "  Pads elements in the batch in order to have the same length,\n",
        "  that is the length of the longest element in the sequence\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  batch : list of nn.tensor\n",
        "    batch of elements. Each sequence of the batch can be either a tensor\n",
        "    containing indexes (i.e. [2 423 1 ... 123] where each number correspond to\n",
        "    one entry in a vocabulary) or can be a tensor containing directly the embeddings\n",
        "    (i.e. [[embedding_word1], [embedding_word2], ..., [embedding_wordn]])\n",
        "  \n",
        "  max_size : int\n",
        "    size of the longest sequence in the batch\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  list of torch.tensor\n",
        "    Batch where all elements are padded\n",
        "\n",
        "  \"\"\"\n",
        "  try:\n",
        "    pad = torch.tensor([-1]*batch[0].size(dim=1), dtype = torch.float).to(\"cuda\")\n",
        "    embedded = 1\n",
        "  except:\n",
        "    pad = torch.tensor([-1])\n",
        "    embedded = 0\n",
        "  for idx in range(len(batch)):\n",
        "      remaining = max_size - batch[idx].size(dim = 0)\n",
        "      abc = pad.repeat(remaining)\n",
        "      if embedded:\n",
        "        batch[idx] = torch.cat((batch[idx], pad.repeat(remaining, 1)), dim = 0)\n",
        "      else:\n",
        "        batch[idx] = torch.cat((batch[idx], pad.repeat(remaining)), dim = 0)\n",
        "  return batch\n",
        "\n",
        "def batch_to_tensor(X: List[torch.tensor], max_size):\n",
        "  \"\"\"\n",
        "  Transforms the entire batch into a tensor\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  X : list of torch.tensor\n",
        "    already padded batch\n",
        "\n",
        "  max_size : int\n",
        "    maximum size of the sequences\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  torch.tensor\n",
        "    Batch in tensor type\n",
        "  \"\"\"\n",
        "  try:\n",
        "    X_tensor = torch.zeros((len(X), max_size, X[0].size(dim=1)), dtype=torch.float).to(\"cuda\")\n",
        "  except:\n",
        "    X_tensor = torch.zeros((len(X), max_size), dtype=torch.int32)\n",
        "\n",
        "  for i, embed in enumerate(X):\n",
        "      X_tensor[i] = embed\n",
        "  return X_tensor\n",
        "\n",
        "def sort_ds(X, Y):\n",
        "  \"\"\"\n",
        "  Sort inputs by document lengths\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  X : list of torch.tensor\n",
        "    The batch\n",
        "  Y : list\n",
        "    Labels\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "    batch sorted, labels sorted (in order to keep correspondances),\n",
        "    document lengths sorted, indexes resulting from the argsort \n",
        "  \"\"\"\n",
        "  document_lengths = np.array([tens.size(dim = 0) for tens in X])\n",
        "  indexes = np.argsort(document_lengths)\n",
        "  document_lengths = document_lengths.tolist()\n",
        "\n",
        "  X_sorted = [X[idx] for idx in indexes][::-1]\n",
        "  Y_sorted = [Y[idx] for idx in indexes][::-1]\n",
        "  document_lengths = torch.tensor([document_lengths[idx] for idx in indexes][::-1])\n",
        "\n",
        "  return X_sorted, Y_sorted, document_lengths, indexes\n",
        "\n",
        "def collate(batch):\n",
        "  \"\"\"\n",
        "  collate function for batch of corpus\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "    packed sequence for the batch, tensor of labels, indexes for original\n",
        "    position of the elements (used for lbsa method)\n",
        "  \"\"\"\n",
        "  X, Y = list(zip(*batch))\n",
        "  # Sort dataset\n",
        "  X, Y, document_lengths, indexes = sort_ds(X, Y)\n",
        "\n",
        "  # Get tensor sizes\n",
        "  max_size = torch.max(document_lengths).item()\n",
        "\n",
        "  # Pad tensor each element\n",
        "  X = pad(X, max_size)\n",
        "\n",
        "  # Transform the batch to a tensor\n",
        "  X_tensor = batch_to_tensor(X, max_size)\n",
        "  Y_tensor = torch.tensor(Y)\n",
        "  # Return the padded sequence object\n",
        "  X_final = pack_padded_sequence(X_tensor, document_lengths, batch_first=True)\n",
        "  return X_final, Y_tensor, indexes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c-flsP1XPxxI"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size: int, dataset, collate_fn, random_state = 42):\n",
        "  \"\"\"\n",
        "  Performs a stratified random split of the dataset using a 80/20 ratio.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "    training set data loader, test set data loader\n",
        "  \"\"\"\n",
        "  train_indexes, test_indexes = train_test_split(list(range(len(dataset.targets))), test_size = 0.2,\n",
        "                                                  stratify = dataset.targets, random_state = random_state)\n",
        "\n",
        "  train_ds = Subset(dataset, train_indexes)\n",
        "  test_ds = Subset(dataset, test_indexes)\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "  test_loader = DataLoader(test_ds, batch_size = batch_size, collate_fn = collate_fn, pin_memory=True)\n",
        "\n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkRBKES24_0v"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "def main_cross_validation(main_fn, dataset, embedding_matrix, collate_fn,\n",
        "                          device = \"cuda\", epochs = 10, random_state = 42, batch_size = 32):\n",
        "  \"\"\"\n",
        "  Perform cross-validation on a given dataset using a given model training and evaluation function.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  main_fn : function\n",
        "    A function that trains and evaluates a model on a given dataset.\n",
        "  dataset : torch.utils.data.Dataset\n",
        "    The dataset to be used for cross-validation.\n",
        "  embedding_matrix : torch.Tensor\n",
        "    An embedding matrix to use for the model.\n",
        "  collate_fn : function\n",
        "    A function that takes a list of samples and returns a single batch as a tensor.\n",
        "  device : str, optional\n",
        "    The device on which to perform the training and testing, by default \"cuda\"\n",
        "  epochs : int, optional\n",
        "    The number of epochs to train the model for, by default 10\n",
        "  random_state : int, optional\n",
        "    The random seed to use for shuffling and splitting the data, by default 42\n",
        "  batch_size : int, optional\n",
        "    The batch size to use, by default 32\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float, float\n",
        "      The mean and standard deviation of the model's accuracy across the folds.\n",
        "  \"\"\"\n",
        "  targets = np.asarray(dataset.targets, dtype=np.int64)\n",
        "\n",
        "  skf = StratifiedKFold(10, shuffle = True, random_state=random_state)\n",
        "\n",
        "  fold_accuracies = []\n",
        "  \n",
        "  for fold, (train_indexes, val_indexes) in enumerate(skf.split(np.zeros(len(dataset)),\n",
        "                                                      targets)):\n",
        "    print(f\"\\n Fold: {fold}\")\n",
        "    train_sampler = SubsetRandomSampler(train_indexes)\n",
        "    val_sampler = SubsetRandomSampler(val_indexes)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size = batch_size, sampler = train_sampler,\n",
        "                              collate_fn = collate_fn, pin_memory=True)\n",
        "    val_loader = DataLoader(dataset, batch_size = batch_size, sampler = val_sampler,\n",
        "                            collate_fn = collate_fn, pin_memory = True)\n",
        "\n",
        "\n",
        "    val_accuracy = main_fn(train_loader, val_loader, embedding_matrix, device, epochs)\n",
        "    \n",
        "    fold_accuracies.append(val_accuracy)\n",
        "\n",
        "\n",
        "  fold_accuracies = np.array(fold_accuracies)\n",
        "\n",
        "  return fold_accuracies.mean(), fold_accuracies.std()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkeZyHH15r6K"
      },
      "source": [
        "## Lexicon Based Supervised Attention Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9gmVtdg_Gf1"
      },
      "outputs": [],
      "source": [
        "class MovieReviewsCorpusLBSA(CorpusBase):\n",
        "  def __init__(self, preprocess_pipeline = None, corpus = None):\n",
        "      \"\"\"\n",
        "      If non preprocess_pipeline is given, the text gets tokenized by default\n",
        "      using spacy tokenizer.\n",
        "      \"\"\"\n",
        "      super().__init__(preprocess_pipeline=preprocess_pipeline, corpus=corpus)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  def get_indexed_corpus(self):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      Dictionary\n",
        "          Containing correspondences word -> index\n",
        "      \n",
        "      list(int)\n",
        "          labels associated with each document\n",
        "      \"\"\"\n",
        "      vocab = {}\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "          vocab[key] = idx\n",
        "      \n",
        "      # each doc is a list of tensor which represent sentences, each sentence is a tensor of indexed words\n",
        "      indexed_corpus = [[torch.tensor([vocab[w] for w in sent], dtype=torch.int32) \n",
        "                        for sent in doc]\n",
        "                        for doc in self.processed_corpus]\n",
        "      return indexed_corpus, self.labels\n",
        "  \n",
        "  \n",
        "\n",
        "c = MovieReviewsCorpusLBSA()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lGBLNBxo4Jv"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "\n",
        "class MovieReviewsDatasetLBSA(Dataset):\n",
        "  def __init__(self, corpus):\n",
        "    super(MovieReviewsDatasetLBSA, self).__init__()\n",
        "    self.corpus = corpus\n",
        "    indexed_corpus = self.corpus.get_indexed_corpus()\n",
        "    # Word level gold attention vector\n",
        "    self.word_lambda = 3\n",
        "    self.sentence_lambda = 3\n",
        "    self.sentiment_degree = self._compute_sentiment_degree()\n",
        "    self.wl_gold_av = self._compute_gold_words()\n",
        "    self.sl_gold_av = self._compute_gold_sents()\n",
        "    self.data = indexed_corpus[0]\n",
        "    self.targets = indexed_corpus[1]\n",
        "  \n",
        "  def _compute_sentiment_degree(self):\n",
        "    senti_vocab = self._build_senti_vocab(self.corpus.vocab)\n",
        "    path = '/content/gdrive/My Drive/nlu-project/lexicons/'\n",
        "    mpqa_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'mpqa/mpqa.json')\n",
        "    bingliu_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'bingliu/bingliu.json')\n",
        "    inquirer_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'inquirer/inquirer.json')\n",
        "    concreteness_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'concreteness/concreteness.json')\n",
        "    twitter_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'twitter/twitter.json')\n",
        "    qwn_vocab = self._build_0_1_vocab(self.corpus.vocab, path + 'qwn/qwn.json')\n",
        "    social_sent = self._build_social_sent_vocab(self.corpus.vocab, path + 'social_sent')\n",
        "    senticnet_vocab = self.build_sentic_net_vocab(self.corpus.vocab, path + \"sentic_net/senticnet.txt\")\n",
        "    social_sent = self._build_social_sent_vocab(self.corpus.vocab, path + \"social_sent\")\n",
        "    res = self._compute_average_sentiment_degree(senti_vocab,\n",
        "                                                 mpqa_vocab,\n",
        "                                                 bingliu_vocab,\n",
        "                                                 inquirer_vocab,\n",
        "                                                 concreteness_vocab,\n",
        "                                                 twitter_vocab,\n",
        "                                                 qwn_vocab,\n",
        "                                                 senticnet_vocab,\n",
        "                                                 social_sent\n",
        "                                                 )\n",
        "    \n",
        "    corpus = self.corpus.processed_corpus\n",
        "    scores = [[[res[word] for word in sent] for sent in doc] for doc in corpus]\n",
        "    return scores\n",
        "\n",
        "  def _compute_gold_sents(self):\n",
        "    sentence_sentiment_degree  = [[sum(sent)/len(sent) for sent in doc] for doc in self.sentiment_degree]\n",
        "    gold = [self._normalized_softmax(doc, self.sentence_lambda) for doc in sentence_sentiment_degree]\n",
        "    return gold\n",
        "\n",
        "\n",
        "  def _compute_gold_words(self):\n",
        "    gold = [[self._normalized_softmax(sent_scores, self.word_lambda) for sent_scores in doc] for doc in self.sentiment_degree]\n",
        "    return gold\n",
        "\n",
        "  def _normalized_softmax(self, sequence, lam):\n",
        "    multiplied_sequence = [lam * el for el in sequence]\n",
        "    total = sum([math.exp(el) for el in sequence])\n",
        "    res = torch.tensor([math.exp(lam * el)/total for el in sequence])\n",
        "    return res\n",
        "\n",
        "  def _build_0_1_vocab(self, vocab, path):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/williamleif/socialsent/blob/master/socialsent/data/lexicons/mpqa.json\n",
        "\n",
        "    Values:\n",
        "    - 1 = positive\n",
        "    - 0 = neutral\n",
        "    - -1 = negative\n",
        "    \n",
        "    The absolute value will be taken\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "      lexicon = json.load(f)\n",
        "    \n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "    \n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = lexicon[key]\n",
        "        res_vocab[key] = abs(value)\n",
        "      except KeyError:\n",
        "        pass\n",
        "    \n",
        "    return res_vocab\n",
        "\n",
        "\n",
        "  def _build_senti_vocab(self, vocab):\n",
        "    \"\"\"\n",
        "    builds a vocab using senti-wordnet\n",
        "    \"\"\"\n",
        "    senti_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      senti_vocab[key] = 0\n",
        "\n",
        "    max_value = 0\n",
        "    for key in senti_vocab.keys():\n",
        "      senses = list(swn.senti_synsets(key))\n",
        "      pos = 0\n",
        "      neg = 0\n",
        "      for sense in senses:\n",
        "        if sense.synset.name().split(\".\")[0] == key:\n",
        "          pos += sense.pos_score()\n",
        "          neg += sense.neg_score()\n",
        "      if (pos != 0) or (neg != 0):\n",
        "        senti_vocab[key] = max(pos, neg)\n",
        "      if senti_vocab[key] > max_value:\n",
        "        max_value = senti_vocab[key]\n",
        "\n",
        "    # for key in senti_vocab.keys():\n",
        "      # senti_vocab[key] = self.maprange((0, max_value), (0, 1), senti_vocab[key])\n",
        "\n",
        "    return senti_vocab\n",
        "  \n",
        "  def build_sentic_net_vocab(self, vocab, path):\n",
        "\n",
        "    df = pd.read_csv(path, sep=\"\\t+\")\n",
        "\n",
        "    df.replace([\"negative\", \"positive\"], 1, inplace = True)\n",
        "    # df.set_index([\"CONCEPT\"], inplace = True)\n",
        "\n",
        "    df = dict(zip(df.CONCEPT, df.POLARITY))\n",
        "\n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "\n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = df[key]\n",
        "        res_vocab[key] = value\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    return res_vocab\n",
        "  \n",
        "  def _build_social_sent_vocab(self, vocab, path):\n",
        "    word_path = f\"{path}/frequent_words/\"\n",
        "    adj_path = f\"{path}/adjectives\"\n",
        "    word_files = [os.path.join(word_path, filename) for filename in os.listdir(word_path) if \".tsv\" in filename]\n",
        "    adj_files = [os.path.join(adj_path, filename) for filename in os.listdir(adj_path) if \".tsv\" in filename]\n",
        "\n",
        "    word_dfs = [pd.read_csv(f, sep = \"\\t\", names = [\"word\", \"mean\", \"std\"]) for f in word_files]\n",
        "    adj_dfs = [pd.read_csv(f, sep = \"\\t\", names = [\"word\", \"mean\", \"std\"]) for f in adj_files]\n",
        "\n",
        "    words = pd.read_csv(f\"{word_path}/2000.tsv\", sep = \"\\t\", names = [\"word\", \"mean\", \"std\"])\n",
        "    adjs = pd.read_csv(f\"{adj_path}/2000.tsv\", sep = \"\\t\", names = [\"word\", \"mean\", \"std\"])\n",
        "    tot = pd.concat([words, adjs])\n",
        "\n",
        "    tot = tot.drop(\"std\", axis = 1)\n",
        "    tot[\"mean\"] = tot[\"mean\"].abs()\n",
        "    tot.sort_values(by=[\"mean\"], inplace = True)\n",
        "    tot.drop_duplicates(subset = \"word\", keep=\"last\", inplace = True)\n",
        "\n",
        "    tot = dict(zip(tot[\"word\"], tot[\"mean\"]))\n",
        "\n",
        "    res_vocab = {}\n",
        "    for key in vocab.keys():\n",
        "      res_vocab[key] = 0\n",
        "\n",
        "    for key in res_vocab.keys():\n",
        "      try:\n",
        "        value = tot[key]\n",
        "        res_vocab[key] = value\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    return res_vocab\n",
        "  \n",
        "  def maprange(self, a, b, s):\n",
        "    \"\"\"\n",
        "    Maps the number s from range a = [a1, a2] to range b = [b1, b2]\n",
        "    \"\"\"\n",
        "    # Source: https://rosettacode.org/wiki/Map_range#Python\n",
        "    (a1, a2), (b1, b2) = a, b\n",
        "    return  b1 + ((s - a1) * (b2 - b1) / (a2 - a1))\n",
        "  \n",
        "  def _compute_average_sentiment_degree(self, *args):\n",
        "    \"\"\"\n",
        "    Assumption: all arguments in args are dictionaries containing the same keys\n",
        "    and a numbers as value.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict\n",
        "      average of the sentiment degree across dictionaries for each word\n",
        "    \n",
        "    Example\n",
        "    -------\n",
        "    we have two dictionaries that give a sentiment degree to words:\n",
        "    >>> a = {\"good\": 0.9, \"bad\": 0.7}\n",
        "    >>> b = {\"good\": 0.5, \"bad\": 0.1}\n",
        "    >>> self._compute_average_sentiment_degree(a, b)\n",
        "    {\"good\": 0.7, \"bad\": 0.4}\n",
        "    \"\"\"\n",
        "    n_args = len(args)\n",
        "    res = {}\n",
        "    for arg in args:\n",
        "      for key in arg.keys():\n",
        "        try:\n",
        "          res[key].append(arg[key])\n",
        "        except KeyError:\n",
        "          res[key] = []\n",
        "\n",
        "    tot = 0\n",
        "    found = 0\n",
        "    for key in res.keys():\n",
        "      if len(res[key]) != 0:\n",
        "        # vocab contains the number of occurences in the whole corpus of the sentence\n",
        "        found += 1\n",
        "      tot += 1\n",
        "    \n",
        "    print(f\"Coverage of the lexicon: {found/tot}\")\n",
        "      \n",
        "    for key in res.keys():\n",
        "      if len(res[key]) != 0:\n",
        "        res[key] = sum(res[key]) / len(res[key])\n",
        "      else:\n",
        "        res[key] = 0\n",
        "    \n",
        "    return res\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = self.data[index]\n",
        "    label = self.targets[index]\n",
        "    gold_word = self.wl_gold_av[index]\n",
        "    gold_sent = self.sl_gold_av[index]\n",
        "    return (item, label, gold_word, gold_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT_UVsevkmNR"
      },
      "source": [
        "To understand:\n",
        "- If it is better to introduce intermediate supervision\n",
        "\n",
        "- If it is better to use one hot encoding for the output\n",
        "\n",
        "- If I intepreted well the word-loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_dniRPY5q2A"
      },
      "outputs": [],
      "source": [
        "class EncoderLBSA(BiLSTMAttention):\n",
        "    # Lexicon Based Supervised Attention model (LBSA) inspired by the following paper: https://aclanthology.org/C18-1074.pdf\n",
        "    def __init__(self, embedding_matrix, device=\"cuda\", input_size=300,\n",
        "                 hidden_size=128, context_size = 20, output_size=2):\n",
        "\n",
        "        super(EncoderLBSA, self).__init__(embedding_matrix, device, input_size, hidden_size, context_size, output_size)\n",
        "        self.fc = Residual(hidden_size*2, hidden_size)\n",
        "\n",
        "    # TODO: Pass the part inside for to super.forward()  \n",
        "    def forward(self, x):\n",
        "      att = []\n",
        "      res = []\n",
        "      for i, doc in enumerate(x):\n",
        "        doc = doc.to(self.device)\n",
        "        batch_size = doc.batch_sizes[0].item()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        doc = self.simple_elementwise_apply(self.embedding, doc)\n",
        "\n",
        "        out, _ = self.lstm(doc, hidden)\n",
        "        out, input_sizes = pad_packed_sequence(out, batch_first=True)\n",
        "        # n_sents, n_words_per_sent, hidden_size * 2 (since is bilstm)\n",
        "\n",
        "\n",
        "        attention_values = torch.tanh(self.attention(out))\n",
        "        # n_sents, n_words_per_sent, context_size\n",
        "\n",
        "        attention_weights = torch.softmax(attention_values.matmul(self.history), dim = 1).unsqueeze(1)\n",
        "        # n_sents, n_words_per_sent\n",
        "\n",
        "        out = torch.sum(attention_weights.matmul(out), dim = 1)\n",
        "        # n_sents, hidden*2\n",
        "\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        attention_weights = attention_weights.squeeze(dim=1)\n",
        "\n",
        "        att.append([sent[:input_sizes[idx]] for idx, sent in enumerate(attention_weights)])\n",
        "\n",
        "        res.append(out)\n",
        "      # n_doc, seq_lengths, hidden * 2\n",
        "      return res, att"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyjRmkGAIu94"
      },
      "outputs": [],
      "source": [
        "def sortLBSA(X, w_gold, s_gold):\n",
        "\n",
        "  sentence_lengths = [np.array([sent.size(dim=0) for sent in doc]) for doc in X]\n",
        "  indexes = [np.argsort(doc) for doc in sentence_lengths]\n",
        "  indexes = [el.tolist() for el in indexes]\n",
        "\n",
        "  X_sorted = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(X)]\n",
        "  # w_gold = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(w_gold)]#\n",
        "  # s_gold = [torch.tensor([doc[idx2] for idx2 in indexes[idx]][::-1]) for idx, doc in enumerate(s_gold)]#\n",
        "  sentence_lengths = [[doc[idx2] for idx2 in indexes[idx]][::-1] for idx, doc in enumerate(sentence_lengths)]\n",
        "\n",
        "  return X_sorted, w_gold, s_gold, sentence_lengths, indexes\n",
        "\n",
        "def padLBSA(batch, max_sizes):\n",
        "    pad = torch.tensor([-1])\n",
        "    for idx1, doc in enumerate(batch):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        remaining = max_sizes[idx1] - sent.size(dim = 0)\n",
        "        batch[idx1][idx2] = torch.cat((sent, pad.repeat(remaining)), dim = 0)\n",
        "    return batch\n",
        "\n",
        "def to_tensorLBSA(batch, max_sizes):\n",
        "  res = []\n",
        "  for idx, doc in enumerate(batch):\n",
        "    buff = torch.zeros(len(doc), max_sizes[idx], dtype=torch.int32)\n",
        "    for idx2, sent in enumerate(doc):\n",
        "      buff[idx2] = sent\n",
        "\n",
        "    res.append(buff)\n",
        "  return res\n",
        "\n",
        "\n",
        "def collateLBSA(batch):\n",
        "  X, Y, w_gold, s_gold = list(zip(*batch))\n",
        "\n",
        "  X, w_gold, s_gold, sentence_lengths, indexes = sortLBSA(X, w_gold, s_gold)\n",
        "  # can take doc[0] since senetence_lengths is sorted\n",
        "  max_sizes = [doc[0] for doc in sentence_lengths]\n",
        "\n",
        "  # Pad tensor each element\n",
        "  X = padLBSA(X, max_sizes)\n",
        "  # Transform the batch to a tensor\n",
        "  X = to_tensorLBSA(X, max_sizes)\n",
        "\n",
        "  # Return the padded sequence object\n",
        "  X = [pack_padded_sequence(doc, sentence_lengths[idx], batch_first=True) for idx, doc in enumerate(X)]\n",
        "  return X, Y, w_gold, s_gold, indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXw9lvmaWM3B"
      },
      "outputs": [],
      "source": [
        "def element_wise_log_loss(out, labels):\n",
        "  res = - out.log().mul(labels).sum(dim=0)\n",
        "  return res\n",
        "\n",
        "def loss_LBSA(outputs, targets, mu_w = 0.0005, mu_s = 0.025):\n",
        "  dec_output, w_att, s_att = outputs\n",
        "  target, w_gold, s_gold = targets\n",
        "\n",
        "  total_loss = 0\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "\n",
        "  cross_loss = ce(dec_output, target)\n",
        "  total_loss += cross_loss\n",
        "\n",
        "  w_loss = torch.mean(torch.tensor([\n",
        "    torch.sum(torch.tensor([\n",
        "        element_wise_log_loss(w_att[idx1][idx2], sent) for idx2, sent in enumerate(doc)\n",
        "    ])) * mu_w for idx1, doc in enumerate(w_gold)\n",
        "  ]))\n",
        "  total_loss += w_loss\n",
        "\n",
        "  s_loss = torch.mean(torch.tensor([\n",
        "      element_wise_log_loss(s_att[idx], doc) * mu_s for idx, doc in enumerate(s_gold)\n",
        "  ]))\n",
        "  total_loss += s_loss\n",
        "\n",
        "  return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZF4uo99VKsh"
      },
      "outputs": [],
      "source": [
        "def training_step_LBSA(encoder, decoder, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "\n",
        "  for batch_idx, (inputs, target, w_gold, s_gold, _) in enumerate(data_loader):\n",
        "    \n",
        "    in_size = len(target)\n",
        "\n",
        "    enc_output, w_att = encoder(inputs)\n",
        "    \n",
        "    batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "    dec_input, target, indexes = collate(batch)\n",
        "    \n",
        "    s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "\n",
        "    target = target.to(device)\n",
        "    for idx1, doc in enumerate(w_gold):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "    for idx, doc in enumerate(s_gold):\n",
        "       s_gold[idx] = doc.to(device)\n",
        "    \n",
        "\n",
        "    dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "    outputs = (dec_output, w_att, s_att)\n",
        "    targets = (target, w_gold, s_gold)\n",
        "\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpyI0qRvC0cF"
      },
      "outputs": [],
      "source": [
        "def test_step_LBSA(encoder, decoder, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, target, w_gold, s_gold, _) in enumerate(data_loader):\n",
        "      in_size = len(target)\n",
        "\n",
        "      enc_output, w_att = encoder(inputs)\n",
        "      \n",
        "      batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "      dec_input, target, indexes = collate(batch)\n",
        "\n",
        "      s_gold = [s_gold[idx] for idx in indexes][::-1]\n",
        "      # Not sorting also w_gold becuse in the encoder, documents don't get shuffled\n",
        "\n",
        "      target = target.to(device)\n",
        "      for idx1, doc in enumerate(w_gold):\n",
        "        for idx2, sent in enumerate(doc):\n",
        "          w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "      for idx, doc in enumerate(s_gold):\n",
        "        s_gold[idx] = doc.to(device)\n",
        "\n",
        "      dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "      outputs = (dec_output, w_att, s_att)\n",
        "      targets = (target, w_gold, s_gold)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "      \n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntMCKJDtuEnX"
      },
      "source": [
        "Sorting must be a problem, otherwise word_level attention doesn't get the correct supervision.\n",
        "Same goes for sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN2AM7tLgIDm"
      },
      "outputs": [],
      "source": [
        "def training_step_LBSA_new(encoder, decoder, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "\n",
        "  for batch_idx, (inputs, target, w_gold, s_gold, sent_indexes) in enumerate(data_loader):\n",
        "    \n",
        "    in_size = len(target)\n",
        "\n",
        "    enc_output, w_att = encoder(inputs)\n",
        "    # Sorting the sentences of the encoder to their original position\n",
        "    # Inverting the output because indexes are in ascending order, output is in descending\n",
        "    enc_output = [torch.flip(enc_output[doc_idx], dims = [0]) for doc_idx, _ in enumerate(enc_output)]\n",
        "    w_att = [w_att[doc_idx][::-1] for doc_idx, _ in enumerate(w_att)]\n",
        "    # Using argsort on the indexes reverses the previous argsort used in the collate function\n",
        "    inverted_indexes = [np.argsort(np.array(doc)) for doc in sent_indexes]\n",
        "    inverted_indexes = [el.tolist() for el in inverted_indexes]\n",
        "    # Sort the sentences with original sorting:\n",
        "    # n_doc, n_sents, hidden*2\n",
        "\n",
        "    enc_output = [enc_output[doc_idx][sent_idx] for doc_idx, sent_idx in enumerate(inverted_indexes)]\n",
        "    w_att = [[doc[idx2] for idx2 in inverted_indexes[idx]] for idx, doc in enumerate(w_att)]\n",
        "\n",
        "    \n",
        "    batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "    dec_input, target, indexes = collate(batch)\n",
        "    \n",
        "    target = target.to(device)\n",
        "    for idx1, doc in enumerate(w_gold):\n",
        "      for idx2, sent in enumerate(doc):\n",
        "        w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "    for idx, doc in enumerate(s_gold):\n",
        "       s_gold[idx] = doc.to(device)\n",
        "    \n",
        "\n",
        "    dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "    dec_output = torch.flip(dec_output, dims = [0])\n",
        "    s_att = s_att[::-1]\n",
        "    target = torch.flip(target, dims = [0])\n",
        "\n",
        "\n",
        "    inverted_indexes = np.argsort(np.array(indexes))\n",
        "    inverted_indexes = inverted_indexes.tolist()\n",
        "    # Sort the sentences with original sorting:\n",
        "    # n_doc, n_sents, hidden*2\n",
        "    dec_output = dec_output[inverted_indexes]\n",
        "    s_att = [s_att[idx] for idx in inverted_indexes]\n",
        "    target = target[inverted_indexes]\n",
        "\n",
        "\n",
        "    outputs = (dec_output, w_att, s_att)\n",
        "    targets = (target, w_gold, s_gold)\n",
        "    \n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    samples += in_size\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "    cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEf6kIOghmBm"
      },
      "outputs": [],
      "source": [
        "def test_step_LBSA_new(encoder, decoder, data_loader, cost_function, device = 'cuda'):\n",
        "  cumulative_loss = 0\n",
        "  cumulative_accuracy = 0\n",
        "  samples = 0\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_idx, (inputs, target, w_gold, s_gold, sent_indexes) in enumerate(data_loader):\n",
        "      in_size = len(target)\n",
        "\n",
        "      enc_output, w_att = encoder(inputs)\n",
        "\n",
        "      # First flip (go to ascending order thats because in the dataloader collate\n",
        "      # orders sentences in descending order)\n",
        "      enc_output = [torch.flip(enc_output[doc_idx], dims = [0]) for doc_idx, _ in enumerate(enc_output)]\n",
        "      w_att = [w_att[doc_idx][::-1] for doc_idx, _ in enumerate(w_att)]\n",
        "\n",
        "      # Second take indexes for getting the original positions\n",
        "      # Using argsort on the indexes reverses the previous argsort\n",
        "      inverted_indexes = [np.argsort(np.array(doc)) for doc in sent_indexes]\n",
        "      inverted_indexes = [el.tolist() for el in inverted_indexes]\n",
        "\n",
        "      # Third Sort the sentences with original sorting:\n",
        "      # n_doc, n_sents, hidden*2\n",
        "      enc_output = [enc_output[doc_idx][sent_idx] for doc_idx, sent_idx in enumerate(inverted_indexes)]\n",
        "      w_att = [[doc[idx2] for idx2 in inverted_indexes[idx]] for idx, doc in enumerate(w_att)]\n",
        "      \n",
        "      # Create batch\n",
        "      batch = [(el, target[idx]) for idx, el in enumerate(enc_output)]\n",
        "\n",
        "      dec_input, target, indexes = collate(batch)\n",
        "\n",
        "      # Not sorting also w_gold becuse in the encoder, documents don't get ordered inside collate\n",
        "\n",
        "      # Send w_gold to device\n",
        "      target = target.to(device)\n",
        "      for idx1, doc in enumerate(w_gold):\n",
        "        for idx2, sent in enumerate(doc):\n",
        "          w_gold[idx1][idx2] = sent.to(device)\n",
        "    \n",
        "      # Send s_gold to device\n",
        "      for idx, doc in enumerate(s_gold):\n",
        "        s_gold[idx] = doc.to(device)\n",
        "\n",
        "      dec_output, s_att = decoder(dec_input)\n",
        "\n",
        "\n",
        "      dec_output = torch.flip(dec_output, dims = [0])\n",
        "      s_att = s_att[::-1]\n",
        "      target = torch.flip(target, dims = [0])\n",
        "\n",
        "      inverted_indexes = np.argsort(np.array(indexes))\n",
        "      inverted_indexes = inverted_indexes.tolist()\n",
        "      # Sort the sentences with original sorting:\n",
        "      # n_doc, n_sents, hidden*2\n",
        "      dec_output = dec_output[inverted_indexes]\n",
        "      s_att = [s_att[idx] for idx in inverted_indexes]\n",
        "      target = target[inverted_indexes]\n",
        "\n",
        "      # Not sorting also s_gold becuse in the encoder, documents don't get ordered inside collate\n",
        "\n",
        "      outputs = (dec_output, w_att, s_att)\n",
        "      targets = (target, w_gold, s_gold)\n",
        "\n",
        "      loss = cost_function(outputs, targets)\n",
        "      \n",
        "      samples += in_size\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = dec_output.max(dim=1)\n",
        "\n",
        "      cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, (cumulative_accuracy/samples)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xzpaa7IDN9Z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim import RAdam\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def main_LBSA(train_loader, test_loader, embedding_matrix, device = \"cuda\", epochs = 10):\n",
        "  encoder = EncoderLBSA(embedding_matrix = embedding_matrix, device = device, input_size=300, hidden_size=100).to(device)\n",
        "  decoder = BiLSTMAttention(device = device, input_size = 100, context_size = 20).to(device)\n",
        "\n",
        "  optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), 0.001, betas = (0.9, 0.999), amsgrad=True)\n",
        "\n",
        "  scheduler = ExponentialLR(optimizer, 0.8)\n",
        "\n",
        "  cost_function = loss_LBSA\n",
        "\n",
        "  flag = False\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step_LBSA_new(encoder, decoder, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    test_loss, test_accuracy = test_step_LBSA_new(encoder, decoder, test_loader, cost_function, device)\n",
        "    print(f\"Test loss: {test_loss} \\n Test accuracy: {test_accuracy}\")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    # Model has converged, so no need to go ahead\n",
        "    if train_accuracy == 100:\n",
        "      if flag == True:\n",
        "        break\n",
        "      else:\n",
        "        flag = True\n",
        "    scheduler.step()\n",
        "\n",
        "  return test_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxwWvbEqJoZx"
      },
      "source": [
        "## Polarity Tests\n",
        "### Shallow baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G26_d58UJxf7"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from sklearn.decomposition import PCA\n",
        "from nltk.corpus import subjectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFb5SwmrJ3ND"
      },
      "outputs": [],
      "source": [
        "def neg_marking_list2str(sent):\n",
        "  # takes the doc and produces a single list\n",
        "  # negates the whole document\n",
        "  negated_doc = mark_negation(sent, double_neg_flip=True)\n",
        "  return \" \".join([w for w in negated_doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAYyh_cRKBxd"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
        "\n",
        "corpus = subj + obj\n",
        "\n",
        "corpus = [neg_marking_list2str(d) for d in corpus]\n",
        "vectors = vectorizer.fit_transform(corpus)\n",
        "labels = numpy.array([1] * len(subj) + [0] * len(obj))\n",
        "scores = cross_validate(classifier, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['accuracy'], return_estimator=True)\n",
        "# Taking the best estimator in accuracy\n",
        "pred = scores[\"estimator\"][scores[\"test_accuracy\"].argmax()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIBGJS_xKNXi"
      },
      "outputs": [],
      "source": [
        "def remove_objective_sents(vectorizer, estimator, corpus):\n",
        "  transformed_corpus = [[vectorizer.transform([neg_marking_list2str(sent)]) for sent in doc] for doc in corpus]\n",
        "  res = [[corpus[doc_idx][sent_idx] for sent_idx, sent in enumerate(doc) if estimator.predict(sent).item()]\n",
        "          for doc_idx, doc in enumerate(transformed_corpus)]\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW1aXfRrLESK"
      },
      "outputs": [],
      "source": [
        "def neg_marking_subj(doc):\n",
        "  # takes the doc and produces a single list\n",
        "  flattened_doc = [w for sent in doc for w in sent]\n",
        "  # negates the whole document\n",
        "  negated_doc = mark_negation(flattened_doc, double_neg_flip=True)\n",
        "  return \" \".join([w for w in negated_doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRShQiEOQXeK"
      },
      "outputs": [],
      "source": [
        "mr = movie_reviews\n",
        "neg = mr.paras(categories = \"neg\")\n",
        "pos = mr.paras(categories = \"pos\")\n",
        "mr_corpus = pos + neg\n",
        "\n",
        "\n",
        "mr_corpus = remove_objective_sents(vectorizer, pred, mr_corpus)\n",
        "mr_corpus = [neg_marking_subj(d) for d in mr_corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-aieBxAKuiw"
      },
      "outputs": [],
      "source": [
        "vectors = vectorizer.fit_transform(mr_corpus)\n",
        "labels = numpy.array([0] * len(pos) + [1] * len(neg))\n",
        "\n",
        "# Redefine vectorizer and classifier since already used for subjectivity\n",
        "# classifier = SVC(kernel = \"linear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q18zrgMSLeol",
        "outputId": "b2f03428-6a18-432a-cf55-bf2f6b916c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.863\n"
          ]
        }
      ],
      "source": [
        "scores = cross_validate(classifier, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['accuracy'], return_estimator=True)\n",
        "average = sum(scores['test_accuracy'])/len(scores['test_accuracy'])\n",
        "print(round(average, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQfY8Ci2J3kI"
      },
      "source": [
        "### Deep Models (FastText Emebdding)\n",
        "In this section I am going to analyze perfomances of deep models using fast-text embedding\n",
        "\n",
        "\n",
        "#### BiLSTM with attention mechanism\n",
        "The first model I am going to test is the BiLSTM with attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtCHBADYUXu1"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import FastText\n",
        "fast_text = FastText('en', cache = \"/content/gdrive/My Drive/nlu-project/Embeddings/.vector_cache\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYNaA-pJUgQn"
      },
      "outputs": [],
      "source": [
        "mr_pipeline = Pipeline(UnderscoreRemover(),\n",
        "                       CharacterRepetitionRemover(),\n",
        "                       ApostrophesMerger(),\n",
        "                       ContractionCleaner(),\n",
        "                       SpecialCharsCleaner()\n",
        "                      )\n",
        "mr_corpus = MovieReviewsCorpus(mr_pipeline)\n",
        "mr_embedding_matrix = mr_corpus.get_embedding_matrix(fast_text, 300)\n",
        "mr_dataset = MovieReviewsDataset(mr_corpus.get_indexed_corpus())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9AyWMOqW9gm"
      },
      "outputs": [],
      "source": [
        "# Smaller batch sizes are noisy, this means that they are more regularizing and the\n",
        "# generalization error will be lower\n",
        "mean, std = main_cross_validation(main, mr_dataset, mr_embedding_matrix, collate, epochs = 10, batch_size=32)\n",
        "# 82 +- 3 using 128 as bsize\n",
        "# 85.6 +- 3.6 using 32 as bsize\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt5S5V9gBol5"
      },
      "source": [
        "#### Removing objective sentences:\n",
        "Now the BiLSTM with attention will be tested again, but the corpus will have the objective sentences removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUphVp6zwkqF"
      },
      "outputs": [],
      "source": [
        "# -1 adds the element in the penultimate position (Since flattener should be the last one)\n",
        "mr_pipeline.add_pipeline_element(ShallowObjectiveSentsRemover(), -1)\n",
        "mr_corpus = MovieReviewsCorpus(mr_pipeline)\n",
        "mr_embedding_matrix = mr_corpus.get_embedding_matrix(fast_text, 300)\n",
        "mr_dataset = MovieReviewsDataset(mr_corpus.get_indexed_corpus())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nYyoqlKFxsr"
      },
      "outputs": [],
      "source": [
        "mean, std = main_cross_validation(main, mr_dataset, mr_embedding_matrix, collate, epochs = 10, batch_size=32)\n",
        "# 87.95 +- 1.54\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv3pNRwIwgSa"
      },
      "source": [
        "##### LBSA Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY34RPgshVi9",
        "outputId": "d1de6f33-ed26-4566-928e-3962d57afbb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coverage of the lexicon: 1.0\n"
          ]
        }
      ],
      "source": [
        "LBSA_pipeline = Pipeline(UnderscoreRemover(),\n",
        "                         CharacterRepetitionRemover(),\n",
        "                         ApostrophesMerger(),\n",
        "                         ContractionCleaner(),\n",
        "                         SpecialCharsCleaner(),\n",
        "                         )\n",
        "LBSA_corpus = MovieReviewsCorpusLBSA(LBSA_pipeline)\n",
        "LBSA_embedding_matrix = LBSA_corpus.get_embedding_matrix(fast_text, 300)\n",
        "LBSA_dataset = MovieReviewsDatasetLBSA(LBSA_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynwJzSF7hlJe",
        "outputId": "f6331332-de59-4f37-826e-b3e75879eff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Fold: 0\n",
            "epoch 0:\n",
            "Training loss: 0.03157787091202206 \n",
            " Training accuracy: 49.666666666666664\n",
            "Test loss: 0.035169357359409334 \n",
            " Test accuracy: 54.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.0304488804936409 \n",
            " Training accuracy: 55.222222222222214\n",
            "Test loss: 0.03135250598192215 \n",
            " Test accuracy: 63.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.024964461988872953 \n",
            " Training accuracy: 76.05555555555556\n",
            "Test loss: 0.02926743447780609 \n",
            " Test accuracy: 70.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.017182401650481755 \n",
            " Training accuracy: 89.5\n",
            "Test loss: 0.020321395695209504 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.012025462836027146 \n",
            " Training accuracy: 97.0\n",
            "Test loss: 0.03899595081806183 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.01060045760538843 \n",
            " Training accuracy: 98.66666666666667\n",
            "Test loss: 0.03645720422267914 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.00981305835975541 \n",
            " Training accuracy: 99.77777777777777\n",
            "Test loss: 0.039002386927604674 \n",
            " Test accuracy: 83.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.009714582165082296 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.03367074370384216 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.009750996496942308 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.03050186574459076 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.009761344956027138 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.035059626996517185 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 1\n",
            "epoch 0:\n",
            "Training loss: 0.03141152908404668 \n",
            " Training accuracy: 51.0\n",
            "Test loss: 0.03387990415096283 \n",
            " Test accuracy: 54.50000000000001\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.03209833602110545 \n",
            " Training accuracy: 50.61111111111111\n",
            "Test loss: 0.034524407088756565 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.028363715840710534 \n",
            " Training accuracy: 62.94444444444445\n",
            "Test loss: 0.026260644495487213 \n",
            " Test accuracy: 78.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.017915425995985668 \n",
            " Training accuracy: 88.88888888888889\n",
            "Test loss: 0.027775032222270967 \n",
            " Test accuracy: 83.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.012945104307598538 \n",
            " Training accuracy: 96.22222222222221\n",
            "Test loss: 0.03329158306121826 \n",
            " Test accuracy: 80.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.010998869455522961 \n",
            " Training accuracy: 99.0\n",
            "Test loss: 0.04509784370660782 \n",
            " Test accuracy: 80.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.010848571211099625 \n",
            " Training accuracy: 99.3888888888889\n",
            "Test loss: 0.03145017936825752 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.010355237076679865 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.040564966201782224 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.0103584838575787 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.03205461025238037 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.010344060311714809 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.04027842789888382 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 2\n",
            "epoch 0:\n",
            "Training loss: 0.0311744800541136 \n",
            " Training accuracy: 50.22222222222222\n",
            "Test loss: 0.033818674981594084 \n",
            " Test accuracy: 54.50000000000001\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.025812150372399225 \n",
            " Training accuracy: 69.94444444444444\n",
            "Test loss: 0.02488006979227066 \n",
            " Test accuracy: 78.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.01654287455810441 \n",
            " Training accuracy: 89.33333333333333\n",
            "Test loss: 0.019707374572753907 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.010893991688887278 \n",
            " Training accuracy: 97.61111111111111\n",
            "Test loss: 0.02357743263244629 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.009803210645914077 \n",
            " Training accuracy: 99.16666666666667\n",
            "Test loss: 0.022364788800477983 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.009312243064244588 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.03600063979625702 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.009214419308635924 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.027172791957855224 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.009214388363891178 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.027235576808452608 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.00926109962993198 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.026286040246486665 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 3\n",
            "epoch 0:\n",
            "Training loss: 0.031138012773460812 \n",
            " Training accuracy: 52.0\n",
            "Test loss: 0.033649357557296755 \n",
            " Test accuracy: 51.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.0286195550693406 \n",
            " Training accuracy: 66.05555555555556\n",
            "Test loss: 0.030736443400382996 \n",
            " Test accuracy: 67.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.01787546341617902 \n",
            " Training accuracy: 89.11111111111111\n",
            "Test loss: 0.02257743239402771 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.013763080338637034 \n",
            " Training accuracy: 95.38888888888889\n",
            "Test loss: 0.02261064976453781 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.011199137601587507 \n",
            " Training accuracy: 99.33333333333333\n",
            "Test loss: 0.04049805760383606 \n",
            " Test accuracy: 80.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.010563098755147722 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.033290044963359834 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.010438002513514625 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.03291294753551483 \n",
            " Test accuracy: 84.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.010434538092878129 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.038589647114276884 \n",
            " Test accuracy: 84.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.010482939448621538 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.047501107454299925 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 4\n",
            "epoch 0:\n",
            "Training loss: 0.030805094010300106 \n",
            " Training accuracy: 51.44444444444445\n",
            "Test loss: 0.03339342713356018 \n",
            " Test accuracy: 62.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.02662123610575994 \n",
            " Training accuracy: 74.3888888888889\n",
            "Test loss: 0.027838184237480162 \n",
            " Test accuracy: 75.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.018700125416119893 \n",
            " Training accuracy: 89.38888888888889\n",
            "Test loss: 0.02287334606051445 \n",
            " Test accuracy: 87.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.01369807137383355 \n",
            " Training accuracy: 96.94444444444444\n",
            "Test loss: 0.027935177087783813 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.011620346903800965 \n",
            " Training accuracy: 99.0\n",
            "Test loss: 0.028520659506320954 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.011086600224177042 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.036488948464393614 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.01104548744029469 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.03682748258113861 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.01101462322804663 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.03351674184203148 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 5\n",
            "epoch 0:\n",
            "Training loss: 0.03173806428909302 \n",
            " Training accuracy: 50.22222222222222\n",
            "Test loss: 0.036575117707252504 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.030275358657042187 \n",
            " Training accuracy: 62.5\n",
            "Test loss: 0.04285540044307709 \n",
            " Test accuracy: 66.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.024242864184909396 \n",
            " Training accuracy: 81.16666666666667\n",
            "Test loss: 0.02418693244457245 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.016340893308321634 \n",
            " Training accuracy: 92.83333333333333\n",
            "Test loss: 0.02501859366893768 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.012428141997920143 \n",
            " Training accuracy: 97.72222222222223\n",
            "Test loss: 0.0290234249830246 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.011332004384862052 \n",
            " Training accuracy: 99.22222222222223\n",
            "Test loss: 0.030841749906539918 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.01075512687365214 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.03296737760305404 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.010759642869234086 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.032704184353351595 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.010788543555471632 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.03272640854120255 \n",
            " Test accuracy: 90.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.010859513050980037 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.029005202651023864 \n",
            " Test accuracy: 90.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 6\n",
            "epoch 0:\n",
            "Training loss: 0.03152530398633745 \n",
            " Training accuracy: 50.77777777777778\n",
            "Test loss: 0.034376211762428284 \n",
            " Test accuracy: 57.99999999999999\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.02939403944545322 \n",
            " Training accuracy: 64.72222222222223\n",
            "Test loss: 0.02886670708656311 \n",
            " Test accuracy: 76.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.019705396509832804 \n",
            " Training accuracy: 85.66666666666667\n",
            "Test loss: 0.02899946093559265 \n",
            " Test accuracy: 77.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.012772995283206304 \n",
            " Training accuracy: 96.22222222222221\n",
            "Test loss: 0.026413494348526002 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.010458053317334917 \n",
            " Training accuracy: 99.0\n",
            "Test loss: 0.03158974885940552 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.009969280279344983 \n",
            " Training accuracy: 99.77777777777777\n",
            "Test loss: 0.03107089638710022 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.009708460304472182 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.03522977277636528 \n",
            " Test accuracy: 84.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.009765978753566742 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.04164621114730835 \n",
            " Test accuracy: 83.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 7\n",
            "epoch 0:\n",
            "Training loss: 0.03147354635927412 \n",
            " Training accuracy: 51.5\n",
            "Test loss: 0.033238902390003204 \n",
            " Test accuracy: 54.50000000000001\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.030123391846815745 \n",
            " Training accuracy: 57.277777777777786\n",
            "Test loss: 0.03804590404033661 \n",
            " Test accuracy: 67.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.020404857645432154 \n",
            " Training accuracy: 84.61111111111111\n",
            "Test loss: 0.024307266771793366 \n",
            " Test accuracy: 79.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.013914103425211377 \n",
            " Training accuracy: 94.0\n",
            "Test loss: 0.02455640509724617 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.01062225608362092 \n",
            " Training accuracy: 97.83333333333334\n",
            "Test loss: 0.03669609248638153 \n",
            " Test accuracy: 80.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.009818870226542155 \n",
            " Training accuracy: 99.33333333333333\n",
            "Test loss: 0.03530274957418442 \n",
            " Test accuracy: 84.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.009518640231755044 \n",
            " Training accuracy: 99.77777777777777\n",
            "Test loss: 0.035319694578647615 \n",
            " Test accuracy: 83.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.00938559083475007 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.03898498862981796 \n",
            " Test accuracy: 79.0\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.009347094578875436 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.037334585785865786 \n",
            " Test accuracy: 82.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.009341632657580905 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.04372477978467941 \n",
            " Test accuracy: 82.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 8\n",
            "epoch 0:\n",
            "Training loss: 0.031408920817905 \n",
            " Training accuracy: 50.16666666666667\n",
            "Test loss: 0.03589244246482849 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.03185975968837738 \n",
            " Training accuracy: 50.33333333333333\n",
            "Test loss: 0.033774833083152773 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.026908899810579087 \n",
            " Training accuracy: 69.5\n",
            "Test loss: 0.02407484918832779 \n",
            " Test accuracy: 78.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.01615431456102265 \n",
            " Training accuracy: 91.05555555555556\n",
            "Test loss: 0.019862476587295532 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.011152462330129412 \n",
            " Training accuracy: 97.55555555555556\n",
            "Test loss: 0.027702922523021697 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.00937324207690027 \n",
            " Training accuracy: 99.55555555555556\n",
            "Test loss: 0.029322592616081236 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.009145321812894609 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.03575520619750023 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.009153320259518094 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.027661918103694914 \n",
            " Test accuracy: 88.0\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 9\n",
            "epoch 0:\n",
            "Training loss: 0.03130367765824 \n",
            " Training accuracy: 51.888888888888886\n",
            "Test loss: 0.035152721703052524 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.03042947126759423 \n",
            " Training accuracy: 56.388888888888886\n",
            "Test loss: 0.03236179053783417 \n",
            " Test accuracy: 59.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.022034478021992578 \n",
            " Training accuracy: 82.83333333333334\n",
            "Test loss: 0.031923963129520415 \n",
            " Test accuracy: 78.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.014367785255114238 \n",
            " Training accuracy: 94.33333333333334\n",
            "Test loss: 0.025069569498300553 \n",
            " Test accuracy: 83.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.011533596234189139 \n",
            " Training accuracy: 98.61111111111111\n",
            "Test loss: 0.028273343592882156 \n",
            " Test accuracy: 87.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.010522493173678716 \n",
            " Training accuracy: 99.8888888888889\n",
            "Test loss: 0.029291122555732726 \n",
            " Test accuracy: 86.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.010352677603562673 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.03188973635435104 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.010404514720042547 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.03407805591821671 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "Folds statistics:\n",
            "----------------\n",
            " - mean: 86.65 \n",
            " - standard deviation: 2.4601829200285086\n"
          ]
        }
      ],
      "source": [
        "mean, std = main_cross_validation(main_LBSA, LBSA_dataset, LBSA_embedding_matrix, collateLBSA, epochs = 10, batch_size=32)\n",
        "# 86.2 +- 2.21\n",
        "# 86.1 +- 3\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXAMZFhNsYVa",
        "outputId": "657efbe7-3e69-46d1-c0c1-ef87a9ff7961"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "LBSA_pipeline.add_pipeline_element(ShallowObjectiveSentsRemover())\n",
        "LBSA_corpus = MovieReviewsCorpusLBSA(LBSA_pipeline)\n",
        "LBSA_embedding_matrix = LBSA_corpus.get_embedding_matrix(fast_text, 300)\n",
        "LBSA_dataset = MovieReviewsDatasetLBSA(LBSA_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8-bi60es0KH"
      },
      "outputs": [],
      "source": [
        "mean, std = main_cross_validation(main_LBSA, LBSA_dataset, LBSA_embedding_matrix, collateLBSA, epochs = 15, batch_size=32)\n",
        "# - mean: 88.5 \n",
        "# - standard deviation: 1.9235384061671346\n",
        "\n",
        "# Good Run:\n",
        "# - mean: 89.35 \n",
        "# - standard deviation: 1.285496013218244\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rnr63YbU5Ej"
      },
      "source": [
        "### Deep Models (Glove 840B Embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgNE3lxXrW4M"
      },
      "outputs": [],
      "source": [
        "global_vectors = GloVe(name='840B', dim=300, cache = \"/content/gdrive/My Drive/nlu-project/Embeddings/.vector_cache\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TtoFzYDsGHk",
        "outputId": "4a527579-16da-4ce9-8649-cc4d0ae225ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "LBSA_pipeline = Pipeline(UnderscoreRemover(),\n",
        "                         CharacterRepetitionRemover(),\n",
        "                         ApostrophesMerger(),\n",
        "                         ContractionCleaner(),\n",
        "                         SpecialCharsCleaner(),\n",
        "                         )\n",
        "LBSA_corpus = MovieReviewsCorpusLBSA(LBSA_pipeline)\n",
        "LBSA_embedding_matrix = LBSA_corpus.get_embedding_matrix(global_vectors, 300)\n",
        "LBSA_dataset = MovieReviewsDatasetLBSA(LBSA_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDrwHvOIEsl2",
        "outputId": "b9625b00-26c6-46e7-e6a6-a2389caee8eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Fold: 0\n",
            "epoch 0:\n",
            "Training loss: 0.03088712328010135 \n",
            " Training accuracy: 52.888888888888886\n",
            "Test loss: 0.033306020200252535 \n",
            " Test accuracy: 71.5\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.023796848704417548 \n",
            " Training accuracy: 78.83333333333333\n",
            "Test loss: 0.022833701968193055 \n",
            " Test accuracy: 84.5\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.01700952697131369 \n",
            " Training accuracy: 89.66666666666666\n",
            "Test loss: 0.0197608944773674 \n",
            " Test accuracy: 89.0\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.012782681518130832 \n",
            " Training accuracy: 96.22222222222221\n",
            "Test loss: 0.026397609114646912 \n",
            " Test accuracy: 86.0\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.011112581921948327 \n",
            " Training accuracy: 99.27777777777777\n",
            "Test loss: 0.024816671013832094 \n",
            " Test accuracy: 88.5\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.011317659897936715 \n",
            " Training accuracy: 99.5\n",
            "Test loss: 0.03286577194929123 \n",
            " Test accuracy: 91.0\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.011155783070458307 \n",
            " Training accuracy: 99.72222222222223\n",
            "Test loss: 0.027676002979278566 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.011136825415823195 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.03186442345380783 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.011168492535750071 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.029534547477960586 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.011164532452821731 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.03129702880978584 \n",
            " Test accuracy: 89.5\n",
            "------------------------------------------------------------------\n",
            "\n",
            " Fold: 1\n",
            "epoch 0:\n"
          ]
        }
      ],
      "source": [
        "# 87.3, 86.75 e qualcosa - 88.15 lbsa\n",
        "\n",
        "mean, std = main_cross_validation(main_LBSA, LBSA_dataset, LBSA_embedding_matrix, collateLBSA, epochs = 15, batch_size=32)\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaOnHvIo3X2i"
      },
      "source": [
        "# Subjectivity Detection\n",
        "Now I'm going to implement a subjectivity detector, in order to find objective sentences. This task allows me to remove objective sentences from the subjectivity dataset, so I am left only (almost only, since the model will not be 100% accurate) with objective sentences.\n",
        "\n",
        "## Dataset exploration\n",
        "First I am going to see how the dataset is composed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFZY-nnW3RzT"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import subjectivity\n",
        "\n",
        "\n",
        "subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "obj = [sent for sent in subjectivity.sents(categories = 'obj')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dejtwm5CYsb9"
      },
      "source": [
        "Then I will print the first 10 sentences of each class (subjective and objective) in order to see how the dataset is arranged:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mK4Rmfr4ziT"
      },
      "outputs": [],
      "source": [
        "subj[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMC78B6bYoLk"
      },
      "outputs": [],
      "source": [
        "obj[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grLJbs8ehlzG"
      },
      "source": [
        "It can be clearly seen that the daataset is composed of single phrases, instead of documents as opposed to the movie reviews dataset.\n",
        "Now I am going to compare the length of data from the two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tknJRLrZhlUy"
      },
      "outputs": [],
      "source": [
        "print(len(obj))\n",
        "print(len(subj))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AxGbL1JiLrk"
      },
      "source": [
        "It's easy to see that the length of both objective and subjective datasets are the same length, so accuracy can be used as a metric since the dataset is balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHTqA_vaA2Vm"
      },
      "outputs": [],
      "source": [
        "class SubjectivityCorpus():\n",
        "  def __init__(self, preprocess_pipeline = None):\n",
        "    # list of documents, each document is a list containing words of that document\n",
        "    self.pipeline = preprocess_pipeline\n",
        "    # Corpus as list of documents. Documents as list of sentences. Sentences as list of tokens\n",
        "    self.corpus, self.labels = self._get_corpus()\n",
        "    if preprocess_pipeline == None:\n",
        "      self.processed_corpus = self.corpus\n",
        "    else:\n",
        "      self.processed_corpus = self._preprocess()\n",
        "\n",
        "    #for optimization purposes\n",
        "    self._vocab_index = 0\n",
        "\n",
        "\n",
        "    self.corpus_words = self.get_corpus_words()\n",
        "    self.vocab = self._create_vocab()\n",
        "\n",
        "\n",
        "\n",
        "  def _list_to_str(self, doc) -> str:\n",
        "      \"\"\"\n",
        "      Put all elements of the list into a single string, separating each element with a space.\n",
        "      \"\"\"\n",
        "      return \" \".join([w for sent in doc for w in sent])\n",
        "\n",
        "  def _preprocess(self):\n",
        "      return self.pipeline(self.corpus)\n",
        "\n",
        "  def _get_corpus(self):\n",
        "    subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "    obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
        "    labels = [1] * len(subj) + [0] * len(obj)\n",
        "    return subj + obj, labels\n",
        "\n",
        "  def subjectivity_dataset_raw(self):\n",
        "      \"\"\"\n",
        "      Returns the dataset containing:\n",
        "\n",
        "      - A list of all the sentences\n",
        "      - The corresponding label for each sentence\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      tuple(list, list)\n",
        "          The dataset: first element is the list of the sentence, the second element of the tuple is the associated label (positive or negative) for each sentence\n",
        "      \"\"\"\n",
        "\n",
        "      return self.corpus, self.labels\n",
        "\n",
        "\n",
        "  def get_corpus_words(self) -> list:\n",
        "      return [w for sent in self.processed_corpus for w in sent]\n",
        "  \n",
        "  def get_embedding_matrix(self, embedding, embedding_dim):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      np.ndarray\n",
        "          A 2D which each row has the corresponding embedding from the vocabulary\n",
        "      \"\"\"\n",
        "      matrix_length = len(self.vocab)\n",
        "      embedding_matrix = np.zeros((matrix_length, embedding_dim))\n",
        "      # If I use torch.zeros directly it crashes (don't know why)\n",
        "      embedding_matrix = torch.from_numpy(embedding_matrix.copy())\n",
        "      null_embedding = torch.tensor([0.0]*embedding_dim)\n",
        "      for key in self.vocab.keys():\n",
        "          if torch.equal(embedding[key], null_embedding):\n",
        "              embedding_matrix[self.vocab[key]] = torch.randn(embedding_dim)\n",
        "          else:\n",
        "              embedding_matrix[self.vocab[key]] = embedding[key]\n",
        "              \n",
        "      return embedding_matrix\n",
        "\n",
        "  \n",
        "  def get_indexed_corpus(self):\n",
        "      \"\"\"\n",
        "      Returns\n",
        "      -------\n",
        "      Dictionary\n",
        "          Containing correspondences word -> index\n",
        "      \n",
        "      list(list(torch.tensor))\n",
        "          The corpus represented as indexes corresponding to each word\n",
        "      \"\"\"\n",
        "      vocab = {}\n",
        "      for idx, key in enumerate(self.vocab.keys()):\n",
        "          vocab[key] = idx\n",
        "      \n",
        "      indexed_corpus = [torch.tensor([vocab[word] for word in sent]) for sent in self.processed_corpus]\n",
        "      return indexed_corpus, self.labels\n",
        "\n",
        "  def embed_vocab(self, vocab):\n",
        "    for word in vocab.keys():\n",
        "      try:\n",
        "          self.vocab[word]\n",
        "      except:\n",
        "          self.vocab[word] = self._vocab_index\n",
        "          self._vocab_index += 1\n",
        "\n",
        "  def _create_vocab(self):\n",
        "      vocab = dict()\n",
        "      for word in self.corpus_words:\n",
        "        try:\n",
        "          vocab[word]\n",
        "        except:\n",
        "          vocab[word] = self._vocab_index\n",
        "          self._vocab_index += 1\n",
        "      return vocab\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NXv420XEUR5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "class SubjectivityDataset(Dataset):\n",
        "  def __init__(self, raw_dataset):\n",
        "    super(SubjectivityDataset, self).__init__()\n",
        "    self.corpus = raw_dataset[0]\n",
        "    self.targets = raw_dataset[1]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.corpus)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    item = self.corpus[index]\n",
        "    label = self.targets[index]\n",
        "    return (item, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_18YMFwpe9e"
      },
      "outputs": [],
      "source": [
        "pipeline_sub = pipeline = Pipeline(UnderscoreRemoverFlat(),\n",
        "                                    CharacterRepetitionRemoverFlat(),\n",
        "                                    ApostrophesMergerFlat(),\n",
        "                                    ContractionCleanerFlat(),\n",
        "                                    SpecialCharsCleanerFlat(),\n",
        "                                    )\n",
        "corpus_subj = SubjectivityCorpus(pipeline)\n",
        "corpus_subj.embed_vocab(corpus.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTOap0UOpxhC"
      },
      "outputs": [],
      "source": [
        "dataset_subj = SubjectivityDataset(corpus_subj.get_indexed_corpus())\n",
        "train_loader_subj, test_loader_subj = get_data(128, dataset_subj, collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7l4AVLtqsaT"
      },
      "outputs": [],
      "source": [
        "embedding_matrix_subj = corpus_subj.get_embedding_matrix(global_vectors, 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1yZv0OksfuA"
      },
      "outputs": [],
      "source": [
        "_, net_obj= main(train_loader_subj, test_loader_subj, embedding_matrix_subj, device = \"cuda\", epochs = 10)\n",
        "\n",
        "# print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUKX632yBajt"
      },
      "source": [
        "## Try to use trained subjectivity in order to improve polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bEcAGPHBy0o"
      },
      "outputs": [],
      "source": [
        "def collate_testing(X, device = \"cuda\"):\n",
        "  if type(X) == list:\n",
        "    X = torch.tensor(X)\n",
        "  if len(X.size()) == 1:\n",
        "    X = torch.unsqueeze(X, 0)\n",
        "  \n",
        "  X_tensor = X.to(device)\n",
        "  X_final = pack_padded_sequence(X_tensor, torch.tensor([1]), batch_first=True)\n",
        "  return X_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-wAhSa0ihJg"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.sentiment.util import mark_negation\n",
        "\n",
        "class ObjectiveSentsRemover(PipelineElement):\n",
        "  def __init__(self, net = None, vocab = None):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    net\n",
        "      Already trained network in order to classify objective sentences\n",
        "    \"\"\"\n",
        "    super(ObjectiveSentsRemover, self).__init__()\n",
        "    if net == None:\n",
        "      raise TypeError(\"Network cannot be None-type\")\n",
        "    self.net = net\n",
        "    self.vocab = vocab\n",
        "  \n",
        "  def _indexed_corpus(self, corpus):\n",
        "    indexed_corpus = []\n",
        "    for doc in corpus:\n",
        "      new_doc = []\n",
        "      for sent in doc:\n",
        "        new_sent = []\n",
        "        for word in sent:\n",
        "          new_sent.append(self.vocab[word])\n",
        "        new_doc.append(new_sent)\n",
        "      indexed_corpus.append(new_doc)\n",
        "    return indexed_corpus\n",
        "\n",
        "\n",
        "  def remove_objective_sents(self, corpus):\n",
        "    indexed_corpus = self._indexed_corpus(corpus)\n",
        "    print(len(corpus[0]))\n",
        "    with torch.no_grad():\n",
        "      self.net.eval()\n",
        "      # I want to keep the sentence if it is subjective i.e. when result of classification = 1\n",
        "      res = [[corpus[doc_idx][sent_idx] for sent_idx, sent in enumerate(doc) if self.net(collate_testing(sent))[0].max(dim = 1)[1].item() == 1]\n",
        "             for doc_idx, doc in enumerate(indexed_corpus)]\n",
        "      print(len(res[0]))\n",
        "    return res\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_objective_sents(corpus)\n",
        "\n",
        "class ShallowObjectiveSentsRemover(PipelineElement):\n",
        "  def __init__(self, threshold = .5, clf = MultinomialNB, trained = False):\n",
        "    self.vectorizer = CountVectorizer()\n",
        "    self.classifier = clf()\n",
        "    if not trained:\n",
        "      self.best_estimator = self._train()\n",
        "    else:\n",
        "      self.best_estimator = self.classifier\n",
        "  \n",
        "  def _train(self):\n",
        "    subj = [sent for sent in subjectivity.sents(categories = 'subj')]\n",
        "    obj = [sent for sent in subjectivity.sents(categories = 'obj')]\n",
        "\n",
        "    corpus = [self.neg_marking_list2str(d) for d in subj] + [self.neg_marking_list2str(d) for d in obj]\n",
        "    vectors = self.vectorizer.fit_transform(corpus)\n",
        "    labels = np.array([1] * len(subj) + [0] * len(obj))\n",
        "    scores = cross_validate(self.classifier, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['accuracy'], return_estimator=True)\n",
        "    estimator = scores[\"estimator\"][scores[\"test_accuracy\"].argmax()]\n",
        "    return estimator\n",
        "\n",
        "  def neg_marking_list2str(self, sent):\n",
        "    # takes the doc and produces a single list\n",
        "    # negates the whole document\n",
        "    negated_doc = mark_negation(sent, double_neg_flip=True)\n",
        "    return \" \".join([w for w in negated_doc])\n",
        "    \n",
        "\n",
        "  def remove_objective_sents(self, corpus):\n",
        "    transformed_corpus = [[self.vectorizer.transform([self.neg_marking_list2str(sent)]) for sent in doc] for doc in corpus]\n",
        "    print(len(corpus[0]))\n",
        "    res = [[corpus[doc_idx][sent_idx] for sent_idx, sent in enumerate(doc) if self.best_estimator.predict(sent).item()]\n",
        "           for doc_idx, doc in enumerate(transformed_corpus)]\n",
        "    print(len(res[0]))\n",
        "    return res\n",
        "  \n",
        "  def __call__(self, corpus):\n",
        "    return self.remove_objective_sents(corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42sw8MWVfKpN",
        "outputId": "eafb9da4-43b6-4ecb-bc74-9b9e467ed3cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "pipeline = Pipeline(UnderscoreRemover(),\n",
        "                    CharacterRepetitionRemover(),\n",
        "                    ApostrophesMerger(),\n",
        "                    ContractionCleaner(),\n",
        "                    SpecialCharsCleaner(),\n",
        "                    ShallowObjectiveSentsRemover(),\n",
        "                    )\n",
        "corpus = MovieReviewsCorpusLBSA(pipeline)\n",
        "# 22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A6Hxlu_fL05",
        "outputId": "f8a3166e-b31b-4b15-e4e9-6aa07b0d3447"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix = corpus.get_embedding_matrix(global_vectors, 300)\n",
        "dataset = MovieReviewsDatasetLBSA(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CU07k0-tjbx5",
        "outputId": "a3cc5efb-a1b1-402a-850c-f646d77aa38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Fold: 0\n",
            "epoch 0:\n",
            "Training loss: 0.007959709366162618 \n",
            " Training accuracy: 61.05555555555555\n",
            "Test loss: 0.009188937544822693 \n",
            " Test accuracy: 50.0\n",
            "------------------------------------------------------------------\n",
            "epoch 1:\n",
            "Training loss: 0.005274999472830031 \n",
            " Training accuracy: 78.38888888888889\n",
            "Test loss: 0.00855442076921463 \n",
            " Test accuracy: 56.49999999999999\n",
            "------------------------------------------------------------------\n",
            "epoch 2:\n",
            "Training loss: 0.003910372091664208 \n",
            " Training accuracy: 89.77777777777777\n",
            "Test loss: 0.0074517691135406496 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 3:\n",
            "Training loss: 0.002459457086192237 \n",
            " Training accuracy: 96.38888888888889\n",
            "Test loss: 0.0054387211799621586 \n",
            " Test accuracy: 84.5\n",
            "------------------------------------------------------------------\n",
            "epoch 4:\n",
            "Training loss: 0.0018047150638368394 \n",
            " Training accuracy: 98.83333333333333\n",
            "Test loss: 0.0091081303358078 \n",
            " Test accuracy: 79.0\n",
            "------------------------------------------------------------------\n",
            "epoch 5:\n",
            "Training loss: 0.0017057364765140745 \n",
            " Training accuracy: 99.3888888888889\n",
            "Test loss: 0.013652061820030212 \n",
            " Test accuracy: 79.5\n",
            "------------------------------------------------------------------\n",
            "epoch 6:\n",
            "Training loss: 0.0016082441558440527 \n",
            " Training accuracy: 99.77777777777777\n",
            "Test loss: 0.01578713774681091 \n",
            " Test accuracy: 75.0\n",
            "------------------------------------------------------------------\n",
            "epoch 7:\n",
            "Training loss: 0.0016047549413310156 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.010998684465885162 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 8:\n",
            "Training loss: 0.0015644259171353446 \n",
            " Training accuracy: 99.94444444444444\n",
            "Test loss: 0.011297451257705688 \n",
            " Test accuracy: 85.0\n",
            "------------------------------------------------------------------\n",
            "epoch 9:\n",
            "Training loss: 0.0016350032389163972 \n",
            " Training accuracy: 100.0\n",
            "Test loss: 0.010835739374160767 \n",
            " Test accuracy: 85.5\n",
            "------------------------------------------------------------------\n",
            "epoch 10:\n",
            "Training loss: 0.0016054754869805443 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.025587269067764283 \n",
            " Test accuracy: 75.0\n",
            "------------------------------------------------------------------\n",
            "epoch 11:\n",
            "Training loss: 0.0016089071333408355 \n",
            " Training accuracy: 99.83333333333333\n",
            "Test loss: 0.024370667934417726 \n",
            " Test accuracy: 72.0\n",
            "------------------------------------------------------------------\n",
            "epoch 12:\n",
            "Training loss: 0.0016930692394574484 \n",
            " Training accuracy: 99.66666666666667\n",
            "Test loss: 0.02161523461341858 \n",
            " Test accuracy: 76.0\n",
            "------------------------------------------------------------------\n",
            "epoch 13:\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a2c47ea2af9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 20 epochs because of the warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_LBSA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollateLBSA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m## 88.65 +- 1.24 no residual neither batchnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## 89.15 +- 2.16             Residual with reg = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-519a3c497c07>\u001b[0m in \u001b[0;36mmain_cross_validation\u001b[0;34m(main_fn, dataset, embedding_matrix, collate_fn, device, epochs, random_state, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mfold_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-f188fda874d1>\u001b[0m in \u001b[0;36mmain_LBSA\u001b[0;34m(train_loader, test_loader, embedding_matrix, device, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch {e}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step_LBSA_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step_LBSA_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-59d9cd3a367f>\u001b[0m in \u001b[0;36mtraining_step_LBSA_new\u001b[0;34m(encoder, decoder, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0min_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Sorting the sentences of the encoder to their original position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Inverting the output because indexes are in ascending order, output is in descending\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-b7c96e878766>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_elementwise_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# n_sents, n_words_per_sent, hidden_size * 2 (since is bilstm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 773\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 20 epochs because of the warmup\n",
        "mean, std = main_cross_validation(main_LBSA, dataset, embedding_matrix, collateLBSA, epochs = 15)\n",
        "## 88.65 +- 1.24 no residual neither batchnorm\n",
        "## 89.15 +- 2.16             Residual with reg = False\n",
        "print(f\"Folds statistics:\\n----------------\\n - mean: {mean} \\n - standard deviation: {std}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZgEYeACSZha"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('nlu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "e0262c2e7a08424d65c968f8ecfc5afb6b5a99089f86fd0fa27478ea619b0ef2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}